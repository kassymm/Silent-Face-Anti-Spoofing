{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\", index_col=0)\n",
    "df.rename(columns={'Real': 'Prediction', '40':'SpoofType', '41':'Illumination', '42':'Environment', '43':'Spoof'}, inplace=True)\n",
    "# the prediction by silent face takes values 0-2. Gotta convert it.\n",
    "df['Prediction'] = df['Prediction'].replace({0.0: 1, 1.0: 0, 2.0: 1})\n",
    "df['Prediction'] = df['Prediction'].astype(int)\n",
    "\n",
    "embeddings = pd.read_csv('dropout_embeddings.csv', index_col=0)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "common_index = df.index.intersection(embeddings.index)\n",
    "df = df.loc[common_index]\n",
    "embeddings = embeddings.loc[common_index]\n",
    "\n",
    "pd.testing.assert_series_equal(df.index.to_series(), embeddings.index.to_series())\n",
    "\n",
    "\n",
    "gmm = GaussianMixture(n_components=100, random_state=42)\n",
    "clusters = gmm.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['Spoof'] = df['Spoof']\n",
    "embeddings['Cluster'] = clusters\n",
    "embeddings['Illumination'] = df['Illumination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = len(embeddings)\n",
    "np.random.seed(seed = 42)\n",
    "random_cluster = np.random.randint(5, size = n)\n",
    "\n",
    "embeddings['Random'] = random_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>Spoof</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Illumination</th>\n",
       "      <th>Random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data/test/6964/spoof/494405.png</th>\n",
       "      <td>0.420929</td>\n",
       "      <td>0.391198</td>\n",
       "      <td>0.395073</td>\n",
       "      <td>-0.421826</td>\n",
       "      <td>0.368112</td>\n",
       "      <td>0.353278</td>\n",
       "      <td>0.749079</td>\n",
       "      <td>0.455125</td>\n",
       "      <td>-0.878119</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765734</td>\n",
       "      <td>-0.755238</td>\n",
       "      <td>0.726697</td>\n",
       "      <td>-0.765685</td>\n",
       "      <td>-0.470916</td>\n",
       "      <td>0.775886</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6407/spoof/494411.png</th>\n",
       "      <td>-0.147611</td>\n",
       "      <td>0.357509</td>\n",
       "      <td>-0.029671</td>\n",
       "      <td>-0.238791</td>\n",
       "      <td>0.286075</td>\n",
       "      <td>-0.134124</td>\n",
       "      <td>0.624391</td>\n",
       "      <td>0.178886</td>\n",
       "      <td>-0.596819</td>\n",
       "      <td>-0.246836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.574118</td>\n",
       "      <td>-0.704340</td>\n",
       "      <td>0.688574</td>\n",
       "      <td>-0.656510</td>\n",
       "      <td>0.135470</td>\n",
       "      <td>0.686906</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6153/spoof/494412.png</th>\n",
       "      <td>-0.105897</td>\n",
       "      <td>-0.249867</td>\n",
       "      <td>-0.077944</td>\n",
       "      <td>0.181672</td>\n",
       "      <td>-0.120957</td>\n",
       "      <td>-0.085050</td>\n",
       "      <td>0.615739</td>\n",
       "      <td>-0.094033</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>0.141131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.605586</td>\n",
       "      <td>-0.580930</td>\n",
       "      <td>0.522196</td>\n",
       "      <td>-0.592422</td>\n",
       "      <td>0.108543</td>\n",
       "      <td>0.643653</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6411/live/494418.png</th>\n",
       "      <td>0.441198</td>\n",
       "      <td>0.528195</td>\n",
       "      <td>0.404773</td>\n",
       "      <td>-0.509041</td>\n",
       "      <td>0.446964</td>\n",
       "      <td>0.322769</td>\n",
       "      <td>0.645940</td>\n",
       "      <td>0.537904</td>\n",
       "      <td>-0.691915</td>\n",
       "      <td>-0.460678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600595</td>\n",
       "      <td>-0.675778</td>\n",
       "      <td>0.649979</td>\n",
       "      <td>-0.616289</td>\n",
       "      <td>-0.460027</td>\n",
       "      <td>0.677907</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6336/spoof/494419.png</th>\n",
       "      <td>0.753356</td>\n",
       "      <td>0.419226</td>\n",
       "      <td>0.744619</td>\n",
       "      <td>-0.425761</td>\n",
       "      <td>0.417521</td>\n",
       "      <td>0.629097</td>\n",
       "      <td>0.054008</td>\n",
       "      <td>0.498429</td>\n",
       "      <td>-0.053692</td>\n",
       "      <td>-0.445463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103442</td>\n",
       "      <td>0.036649</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.089932</td>\n",
       "      <td>-0.705690</td>\n",
       "      <td>0.065030</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/9909/live/561566.png</th>\n",
       "      <td>-0.519080</td>\n",
       "      <td>-0.166543</td>\n",
       "      <td>-0.483397</td>\n",
       "      <td>0.018525</td>\n",
       "      <td>-0.025248</td>\n",
       "      <td>-0.455392</td>\n",
       "      <td>0.709185</td>\n",
       "      <td>-0.054364</td>\n",
       "      <td>-0.614981</td>\n",
       "      <td>0.179256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.637598</td>\n",
       "      <td>-0.820888</td>\n",
       "      <td>0.733749</td>\n",
       "      <td>-0.704391</td>\n",
       "      <td>0.470004</td>\n",
       "      <td>0.818063</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6794/spoof/561567.png</th>\n",
       "      <td>-0.518010</td>\n",
       "      <td>-0.176494</td>\n",
       "      <td>-0.482479</td>\n",
       "      <td>0.025188</td>\n",
       "      <td>-0.025568</td>\n",
       "      <td>-0.453009</td>\n",
       "      <td>0.698944</td>\n",
       "      <td>-0.063262</td>\n",
       "      <td>-0.605874</td>\n",
       "      <td>0.179502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.634073</td>\n",
       "      <td>-0.810268</td>\n",
       "      <td>0.728125</td>\n",
       "      <td>-0.697531</td>\n",
       "      <td>0.467782</td>\n",
       "      <td>0.813546</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6708/spoof/561569.png</th>\n",
       "      <td>-0.531236</td>\n",
       "      <td>-0.176063</td>\n",
       "      <td>-0.496923</td>\n",
       "      <td>0.025220</td>\n",
       "      <td>-0.030200</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>0.701359</td>\n",
       "      <td>-0.064843</td>\n",
       "      <td>-0.605759</td>\n",
       "      <td>0.184143</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.627472</td>\n",
       "      <td>-0.813603</td>\n",
       "      <td>0.736568</td>\n",
       "      <td>-0.705165</td>\n",
       "      <td>0.482111</td>\n",
       "      <td>0.815603</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6551/spoof/561570.png</th>\n",
       "      <td>-0.517504</td>\n",
       "      <td>-0.174754</td>\n",
       "      <td>-0.485925</td>\n",
       "      <td>0.022997</td>\n",
       "      <td>-0.027235</td>\n",
       "      <td>-0.453871</td>\n",
       "      <td>0.709815</td>\n",
       "      <td>-0.061128</td>\n",
       "      <td>-0.617192</td>\n",
       "      <td>0.182697</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638211</td>\n",
       "      <td>-0.822434</td>\n",
       "      <td>0.742672</td>\n",
       "      <td>-0.712718</td>\n",
       "      <td>0.464891</td>\n",
       "      <td>0.819803</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6283/spoof/561571.png</th>\n",
       "      <td>-0.506432</td>\n",
       "      <td>-0.163607</td>\n",
       "      <td>-0.467940</td>\n",
       "      <td>0.015219</td>\n",
       "      <td>-0.019917</td>\n",
       "      <td>-0.440108</td>\n",
       "      <td>0.698670</td>\n",
       "      <td>-0.053623</td>\n",
       "      <td>-0.602160</td>\n",
       "      <td>0.173266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.627014</td>\n",
       "      <td>-0.804246</td>\n",
       "      <td>0.730291</td>\n",
       "      <td>-0.697133</td>\n",
       "      <td>0.451270</td>\n",
       "      <td>0.808349</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25755 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        1         2         3         4  \\\n",
       "Data/test/6964/spoof/494405.png  0.420929  0.391198  0.395073 -0.421826   \n",
       "Data/test/6407/spoof/494411.png -0.147611  0.357509 -0.029671 -0.238791   \n",
       "Data/test/6153/spoof/494412.png -0.105897 -0.249867 -0.077944  0.181672   \n",
       "Data/test/6411/live/494418.png   0.441198  0.528195  0.404773 -0.509041   \n",
       "Data/test/6336/spoof/494419.png  0.753356  0.419226  0.744619 -0.425761   \n",
       "...                                   ...       ...       ...       ...   \n",
       "Data/test/9909/live/561566.png  -0.519080 -0.166543 -0.483397  0.018525   \n",
       "Data/test/6794/spoof/561567.png -0.518010 -0.176494 -0.482479  0.025188   \n",
       "Data/test/6708/spoof/561569.png -0.531236 -0.176063 -0.496923  0.025220   \n",
       "Data/test/6551/spoof/561570.png -0.517504 -0.174754 -0.485925  0.022997   \n",
       "Data/test/6283/spoof/561571.png -0.506432 -0.163607 -0.467940  0.015219   \n",
       "\n",
       "                                        5         6         7         8  \\\n",
       "Data/test/6964/spoof/494405.png  0.368112  0.353278  0.749079  0.455125   \n",
       "Data/test/6407/spoof/494411.png  0.286075 -0.134124  0.624391  0.178886   \n",
       "Data/test/6153/spoof/494412.png -0.120957 -0.085050  0.615739 -0.094033   \n",
       "Data/test/6411/live/494418.png   0.446964  0.322769  0.645940  0.537904   \n",
       "Data/test/6336/spoof/494419.png  0.417521  0.629097  0.054008  0.498429   \n",
       "...                                   ...       ...       ...       ...   \n",
       "Data/test/9909/live/561566.png  -0.025248 -0.455392  0.709185 -0.054364   \n",
       "Data/test/6794/spoof/561567.png -0.025568 -0.453009  0.698944 -0.063262   \n",
       "Data/test/6708/spoof/561569.png -0.030200 -0.465959  0.701359 -0.064843   \n",
       "Data/test/6551/spoof/561570.png -0.027235 -0.453871  0.709815 -0.061128   \n",
       "Data/test/6283/spoof/561571.png -0.019917 -0.440108  0.698670 -0.053623   \n",
       "\n",
       "                                        9        10  ...       123       124  \\\n",
       "Data/test/6964/spoof/494405.png -0.878119 -0.339846  ... -0.765734 -0.755238   \n",
       "Data/test/6407/spoof/494411.png -0.596819 -0.246836  ... -0.574118 -0.704340   \n",
       "Data/test/6153/spoof/494412.png -0.588235  0.141131  ... -0.605586 -0.580930   \n",
       "Data/test/6411/live/494418.png  -0.691915 -0.460678  ... -0.600595 -0.675778   \n",
       "Data/test/6336/spoof/494419.png -0.053692 -0.445463  ... -0.103442  0.036649   \n",
       "...                                   ...       ...  ...       ...       ...   \n",
       "Data/test/9909/live/561566.png  -0.614981  0.179256  ... -0.637598 -0.820888   \n",
       "Data/test/6794/spoof/561567.png -0.605874  0.179502  ... -0.634073 -0.810268   \n",
       "Data/test/6708/spoof/561569.png -0.605759  0.184143  ... -0.627472 -0.813603   \n",
       "Data/test/6551/spoof/561570.png -0.617192  0.182697  ... -0.638211 -0.822434   \n",
       "Data/test/6283/spoof/561571.png -0.602160  0.173266  ... -0.627014 -0.804246   \n",
       "\n",
       "                                      125       126       127       128  \\\n",
       "Data/test/6964/spoof/494405.png  0.726697 -0.765685 -0.470916  0.775886   \n",
       "Data/test/6407/spoof/494411.png  0.688574 -0.656510  0.135470  0.686906   \n",
       "Data/test/6153/spoof/494412.png  0.522196 -0.592422  0.108543  0.643653   \n",
       "Data/test/6411/live/494418.png   0.649979 -0.616289 -0.460027  0.677907   \n",
       "Data/test/6336/spoof/494419.png  0.001247 -0.089932 -0.705690  0.065030   \n",
       "...                                   ...       ...       ...       ...   \n",
       "Data/test/9909/live/561566.png   0.733749 -0.704391  0.470004  0.818063   \n",
       "Data/test/6794/spoof/561567.png  0.728125 -0.697531  0.467782  0.813546   \n",
       "Data/test/6708/spoof/561569.png  0.736568 -0.705165  0.482111  0.815603   \n",
       "Data/test/6551/spoof/561570.png  0.742672 -0.712718  0.464891  0.819803   \n",
       "Data/test/6283/spoof/561571.png  0.730291 -0.697133  0.451270  0.808349   \n",
       "\n",
       "                                 Spoof  Cluster  Illumination  Random  \n",
       "Data/test/6964/spoof/494405.png      1        6             2       3  \n",
       "Data/test/6407/spoof/494411.png      1       23             1       4  \n",
       "Data/test/6153/spoof/494412.png      1       41             1       2  \n",
       "Data/test/6411/live/494418.png       0       76             0       4  \n",
       "Data/test/6336/spoof/494419.png      1       50             1       4  \n",
       "...                                ...      ...           ...     ...  \n",
       "Data/test/9909/live/561566.png       0       89             0       1  \n",
       "Data/test/6794/spoof/561567.png      1       89             1       0  \n",
       "Data/test/6708/spoof/561569.png      1       89             1       2  \n",
       "Data/test/6551/spoof/561570.png      1       89             1       4  \n",
       "Data/test/6283/spoof/561571.png      1       89             2       4  \n",
       "\n",
       "[25755 rows x 132 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(20, 4))\n",
    "\n",
    "# # Create a histogram for each cluster\n",
    "# for cluster_id, group in embeddings.groupby('Cluster'):\n",
    "#     # Count the frequency of each value of the current feature in the current cluster\n",
    "#     counts = group[\"Spoof\"].value_counts()\n",
    "\n",
    "#     # Create a bar chart of the counts in the current subplot\n",
    "#     axs[cluster_id].bar(counts.index, counts.values)\n",
    "#     axs[cluster_id].set_xlabel(\"Spoof\")\n",
    "#     axs[cluster_id].set_ylabel(\"Count\")\n",
    "#     axs[cluster_id].set_title(f\"Cluster {cluster_id}\")\n",
    "\n",
    "# # Add a title to the overall plot\n",
    "# plt.suptitle(\"Histograms of Spoof\")\n",
    "\n",
    "# # Adjust the layout of the subplots\n",
    "# plt.tight_layout()\n",
    "\n",
    "# # Display the overall plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sf_column = {'Spoof':128, 'Cluster':129, 'Illumination':130, 'Random':131}\n",
    "\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.embeddings.iloc[idx, :128].values, dtype=torch.float32)\n",
    "        spoof = torch.tensor(self.embeddings.iloc[idx, 128], dtype=torch.long)\n",
    "        # 128 = Spoof, 129 = Cluster, 130 = Illumination\n",
    "        domain = torch.tensor(self.embeddings.iloc[idx, 131], dtype=torch.long)\n",
    "        return embedding, spoof, domain\n",
    "\n",
    "\n",
    "class AdversarialModel(nn.Module):\n",
    "    def __init__(self, num_clusters):\n",
    "        super(AdversarialModel, self).__init__()\n",
    "        self.step = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.anti_spoofing_head = nn.Sequential(\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "        self.domain_head = nn.Sequential(\n",
    "            nn.Linear(64, num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.step(x)\n",
    "        anti_spoofing_logits = self.anti_spoofing_head(x)\n",
    "        domain_logits = self.domain_head(x)\n",
    "        return anti_spoofing_logits, domain_logits\n",
    "\n",
    "class RegularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegularModel, self).__init__()\n",
    "        self.step = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.step(x)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# sf_column = {'Spoof':128, 'Cluster':129, 'Illumination':130}\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.embeddings.iloc[idx, :128].values, dtype=torch.float32)\n",
    "        spoof = torch.tensor(self.embeddings.iloc[idx, 128], dtype=torch.long)\n",
    "        # 128 = Spoof, 129 = Cluster, 130 = Illumination\n",
    "        domain = torch.tensor(self.embeddings.iloc[idx, 129], dtype=torch.long)\n",
    "        return embedding, spoof, domain\n",
    "\n",
    "def train(target_domain, num_folds = 5, num_epochs = 4, batch_size = 64, learning_rate = 0.001):\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = EmbeddingsDataset(embeddings)\n",
    "\n",
    "    # Create the KFold object\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Iterate over the folds\n",
    "    for fold, (train_indices, test_indices) in enumerate(kf.split(dataset)):\n",
    "        print(f\"Domain: {target_domain}, Fold {fold+1}\")\n",
    "\n",
    "        # Create the data loaders for the current fold\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "        # Create the model and set the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AdversarialModel(2).to(device)\n",
    "\n",
    "        # Define the loss functions and optimizers\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_classifier = optim.Adam(model.anti_spoofing_head.parameters(), lr=learning_rate)\n",
    "        optimizer_domain = optim.Adam(model.domain_head.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the model for the current fold\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (embedding, spoof, domain) in enumerate(train_loader):\n",
    "                # Move the data to the device\n",
    "                embedding = embedding.to(device)\n",
    "                spoof = spoof.to(device)\n",
    "                domain = (domain == target_domain).long().to(device)\n",
    "\n",
    "                # Step 1: Update all weights except the domain head\n",
    "                optimizer_classifier.zero_grad()\n",
    "                anti_spoofing_logits, domain_logits = model(embedding)\n",
    "                loss_main = criterion(anti_spoofing_logits, spoof) - criterion(domain_logits, domain)\n",
    "                loss_main.backward()\n",
    "                optimizer_classifier.step()\n",
    "\n",
    "                # Step 2: Update the domain head \n",
    "                if i%100 == 0:\n",
    "                    optimizer_domain.zero_grad()\n",
    "                    _, domain_logits = model(embedding)\n",
    "                    loss_domain = criterion(domain_logits, domain)\n",
    "                    loss_domain.backward()\n",
    "                    optimizer_domain.step()\n",
    "\n",
    "                # # Print progress\n",
    "                # if (i+1) % 100 == 0:\n",
    "                #     print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss_main: {loss_main.item()}, Loss_domain: {loss_domain.item()}\")\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss_main: {loss_main.item()}, Loss_domain: {loss_domain.item()}\")\n",
    "        print(\"Training complete for fold\", fold+1)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Define the lists to store the true and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Iterate over the test data\n",
    "    for i, (embedding, spoof, domain) in enumerate(test_loader):\n",
    "\n",
    "        # Move the data to the device\n",
    "        embedding = embedding.to(device)\n",
    "        spoof = spoof.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        anti_spoofing_logits, domain_logits = model(embedding)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        _, predicted = torch.max(anti_spoofing_logits, 1)\n",
    "\n",
    "        # Append the true and predicted labels to the lists\n",
    "        true_labels += spoof.tolist()\n",
    "        predicted_labels += predicted.tolist()\n",
    "\n",
    "    # Calculate the evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    roc_auc = roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "    metrics = [accuracy, precision, recall, f1, roc_auc]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: 0, Fold 1\n",
      "Epoch [1/4], Loss_main: -0.18723362684249878, Loss_domain: 0.6874615550041199\n",
      "Epoch [2/4], Loss_main: -0.2076023519039154, Loss_domain: 0.675388514995575\n",
      "Epoch [3/4], Loss_main: -0.17381122708320618, Loss_domain: 0.659299910068512\n",
      "Epoch [4/4], Loss_main: -0.22934910655021667, Loss_domain: 0.6402658820152283\n",
      "Training complete for fold 1\n",
      "Domain: 0, Fold 2\n",
      "Epoch [1/4], Loss_main: -0.14696425199508667, Loss_domain: 0.6860234141349792\n",
      "Epoch [2/4], Loss_main: -0.22533896565437317, Loss_domain: 0.67408686876297\n",
      "Epoch [3/4], Loss_main: -0.30919480323791504, Loss_domain: 0.6550109386444092\n",
      "Epoch [4/4], Loss_main: -0.21617957949638367, Loss_domain: 0.638133704662323\n",
      "Training complete for fold 2\n",
      "Domain: 0, Fold 3\n",
      "Epoch [1/4], Loss_main: -0.10998457670211792, Loss_domain: 0.6758205890655518\n",
      "Epoch [2/4], Loss_main: -0.20072174072265625, Loss_domain: 0.6580411791801453\n",
      "Epoch [3/4], Loss_main: -0.12752848863601685, Loss_domain: 0.63765549659729\n",
      "Epoch [4/4], Loss_main: -0.15169402956962585, Loss_domain: 0.61913001537323\n",
      "Training complete for fold 3\n",
      "Domain: 0, Fold 4\n",
      "Epoch [1/4], Loss_main: -0.26304757595062256, Loss_domain: 0.7255219221115112\n",
      "Epoch [2/4], Loss_main: -0.21480965614318848, Loss_domain: 0.705118715763092\n",
      "Epoch [3/4], Loss_main: -0.22911038994789124, Loss_domain: 0.6881216764450073\n",
      "Epoch [4/4], Loss_main: -0.25224563479423523, Loss_domain: 0.6816453337669373\n",
      "Training complete for fold 4\n",
      "Domain: 0, Fold 5\n",
      "Epoch [1/4], Loss_main: -0.07584893703460693, Loss_domain: 0.6281323432922363\n",
      "Epoch [2/4], Loss_main: -0.0761178731918335, Loss_domain: 0.6205652952194214\n",
      "Epoch [3/4], Loss_main: -0.1562555730342865, Loss_domain: 0.6100083589553833\n",
      "Epoch [4/4], Loss_main: -0.09588024020195007, Loss_domain: 0.5821135640144348\n",
      "Training complete for fold 5\n",
      "Domain: 1, Fold 1\n",
      "Epoch [1/4], Loss_main: -0.250199556350708, Loss_domain: 0.755068838596344\n",
      "Epoch [2/4], Loss_main: -0.24521350860595703, Loss_domain: 0.7367857694625854\n",
      "Epoch [3/4], Loss_main: -0.20912709832191467, Loss_domain: 0.7172337770462036\n",
      "Epoch [4/4], Loss_main: -0.24708038568496704, Loss_domain: 0.6988050937652588\n",
      "Training complete for fold 1\n",
      "Domain: 1, Fold 2\n",
      "Epoch [1/4], Loss_main: -0.199382483959198, Loss_domain: 0.7078431844711304\n",
      "Epoch [2/4], Loss_main: -0.2701394557952881, Loss_domain: 0.69720858335495\n",
      "Epoch [3/4], Loss_main: -0.19760805368423462, Loss_domain: 0.6794225573539734\n",
      "Epoch [4/4], Loss_main: -0.28535351157188416, Loss_domain: 0.6614887714385986\n",
      "Training complete for fold 2\n",
      "Domain: 1, Fold 3\n",
      "Epoch [1/4], Loss_main: -0.2858823239803314, Loss_domain: 0.7807943820953369\n",
      "Epoch [2/4], Loss_main: -0.2943166494369507, Loss_domain: 0.7710484266281128\n",
      "Epoch [3/4], Loss_main: -0.2476567029953003, Loss_domain: 0.7551271319389343\n",
      "Epoch [4/4], Loss_main: -0.2541932463645935, Loss_domain: 0.737983226776123\n",
      "Training complete for fold 3\n",
      "Domain: 1, Fold 4\n",
      "Epoch [1/4], Loss_main: -0.16535118222236633, Loss_domain: 0.6633746027946472\n",
      "Epoch [2/4], Loss_main: -0.19388258457183838, Loss_domain: 0.6502496004104614\n",
      "Epoch [3/4], Loss_main: -0.1617247462272644, Loss_domain: 0.6357133388519287\n",
      "Epoch [4/4], Loss_main: -0.1407761573791504, Loss_domain: 0.6220418214797974\n",
      "Training complete for fold 4\n",
      "Domain: 1, Fold 5\n",
      "Epoch [1/4], Loss_main: -0.2309129238128662, Loss_domain: 0.7911370396614075\n",
      "Epoch [2/4], Loss_main: -0.2942734360694885, Loss_domain: 0.7783198356628418\n",
      "Epoch [3/4], Loss_main: -0.35923656821250916, Loss_domain: 0.7565909624099731\n",
      "Epoch [4/4], Loss_main: -0.2701161503791809, Loss_domain: 0.7380563616752625\n",
      "Training complete for fold 5\n",
      "Domain: 2, Fold 1\n",
      "Epoch [1/4], Loss_main: -0.1755046546459198, Loss_domain: 0.6713176369667053\n",
      "Epoch [2/4], Loss_main: -0.19414985179901123, Loss_domain: 0.653011679649353\n",
      "Epoch [3/4], Loss_main: -0.11249983310699463, Loss_domain: 0.6408197283744812\n",
      "Epoch [4/4], Loss_main: -0.17391812801361084, Loss_domain: 0.6258299350738525\n",
      "Training complete for fold 1\n",
      "Domain: 2, Fold 2\n",
      "Epoch [1/4], Loss_main: -0.140758216381073, Loss_domain: 0.738206148147583\n",
      "Epoch [2/4], Loss_main: -0.20458215475082397, Loss_domain: 0.7180298566818237\n",
      "Epoch [3/4], Loss_main: -0.3373298943042755, Loss_domain: 0.7036042213439941\n",
      "Epoch [4/4], Loss_main: -0.2509970963001251, Loss_domain: 0.6908735632896423\n",
      "Training complete for fold 2\n",
      "Domain: 2, Fold 3\n",
      "Epoch [1/4], Loss_main: -0.20932865142822266, Loss_domain: 0.7618722915649414\n",
      "Epoch [2/4], Loss_main: -0.31598976254463196, Loss_domain: 0.7456220984458923\n",
      "Epoch [3/4], Loss_main: -0.23292580246925354, Loss_domain: 0.7220783829689026\n",
      "Epoch [4/4], Loss_main: -0.1767248511314392, Loss_domain: 0.6977992653846741\n",
      "Training complete for fold 3\n",
      "Domain: 2, Fold 4\n",
      "Epoch [1/4], Loss_main: -0.24397653341293335, Loss_domain: 0.7637486457824707\n",
      "Epoch [2/4], Loss_main: -0.23771774768829346, Loss_domain: 0.7438950538635254\n",
      "Epoch [3/4], Loss_main: -0.38530433177948, Loss_domain: 0.7328639626502991\n",
      "Epoch [4/4], Loss_main: -0.3590331971645355, Loss_domain: 0.714425265789032\n",
      "Training complete for fold 4\n",
      "Domain: 2, Fold 5\n",
      "Epoch [1/4], Loss_main: -0.14603358507156372, Loss_domain: 0.666867733001709\n",
      "Epoch [2/4], Loss_main: -0.2564490735530853, Loss_domain: 0.6487429738044739\n",
      "Epoch [3/4], Loss_main: -0.18254625797271729, Loss_domain: 0.6432404518127441\n",
      "Epoch [4/4], Loss_main: -0.21238061785697937, Loss_domain: 0.6233871579170227\n",
      "Training complete for fold 5\n",
      "Domain: 3, Fold 1\n",
      "Epoch [1/4], Loss_main: -0.17432010173797607, Loss_domain: 0.6923646926879883\n",
      "Epoch [2/4], Loss_main: -0.1874723732471466, Loss_domain: 0.6781728267669678\n",
      "Epoch [3/4], Loss_main: -0.1525963544845581, Loss_domain: 0.6606741547584534\n",
      "Epoch [4/4], Loss_main: -0.19633185863494873, Loss_domain: 0.6472671627998352\n",
      "Training complete for fold 1\n",
      "Domain: 3, Fold 2\n",
      "Epoch [1/4], Loss_main: -0.17160367965698242, Loss_domain: 0.6940390467643738\n",
      "Epoch [2/4], Loss_main: -0.21460238099098206, Loss_domain: 0.6797941327095032\n",
      "Epoch [3/4], Loss_main: -0.2097669243812561, Loss_domain: 0.6606631875038147\n",
      "Epoch [4/4], Loss_main: -0.30071479082107544, Loss_domain: 0.6418524384498596\n",
      "Training complete for fold 2\n",
      "Domain: 3, Fold 3\n",
      "Epoch [1/4], Loss_main: -0.16583770513534546, Loss_domain: 0.7455141544342041\n",
      "Epoch [2/4], Loss_main: -0.32096824049949646, Loss_domain: 0.7246634364128113\n",
      "Epoch [3/4], Loss_main: -0.22387462854385376, Loss_domain: 0.7080873250961304\n",
      "Epoch [4/4], Loss_main: -0.3001128137111664, Loss_domain: 0.6915071606636047\n",
      "Training complete for fold 3\n",
      "Domain: 3, Fold 4\n",
      "Epoch [1/4], Loss_main: -0.08204585313796997, Loss_domain: 0.6180757880210876\n",
      "Epoch [2/4], Loss_main: -0.19172194600105286, Loss_domain: 0.6027772426605225\n",
      "Epoch [3/4], Loss_main: -0.13336440920829773, Loss_domain: 0.5844083428382874\n",
      "Epoch [4/4], Loss_main: -0.17080190777778625, Loss_domain: 0.5724923014640808\n",
      "Training complete for fold 4\n",
      "Domain: 3, Fold 5\n",
      "Epoch [1/4], Loss_main: -0.1816006600856781, Loss_domain: 0.6725448369979858\n",
      "Epoch [2/4], Loss_main: -0.24419885873794556, Loss_domain: 0.646255373954773\n",
      "Epoch [3/4], Loss_main: -0.2752259075641632, Loss_domain: 0.6351935267448425\n",
      "Epoch [4/4], Loss_main: -0.19038540124893188, Loss_domain: 0.6094112396240234\n",
      "Training complete for fold 5\n",
      "Domain: 4, Fold 1\n",
      "Epoch [1/4], Loss_main: -0.133877694606781, Loss_domain: 0.6147716045379639\n",
      "Epoch [2/4], Loss_main: -0.17567279934883118, Loss_domain: 0.6028610467910767\n",
      "Epoch [3/4], Loss_main: -0.16941577196121216, Loss_domain: 0.5906772017478943\n",
      "Epoch [4/4], Loss_main: -0.1674671173095703, Loss_domain: 0.5771489143371582\n",
      "Training complete for fold 1\n",
      "Domain: 4, Fold 2\n",
      "Epoch [1/4], Loss_main: -0.1821247935295105, Loss_domain: 0.7171621918678284\n",
      "Epoch [2/4], Loss_main: -0.1649962067604065, Loss_domain: 0.703586220741272\n",
      "Epoch [3/4], Loss_main: -0.2581658661365509, Loss_domain: 0.6897720098495483\n",
      "Epoch [4/4], Loss_main: -0.299651175737381, Loss_domain: 0.670779287815094\n",
      "Training complete for fold 2\n",
      "Domain: 4, Fold 3\n",
      "Epoch [1/4], Loss_main: -0.1420900523662567, Loss_domain: 0.6347730159759521\n",
      "Epoch [2/4], Loss_main: -0.14507442712783813, Loss_domain: 0.62180495262146\n",
      "Epoch [3/4], Loss_main: -0.1665305197238922, Loss_domain: 0.6108459234237671\n",
      "Epoch [4/4], Loss_main: -0.19557401537895203, Loss_domain: 0.592333197593689\n",
      "Training complete for fold 3\n",
      "Domain: 4, Fold 4\n",
      "Epoch [1/4], Loss_main: -0.2449670433998108, Loss_domain: 0.7807270288467407\n",
      "Epoch [2/4], Loss_main: -0.28829431533813477, Loss_domain: 0.7571449279785156\n",
      "Epoch [3/4], Loss_main: -0.24390912055969238, Loss_domain: 0.7462051510810852\n",
      "Epoch [4/4], Loss_main: -0.225352942943573, Loss_domain: 0.7275875210762024\n",
      "Training complete for fold 4\n",
      "Domain: 4, Fold 5\n",
      "Epoch [1/4], Loss_main: -0.156365305185318, Loss_domain: 0.625778317451477\n",
      "Epoch [2/4], Loss_main: -0.10929113626480103, Loss_domain: 0.6147165894508362\n",
      "Epoch [3/4], Loss_main: -0.08143693208694458, Loss_domain: 0.599963366985321\n",
      "Epoch [4/4], Loss_main: -0.23022368550300598, Loss_domain: 0.5829347968101501\n",
      "Training complete for fold 5\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in range(5):\n",
    "    metrics = train(target_domain=i)\n",
    "    name = f'Random_interval5_training_{i}'\n",
    "    results[name] = metrics\n",
    "\n",
    "metrics = pd.read_csv('metrics.csv', index_col=0)\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC'])\n",
    "\n",
    "metrics = pd.concat([metrics, results_df])\n",
    "\n",
    "metrics.to_csv('metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('HundredClusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0}, {1}, {0, 1}, {2}, {0, 2}, {1, 2}, {0, 1, 2}, {3}, {0, 3}, {1, 3}, {0, 1, 3}, {2, 3}, {0, 2, 3}, {1, 2, 3}, {0, 1, 2, 3}, {4}, {0, 4}, {1, 4}, {0, 1, 4}, {2, 4}, {0, 2, 4}, {1, 2, 4}, {0, 1, 2, 4}, {3, 4}, {0, 3, 4}, {1, 3, 4}, {0, 1, 3, 4}, {2, 3, 4}, {0, 2, 3, 4}, {1, 2, 3, 4}]\n"
     ]
    }
   ],
   "source": [
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield {ss for mask, ss in zip(masks, s) if i & mask}\n",
    "\n",
    "subsets = list(powerset([i for i in range(5)]))\n",
    "\n",
    "# remove the full and empty set\n",
    "subsets = [x for x in subsets if x]\n",
    "subsets.remove({0,1,2,3,4})\n",
    "print(subsets)\n",
    "# print(len(subsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: {0}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.342884361743927, Loss_domain: 0.16027705371379852\n",
      "Epoch [2/3], Loss_main: 0.38096553087234497, Loss_domain: 0.06485865265130997\n",
      "Epoch [3/3], Loss_main: 0.4013586640357971, Loss_domain: 0.03605160862207413\n",
      "Training complete for fold 1\n",
      "Domain: {0}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.39563482999801636, Loss_domain: 0.1047213152050972\n",
      "Epoch [2/3], Loss_main: 0.443238228559494, Loss_domain: 0.05563052371144295\n",
      "Epoch [3/3], Loss_main: 0.5099105834960938, Loss_domain: 0.026765985414385796\n",
      "Training complete for fold 2\n",
      "Domain: {0}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.37540706992149353, Loss_domain: 0.14230605959892273\n",
      "Epoch [2/3], Loss_main: 0.45932862162590027, Loss_domain: 0.06985440105199814\n",
      "Epoch [3/3], Loss_main: 0.40205439925193787, Loss_domain: 0.03859870880842209\n",
      "Training complete for fold 3\n",
      "Domain: {0}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.39458411931991577, Loss_domain: 0.13456201553344727\n",
      "Epoch [2/3], Loss_main: 0.3555627465248108, Loss_domain: 0.052808042615652084\n",
      "Epoch [3/3], Loss_main: 0.3300926387310028, Loss_domain: 0.03028666414320469\n",
      "Training complete for fold 4\n",
      "Domain: {0}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.370656818151474, Loss_domain: 0.17580220103263855\n",
      "Epoch [2/3], Loss_main: 0.37714192271232605, Loss_domain: 0.06988835334777832\n",
      "Epoch [3/3], Loss_main: 0.42914462089538574, Loss_domain: 0.04034282639622688\n",
      "Training complete for fold 5\n",
      "Domain: {1}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.4225286841392517, Loss_domain: 0.1601424664258957\n",
      "Epoch [2/3], Loss_main: 0.4358278214931488, Loss_domain: 0.0665002167224884\n",
      "Epoch [3/3], Loss_main: 0.348580002784729, Loss_domain: 0.0360104963183403\n",
      "Training complete for fold 1\n",
      "Domain: {1}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3235998749732971, Loss_domain: 0.15924081206321716\n",
      "Epoch [2/3], Loss_main: 0.40181073546409607, Loss_domain: 0.06680040806531906\n",
      "Epoch [3/3], Loss_main: 0.3926204442977905, Loss_domain: 0.038164202123880386\n",
      "Training complete for fold 2\n",
      "Domain: {1}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3492480516433716, Loss_domain: 0.15752343833446503\n",
      "Epoch [2/3], Loss_main: 0.36937326192855835, Loss_domain: 0.060832660645246506\n",
      "Epoch [3/3], Loss_main: 0.4537489116191864, Loss_domain: 0.03274116292595863\n",
      "Training complete for fold 3\n",
      "Domain: {1}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.35560762882232666, Loss_domain: 0.19413013756275177\n",
      "Epoch [2/3], Loss_main: 0.39312201738357544, Loss_domain: 0.08148111402988434\n",
      "Epoch [3/3], Loss_main: 0.3700839579105377, Loss_domain: 0.046189308166503906\n",
      "Training complete for fold 4\n",
      "Domain: {1}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.4130737781524658, Loss_domain: 0.13910239934921265\n",
      "Epoch [2/3], Loss_main: 0.4522685408592224, Loss_domain: 0.05550694093108177\n",
      "Epoch [3/3], Loss_main: 0.3217916786670685, Loss_domain: 0.029674245044589043\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3322749733924866, Loss_domain: 0.23041220009326935\n",
      "Epoch [2/3], Loss_main: 0.34693190455436707, Loss_domain: 0.11067283153533936\n",
      "Epoch [3/3], Loss_main: 0.4367952346801758, Loss_domain: 0.06234065443277359\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3140040636062622, Loss_domain: 0.179125115275383\n",
      "Epoch [2/3], Loss_main: 0.38242796063423157, Loss_domain: 0.06968242675065994\n",
      "Epoch [3/3], Loss_main: 0.38770586252212524, Loss_domain: 0.04552384838461876\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3994058072566986, Loss_domain: 0.16976365447044373\n",
      "Epoch [2/3], Loss_main: 0.40117186307907104, Loss_domain: 0.06952103972434998\n",
      "Epoch [3/3], Loss_main: 0.5456827282905579, Loss_domain: 0.04437726363539696\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3708788752555847, Loss_domain: 0.15098808705806732\n",
      "Epoch [2/3], Loss_main: 0.34105536341667175, Loss_domain: 0.0781741514801979\n",
      "Epoch [3/3], Loss_main: 0.4202427268028259, Loss_domain: 0.03820659965276718\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.36502605676651, Loss_domain: 0.15522177517414093\n",
      "Epoch [2/3], Loss_main: 0.3899751603603363, Loss_domain: 0.07657480984926224\n",
      "Epoch [3/3], Loss_main: 0.49437230825424194, Loss_domain: 0.036669857800006866\n",
      "Training complete for fold 5\n",
      "Domain: {2}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3989788293838501, Loss_domain: 0.14857815206050873\n",
      "Epoch [2/3], Loss_main: 0.3793472647666931, Loss_domain: 0.07594027370214462\n",
      "Epoch [3/3], Loss_main: 0.3463188409805298, Loss_domain: 0.042252253741025925\n",
      "Training complete for fold 1\n",
      "Domain: {2}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3943098783493042, Loss_domain: 0.12718652188777924\n",
      "Epoch [2/3], Loss_main: 0.34848690032958984, Loss_domain: 0.05203479900956154\n",
      "Epoch [3/3], Loss_main: 0.40442004799842834, Loss_domain: 0.033879321068525314\n",
      "Training complete for fold 2\n",
      "Domain: {2}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.4397733509540558, Loss_domain: 0.11167659610509872\n",
      "Epoch [2/3], Loss_main: 0.4816715717315674, Loss_domain: 0.042269520461559296\n",
      "Epoch [3/3], Loss_main: 0.48893189430236816, Loss_domain: 0.02452259510755539\n",
      "Training complete for fold 3\n",
      "Domain: {2}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.34794849157333374, Loss_domain: 0.18265850841999054\n",
      "Epoch [2/3], Loss_main: 0.3171221613883972, Loss_domain: 0.08422230184078217\n",
      "Epoch [3/3], Loss_main: 0.4435265064239502, Loss_domain: 0.0526195764541626\n",
      "Training complete for fold 4\n",
      "Domain: {2}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.2969762980937958, Loss_domain: 0.1988828480243683\n",
      "Epoch [2/3], Loss_main: 0.38541179895401, Loss_domain: 0.08412650972604752\n",
      "Epoch [3/3], Loss_main: 0.3268076479434967, Loss_domain: 0.05285900458693504\n",
      "Training complete for fold 5\n",
      "Domain: {0, 2}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.2930804491043091, Loss_domain: 0.19050174951553345\n",
      "Epoch [2/3], Loss_main: 0.3442145586013794, Loss_domain: 0.09747631847858429\n",
      "Epoch [3/3], Loss_main: 0.3536628186702728, Loss_domain: 0.045500390231609344\n",
      "Training complete for fold 1\n",
      "Domain: {0, 2}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.42577534914016724, Loss_domain: 0.1469241976737976\n",
      "Epoch [2/3], Loss_main: 0.5026815533638, Loss_domain: 0.0609273724257946\n",
      "Epoch [3/3], Loss_main: 0.3943844139575958, Loss_domain: 0.03424040600657463\n",
      "Training complete for fold 2\n",
      "Domain: {0, 2}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.4332381784915924, Loss_domain: 0.10912749916315079\n",
      "Epoch [2/3], Loss_main: 0.3853822946548462, Loss_domain: 0.043634600937366486\n",
      "Epoch [3/3], Loss_main: 0.3856363594532013, Loss_domain: 0.02676871418952942\n",
      "Training complete for fold 3\n",
      "Domain: {0, 2}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.355194628238678, Loss_domain: 0.14365245401859283\n",
      "Epoch [2/3], Loss_main: 0.4109721779823303, Loss_domain: 0.062034446746110916\n",
      "Epoch [3/3], Loss_main: 0.43441712856292725, Loss_domain: 0.03934818506240845\n",
      "Training complete for fold 4\n",
      "Domain: {0, 2}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.4233976900577545, Loss_domain: 0.11584755778312683\n",
      "Epoch [2/3], Loss_main: 0.39222678542137146, Loss_domain: 0.05279003828763962\n",
      "Epoch [3/3], Loss_main: 0.48016729950904846, Loss_domain: 0.03392050787806511\n",
      "Training complete for fold 5\n",
      "Domain: {1, 2}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3875448703765869, Loss_domain: 0.1571858674287796\n",
      "Epoch [2/3], Loss_main: 0.4596617519855499, Loss_domain: 0.06670693308115005\n",
      "Epoch [3/3], Loss_main: 0.4593668282032013, Loss_domain: 0.033773355185985565\n",
      "Training complete for fold 1\n",
      "Domain: {1, 2}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.43102484941482544, Loss_domain: 0.16152416169643402\n",
      "Epoch [2/3], Loss_main: 0.4437929689884186, Loss_domain: 0.06795445829629898\n",
      "Epoch [3/3], Loss_main: 0.3420228064060211, Loss_domain: 0.04639924317598343\n",
      "Training complete for fold 2\n",
      "Domain: {1, 2}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.344595342874527, Loss_domain: 0.14831367135047913\n",
      "Epoch [2/3], Loss_main: 0.3286605477333069, Loss_domain: 0.06647361814975739\n",
      "Epoch [3/3], Loss_main: 0.432282030582428, Loss_domain: 0.03501035273075104\n",
      "Training complete for fold 3\n",
      "Domain: {1, 2}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3931930661201477, Loss_domain: 0.1450682431459427\n",
      "Epoch [2/3], Loss_main: 0.28649795055389404, Loss_domain: 0.05603742226958275\n",
      "Epoch [3/3], Loss_main: 0.43852898478507996, Loss_domain: 0.032504212111234665\n",
      "Training complete for fold 4\n",
      "Domain: {1, 2}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.34899431467056274, Loss_domain: 0.14578227698802948\n",
      "Epoch [2/3], Loss_main: 0.39962995052337646, Loss_domain: 0.06088189408183098\n",
      "Epoch [3/3], Loss_main: 0.39896172285079956, Loss_domain: 0.03280024230480194\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 2}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3977917432785034, Loss_domain: 0.14315910637378693\n",
      "Epoch [2/3], Loss_main: 0.41364043951034546, Loss_domain: 0.06244385987520218\n",
      "Epoch [3/3], Loss_main: 0.3733387887477875, Loss_domain: 0.03572831302881241\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 2}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3544608950614929, Loss_domain: 0.13470283150672913\n",
      "Epoch [2/3], Loss_main: 0.3990345001220703, Loss_domain: 0.0585697703063488\n",
      "Epoch [3/3], Loss_main: 0.3871178925037384, Loss_domain: 0.03660818189382553\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 2}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.32770997285842896, Loss_domain: 0.1505347341299057\n",
      "Epoch [2/3], Loss_main: 0.427675724029541, Loss_domain: 0.05510997772216797\n",
      "Epoch [3/3], Loss_main: 0.42050448060035706, Loss_domain: 0.035566262900829315\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 2}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3634774684906006, Loss_domain: 0.14276719093322754\n",
      "Epoch [2/3], Loss_main: 0.38032394647598267, Loss_domain: 0.05944656953215599\n",
      "Epoch [3/3], Loss_main: 0.37934961915016174, Loss_domain: 0.03154934197664261\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 2}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.37977588176727295, Loss_domain: 0.14268280565738678\n",
      "Epoch [2/3], Loss_main: 0.44946756958961487, Loss_domain: 0.054707322269678116\n",
      "Epoch [3/3], Loss_main: 0.3582911193370819, Loss_domain: 0.03353910893201828\n",
      "Training complete for fold 5\n",
      "Domain: {3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.327452152967453, Loss_domain: 0.1713705062866211\n",
      "Epoch [2/3], Loss_main: 0.40827032923698425, Loss_domain: 0.08200345188379288\n",
      "Epoch [3/3], Loss_main: 0.44333580136299133, Loss_domain: 0.04743387550115585\n",
      "Training complete for fold 1\n",
      "Domain: {3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.4189727008342743, Loss_domain: 0.11540428549051285\n",
      "Epoch [2/3], Loss_main: 0.4263172149658203, Loss_domain: 0.04998566210269928\n",
      "Epoch [3/3], Loss_main: 0.37526753544807434, Loss_domain: 0.02879183739423752\n",
      "Training complete for fold 2\n",
      "Domain: {3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3766164183616638, Loss_domain: 0.19512587785720825\n",
      "Epoch [2/3], Loss_main: 0.4147266447544098, Loss_domain: 0.0862986147403717\n",
      "Epoch [3/3], Loss_main: 0.43621984124183655, Loss_domain: 0.05407743528485298\n",
      "Training complete for fold 3\n",
      "Domain: {3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.42183148860931396, Loss_domain: 0.13597773015499115\n",
      "Epoch [2/3], Loss_main: 0.39795005321502686, Loss_domain: 0.05479966476559639\n",
      "Epoch [3/3], Loss_main: 0.5227000117301941, Loss_domain: 0.02805917337536812\n",
      "Training complete for fold 4\n",
      "Domain: {3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3321616053581238, Loss_domain: 0.14753471314907074\n",
      "Epoch [2/3], Loss_main: 0.43382200598716736, Loss_domain: 0.06726953387260437\n",
      "Epoch [3/3], Loss_main: 0.38498350977897644, Loss_domain: 0.03601551428437233\n",
      "Training complete for fold 5\n",
      "Domain: {0, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.39774563908576965, Loss_domain: 0.13128164410591125\n",
      "Epoch [2/3], Loss_main: 0.45726191997528076, Loss_domain: 0.05461371690034866\n",
      "Epoch [3/3], Loss_main: 0.5181876420974731, Loss_domain: 0.02559295855462551\n",
      "Training complete for fold 1\n",
      "Domain: {0, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.32066598534584045, Loss_domain: 0.156999409198761\n",
      "Epoch [2/3], Loss_main: 0.4362192153930664, Loss_domain: 0.07055199146270752\n",
      "Epoch [3/3], Loss_main: 0.37521055340766907, Loss_domain: 0.04513067752122879\n",
      "Training complete for fold 2\n",
      "Domain: {0, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.42918193340301514, Loss_domain: 0.14737500250339508\n",
      "Epoch [2/3], Loss_main: 0.38854968547821045, Loss_domain: 0.0656382292509079\n",
      "Epoch [3/3], Loss_main: 0.46894633769989014, Loss_domain: 0.03581206500530243\n",
      "Training complete for fold 3\n",
      "Domain: {0, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.36503756046295166, Loss_domain: 0.14141803979873657\n",
      "Epoch [2/3], Loss_main: 0.3283742666244507, Loss_domain: 0.06556911766529083\n",
      "Epoch [3/3], Loss_main: 0.3601369559764862, Loss_domain: 0.03395015373826027\n",
      "Training complete for fold 4\n",
      "Domain: {0, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3381555676460266, Loss_domain: 0.16878144443035126\n",
      "Epoch [2/3], Loss_main: 0.3744536340236664, Loss_domain: 0.06280618906021118\n",
      "Epoch [3/3], Loss_main: 0.428895503282547, Loss_domain: 0.03597491979598999\n",
      "Training complete for fold 5\n",
      "Domain: {1, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.4279143214225769, Loss_domain: 0.13851362466812134\n",
      "Epoch [2/3], Loss_main: 0.470393568277359, Loss_domain: 0.05734070762991905\n",
      "Epoch [3/3], Loss_main: 0.4525853097438812, Loss_domain: 0.034924767911434174\n",
      "Training complete for fold 1\n",
      "Domain: {1, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3908514380455017, Loss_domain: 0.131709024310112\n",
      "Epoch [2/3], Loss_main: 0.39089423418045044, Loss_domain: 0.05813298001885414\n",
      "Epoch [3/3], Loss_main: 0.4585496783256531, Loss_domain: 0.03699948638677597\n",
      "Training complete for fold 2\n",
      "Domain: {1, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.42388954758644104, Loss_domain: 0.18624040484428406\n",
      "Epoch [2/3], Loss_main: 0.4412126839160919, Loss_domain: 0.08523640036582947\n",
      "Epoch [3/3], Loss_main: 0.36696857213974, Loss_domain: 0.04475143924355507\n",
      "Training complete for fold 3\n",
      "Domain: {1, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.311603844165802, Loss_domain: 0.1590299904346466\n",
      "Epoch [2/3], Loss_main: 0.44049689173698425, Loss_domain: 0.06773313879966736\n",
      "Epoch [3/3], Loss_main: 0.45204925537109375, Loss_domain: 0.028650663793087006\n",
      "Training complete for fold 4\n",
      "Domain: {1, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.31585246324539185, Loss_domain: 0.14084139466285706\n",
      "Epoch [2/3], Loss_main: 0.3780750632286072, Loss_domain: 0.06558296084403992\n",
      "Epoch [3/3], Loss_main: 0.4358713924884796, Loss_domain: 0.035404931753873825\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.2977018356323242, Loss_domain: 0.15130676329135895\n",
      "Epoch [2/3], Loss_main: 0.3221377730369568, Loss_domain: 0.06310092657804489\n",
      "Epoch [3/3], Loss_main: 0.38030189275741577, Loss_domain: 0.0362384095788002\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.38872087001800537, Loss_domain: 0.1666427105665207\n",
      "Epoch [2/3], Loss_main: 0.35943278670310974, Loss_domain: 0.06996046751737595\n",
      "Epoch [3/3], Loss_main: 0.41552871465682983, Loss_domain: 0.037181638181209564\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.40661710500717163, Loss_domain: 0.1385882943868637\n",
      "Epoch [2/3], Loss_main: 0.4246887266635895, Loss_domain: 0.05393685773015022\n",
      "Epoch [3/3], Loss_main: 0.5116384029388428, Loss_domain: 0.03188764676451683\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3876721262931824, Loss_domain: 0.13030217587947845\n",
      "Epoch [2/3], Loss_main: 0.398674339056015, Loss_domain: 0.060141146183013916\n",
      "Epoch [3/3], Loss_main: 0.3456289768218994, Loss_domain: 0.03632209450006485\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3927721381187439, Loss_domain: 0.1273983120918274\n",
      "Epoch [2/3], Loss_main: 0.4395105242729187, Loss_domain: 0.06191324070096016\n",
      "Epoch [3/3], Loss_main: 0.3761559724807739, Loss_domain: 0.03514270484447479\n",
      "Training complete for fold 5\n",
      "Domain: {2, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.33718639612197876, Loss_domain: 0.14801345765590668\n",
      "Epoch [2/3], Loss_main: 0.42792728543281555, Loss_domain: 0.06949935108423233\n",
      "Epoch [3/3], Loss_main: 0.42708638310432434, Loss_domain: 0.03459494560956955\n",
      "Training complete for fold 1\n",
      "Domain: {2, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.33005291223526, Loss_domain: 0.14219358563423157\n",
      "Epoch [2/3], Loss_main: 0.39107969403266907, Loss_domain: 0.055245038121938705\n",
      "Epoch [3/3], Loss_main: 0.3850593566894531, Loss_domain: 0.03402561694383621\n",
      "Training complete for fold 2\n",
      "Domain: {2, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.33558154106140137, Loss_domain: 0.1712321788072586\n",
      "Epoch [2/3], Loss_main: 0.4087311029434204, Loss_domain: 0.08833909034729004\n",
      "Epoch [3/3], Loss_main: 0.5034211277961731, Loss_domain: 0.05298526957631111\n",
      "Training complete for fold 3\n",
      "Domain: {2, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.34609153866767883, Loss_domain: 0.13936668634414673\n",
      "Epoch [2/3], Loss_main: 0.29810959100723267, Loss_domain: 0.0629243403673172\n",
      "Epoch [3/3], Loss_main: 0.3336695730686188, Loss_domain: 0.03366832807660103\n",
      "Training complete for fold 4\n",
      "Domain: {2, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.32722973823547363, Loss_domain: 0.13215431571006775\n",
      "Epoch [2/3], Loss_main: 0.35823357105255127, Loss_domain: 0.06052635237574577\n",
      "Epoch [3/3], Loss_main: 0.3938932716846466, Loss_domain: 0.03481867164373398\n",
      "Training complete for fold 5\n",
      "Domain: {0, 2, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.4163625240325928, Loss_domain: 0.12279313057661057\n",
      "Epoch [2/3], Loss_main: 0.4785798192024231, Loss_domain: 0.051939595490694046\n",
      "Epoch [3/3], Loss_main: 0.4841751456260681, Loss_domain: 0.02615046128630638\n",
      "Training complete for fold 1\n",
      "Domain: {0, 2, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.31487447023391724, Loss_domain: 0.1409161537885666\n",
      "Epoch [2/3], Loss_main: 0.49012014269828796, Loss_domain: 0.055397067219018936\n",
      "Epoch [3/3], Loss_main: 0.38828474283218384, Loss_domain: 0.03733087331056595\n",
      "Training complete for fold 2\n",
      "Domain: {0, 2, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3894660472869873, Loss_domain: 0.12372362613677979\n",
      "Epoch [2/3], Loss_main: 0.3581194579601288, Loss_domain: 0.06095709651708603\n",
      "Epoch [3/3], Loss_main: 0.39158540964126587, Loss_domain: 0.024926453828811646\n",
      "Training complete for fold 3\n",
      "Domain: {0, 2, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.40143391489982605, Loss_domain: 0.14243033528327942\n",
      "Epoch [2/3], Loss_main: 0.5209810137748718, Loss_domain: 0.07082711160182953\n",
      "Epoch [3/3], Loss_main: 0.45003190636634827, Loss_domain: 0.039313655346632004\n",
      "Training complete for fold 4\n",
      "Domain: {0, 2, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.34723392128944397, Loss_domain: 0.17492881417274475\n",
      "Epoch [2/3], Loss_main: 0.3401470482349396, Loss_domain: 0.0721210241317749\n",
      "Epoch [3/3], Loss_main: 0.3889400064945221, Loss_domain: 0.040958795696496964\n",
      "Training complete for fold 5\n",
      "Domain: {1, 2, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3778797686100006, Loss_domain: 0.12539377808570862\n",
      "Epoch [2/3], Loss_main: 0.36679166555404663, Loss_domain: 0.06219387426972389\n",
      "Epoch [3/3], Loss_main: 0.5587149858474731, Loss_domain: 0.03774946555495262\n",
      "Training complete for fold 1\n",
      "Domain: {1, 2, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3848465085029602, Loss_domain: 0.12998981773853302\n",
      "Epoch [2/3], Loss_main: 0.42311757802963257, Loss_domain: 0.06280489265918732\n",
      "Epoch [3/3], Loss_main: 0.4130707085132599, Loss_domain: 0.03323349729180336\n",
      "Training complete for fold 2\n",
      "Domain: {1, 2, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3484151363372803, Loss_domain: 0.17216402292251587\n",
      "Epoch [2/3], Loss_main: 0.35350722074508667, Loss_domain: 0.06828215718269348\n",
      "Epoch [3/3], Loss_main: 0.43644875288009644, Loss_domain: 0.038461849093437195\n",
      "Training complete for fold 3\n",
      "Domain: {1, 2, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3353322446346283, Loss_domain: 0.16133543848991394\n",
      "Epoch [2/3], Loss_main: 0.4556187689304352, Loss_domain: 0.07810495048761368\n",
      "Epoch [3/3], Loss_main: 0.33030974864959717, Loss_domain: 0.039530832320451736\n",
      "Training complete for fold 4\n",
      "Domain: {1, 2, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3935033082962036, Loss_domain: 0.1582966446876526\n",
      "Epoch [2/3], Loss_main: 0.46607333421707153, Loss_domain: 0.06538562476634979\n",
      "Epoch [3/3], Loss_main: 0.44223248958587646, Loss_domain: 0.035245660692453384\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 2, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3405487537384033, Loss_domain: 0.21365875005722046\n",
      "Epoch [2/3], Loss_main: 0.39781591296195984, Loss_domain: 0.08899242430925369\n",
      "Epoch [3/3], Loss_main: 0.4212818741798401, Loss_domain: 0.05518876761198044\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 2, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.40010079741477966, Loss_domain: 0.12423649430274963\n",
      "Epoch [2/3], Loss_main: 0.3285403847694397, Loss_domain: 0.053869783878326416\n",
      "Epoch [3/3], Loss_main: 0.4971540868282318, Loss_domain: 0.029330356046557426\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 2, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.37698981165885925, Loss_domain: 0.12747910618782043\n",
      "Epoch [2/3], Loss_main: 0.40242648124694824, Loss_domain: 0.04997639358043671\n",
      "Epoch [3/3], Loss_main: 0.31476306915283203, Loss_domain: 0.032343052327632904\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 2, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3543672561645508, Loss_domain: 0.11256448924541473\n",
      "Epoch [2/3], Loss_main: 0.46523338556289673, Loss_domain: 0.052505262196063995\n",
      "Epoch [3/3], Loss_main: 0.3798759877681732, Loss_domain: 0.028884174302220345\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 2, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.39615797996520996, Loss_domain: 0.12415234744548798\n",
      "Epoch [2/3], Loss_main: 0.4443715810775757, Loss_domain: 0.057940948754549026\n",
      "Epoch [3/3], Loss_main: 0.3259168863296509, Loss_domain: 0.03349299356341362\n",
      "Training complete for fold 5\n",
      "Domain: {4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.36739304661750793, Loss_domain: 0.15919968485832214\n",
      "Epoch [2/3], Loss_main: 0.412930428981781, Loss_domain: 0.06555305421352386\n",
      "Epoch [3/3], Loss_main: 0.42039355635643005, Loss_domain: 0.03810282424092293\n",
      "Training complete for fold 1\n",
      "Domain: {4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3623477816581726, Loss_domain: 0.1559787392616272\n",
      "Epoch [2/3], Loss_main: 0.43209224939346313, Loss_domain: 0.06640183925628662\n",
      "Epoch [3/3], Loss_main: 0.42398712038993835, Loss_domain: 0.04198571667075157\n",
      "Training complete for fold 2\n",
      "Domain: {4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.4063560366630554, Loss_domain: 0.19414283335208893\n",
      "Epoch [2/3], Loss_main: 0.4571065902709961, Loss_domain: 0.08614318817853928\n",
      "Epoch [3/3], Loss_main: 0.4012896418571472, Loss_domain: 0.03986399993300438\n",
      "Training complete for fold 3\n",
      "Domain: {4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3368059992790222, Loss_domain: 0.15220992267131805\n",
      "Epoch [2/3], Loss_main: 0.5762848854064941, Loss_domain: 0.07239183783531189\n",
      "Epoch [3/3], Loss_main: 0.4291568696498871, Loss_domain: 0.036255426704883575\n",
      "Training complete for fold 4\n",
      "Domain: {4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3793448805809021, Loss_domain: 0.1268150955438614\n",
      "Epoch [2/3], Loss_main: 0.4218886196613312, Loss_domain: 0.0566941499710083\n",
      "Epoch [3/3], Loss_main: 0.3581840395927429, Loss_domain: 0.03201134130358696\n",
      "Training complete for fold 5\n",
      "Domain: {0, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.343141108751297, Loss_domain: 0.11134939640760422\n",
      "Epoch [2/3], Loss_main: 0.36374667286872864, Loss_domain: 0.04768675938248634\n",
      "Epoch [3/3], Loss_main: 0.4359520673751831, Loss_domain: 0.018694844096899033\n",
      "Training complete for fold 1\n",
      "Domain: {0, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.355682373046875, Loss_domain: 0.1605730503797531\n",
      "Epoch [2/3], Loss_main: 0.461206316947937, Loss_domain: 0.0637252926826477\n",
      "Epoch [3/3], Loss_main: 0.3607715368270874, Loss_domain: 0.03735371306538582\n",
      "Training complete for fold 2\n",
      "Domain: {0, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.35193532705307007, Loss_domain: 0.16237188875675201\n",
      "Epoch [2/3], Loss_main: 0.41634389758110046, Loss_domain: 0.08494338393211365\n",
      "Epoch [3/3], Loss_main: 0.42513513565063477, Loss_domain: 0.04498542472720146\n",
      "Training complete for fold 3\n",
      "Domain: {0, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.32757148146629333, Loss_domain: 0.11382926255464554\n",
      "Epoch [2/3], Loss_main: 0.38871103525161743, Loss_domain: 0.04872173070907593\n",
      "Epoch [3/3], Loss_main: 0.4346486032009125, Loss_domain: 0.03170282393693924\n",
      "Training complete for fold 4\n",
      "Domain: {0, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.38189229369163513, Loss_domain: 0.11528695374727249\n",
      "Epoch [2/3], Loss_main: 0.3782927691936493, Loss_domain: 0.05032059922814369\n",
      "Epoch [3/3], Loss_main: 0.40827566385269165, Loss_domain: 0.026138752698898315\n",
      "Training complete for fold 5\n",
      "Domain: {1, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3639334440231323, Loss_domain: 0.17281381785869598\n",
      "Epoch [2/3], Loss_main: 0.37231969833374023, Loss_domain: 0.07737068086862564\n",
      "Epoch [3/3], Loss_main: 0.3614838421344757, Loss_domain: 0.04506311193108559\n",
      "Training complete for fold 1\n",
      "Domain: {1, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.4101489782333374, Loss_domain: 0.12302099913358688\n",
      "Epoch [2/3], Loss_main: 0.39865899085998535, Loss_domain: 0.055966682732105255\n",
      "Epoch [3/3], Loss_main: 0.3654099702835083, Loss_domain: 0.030753763392567635\n",
      "Training complete for fold 2\n",
      "Domain: {1, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.36348262429237366, Loss_domain: 0.16121670603752136\n",
      "Epoch [2/3], Loss_main: 0.4114993214607239, Loss_domain: 0.07402960956096649\n",
      "Epoch [3/3], Loss_main: 0.4086475074291229, Loss_domain: 0.03821039944887161\n",
      "Training complete for fold 3\n",
      "Domain: {1, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.4023332893848419, Loss_domain: 0.11910942941904068\n",
      "Epoch [2/3], Loss_main: 0.4055456519126892, Loss_domain: 0.057677652686834335\n",
      "Epoch [3/3], Loss_main: 0.42044469714164734, Loss_domain: 0.030216781422495842\n",
      "Training complete for fold 4\n",
      "Domain: {1, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3581113815307617, Loss_domain: 0.13854385912418365\n",
      "Epoch [2/3], Loss_main: 0.46857011318206787, Loss_domain: 0.05823655053973198\n",
      "Epoch [3/3], Loss_main: 0.4196709096431732, Loss_domain: 0.033533744513988495\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3902996778488159, Loss_domain: 0.16397446393966675\n",
      "Epoch [2/3], Loss_main: 0.35796505212783813, Loss_domain: 0.07350829243659973\n",
      "Epoch [3/3], Loss_main: 0.39927756786346436, Loss_domain: 0.0346854142844677\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.36213865876197815, Loss_domain: 0.1558552086353302\n",
      "Epoch [2/3], Loss_main: 0.5089906454086304, Loss_domain: 0.07242897152900696\n",
      "Epoch [3/3], Loss_main: 0.3359871506690979, Loss_domain: 0.03643012046813965\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3401402235031128, Loss_domain: 0.1656249612569809\n",
      "Epoch [2/3], Loss_main: 0.4377480149269104, Loss_domain: 0.07291863858699799\n",
      "Epoch [3/3], Loss_main: 0.34594741463661194, Loss_domain: 0.04330722242593765\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.28307825326919556, Loss_domain: 0.21845583617687225\n",
      "Epoch [2/3], Loss_main: 0.38129550218582153, Loss_domain: 0.1032630056142807\n",
      "Epoch [3/3], Loss_main: 0.3821524977684021, Loss_domain: 0.050130054354667664\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.32226261496543884, Loss_domain: 0.18832537531852722\n",
      "Epoch [2/3], Loss_main: 0.38165217638015747, Loss_domain: 0.0910770446062088\n",
      "Epoch [3/3], Loss_main: 0.4099384844303131, Loss_domain: 0.04650557413697243\n",
      "Training complete for fold 5\n",
      "Domain: {2, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.30866900086402893, Loss_domain: 0.16113489866256714\n",
      "Epoch [2/3], Loss_main: 0.3775630593299866, Loss_domain: 0.07163836807012558\n",
      "Epoch [3/3], Loss_main: 0.4215542674064636, Loss_domain: 0.03746248781681061\n",
      "Training complete for fold 1\n",
      "Domain: {2, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.36941415071487427, Loss_domain: 0.13315285742282867\n",
      "Epoch [2/3], Loss_main: 0.37716561555862427, Loss_domain: 0.06063465029001236\n",
      "Epoch [3/3], Loss_main: 0.33565863966941833, Loss_domain: 0.03212523087859154\n",
      "Training complete for fold 2\n",
      "Domain: {2, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.31597062945365906, Loss_domain: 0.15598297119140625\n",
      "Epoch [2/3], Loss_main: 0.37319648265838623, Loss_domain: 0.06687726080417633\n",
      "Epoch [3/3], Loss_main: 0.49224838614463806, Loss_domain: 0.033161718398332596\n",
      "Training complete for fold 3\n",
      "Domain: {2, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.35863858461380005, Loss_domain: 0.1648082137107849\n",
      "Epoch [2/3], Loss_main: 0.35843053460121155, Loss_domain: 0.054568685591220856\n",
      "Epoch [3/3], Loss_main: 0.4360114336013794, Loss_domain: 0.03785412013530731\n",
      "Training complete for fold 4\n",
      "Domain: {2, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.350469708442688, Loss_domain: 0.1343727558851242\n",
      "Epoch [2/3], Loss_main: 0.36632901430130005, Loss_domain: 0.0638340562582016\n",
      "Epoch [3/3], Loss_main: 0.32867011427879333, Loss_domain: 0.03770100325345993\n",
      "Training complete for fold 5\n",
      "Domain: {0, 2, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3453032374382019, Loss_domain: 0.1798122674226761\n",
      "Epoch [2/3], Loss_main: 0.4178342819213867, Loss_domain: 0.07266966998577118\n",
      "Epoch [3/3], Loss_main: 0.3832200765609741, Loss_domain: 0.04517771303653717\n",
      "Training complete for fold 1\n",
      "Domain: {0, 2, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3763276934623718, Loss_domain: 0.1226348876953125\n",
      "Epoch [2/3], Loss_main: 0.3569090664386749, Loss_domain: 0.05713510140776634\n",
      "Epoch [3/3], Loss_main: 0.3501705825328827, Loss_domain: 0.03396648168563843\n",
      "Training complete for fold 2\n",
      "Domain: {0, 2, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3763655424118042, Loss_domain: 0.10427466779947281\n",
      "Epoch [2/3], Loss_main: 0.326342910528183, Loss_domain: 0.042987626045942307\n",
      "Epoch [3/3], Loss_main: 0.43006250262260437, Loss_domain: 0.02566027268767357\n",
      "Training complete for fold 3\n",
      "Domain: {0, 2, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.44386571645736694, Loss_domain: 0.12196684628725052\n",
      "Epoch [2/3], Loss_main: 0.479810506105423, Loss_domain: 0.04884801432490349\n",
      "Epoch [3/3], Loss_main: 0.34255900979042053, Loss_domain: 0.02633720636367798\n",
      "Training complete for fold 4\n",
      "Domain: {0, 2, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.28042471408843994, Loss_domain: 0.169718936085701\n",
      "Epoch [2/3], Loss_main: 0.44298920035362244, Loss_domain: 0.06339874863624573\n",
      "Epoch [3/3], Loss_main: 0.324737548828125, Loss_domain: 0.03666189685463905\n",
      "Training complete for fold 5\n",
      "Domain: {1, 2, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.35565727949142456, Loss_domain: 0.16947001218795776\n",
      "Epoch [2/3], Loss_main: 0.3443673253059387, Loss_domain: 0.07589273154735565\n",
      "Epoch [3/3], Loss_main: 0.4183711111545563, Loss_domain: 0.04214949160814285\n",
      "Training complete for fold 1\n",
      "Domain: {1, 2, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3631996512413025, Loss_domain: 0.13726694881916046\n",
      "Epoch [2/3], Loss_main: 0.44614169001579285, Loss_domain: 0.057174526154994965\n",
      "Epoch [3/3], Loss_main: 0.464408278465271, Loss_domain: 0.03967630863189697\n",
      "Training complete for fold 2\n",
      "Domain: {1, 2, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3500734567642212, Loss_domain: 0.13226275146007538\n",
      "Epoch [2/3], Loss_main: 0.40853598713874817, Loss_domain: 0.06152012199163437\n",
      "Epoch [3/3], Loss_main: 0.4694114029407501, Loss_domain: 0.039904508739709854\n",
      "Training complete for fold 3\n",
      "Domain: {1, 2, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.34792160987854004, Loss_domain: 0.16813330352306366\n",
      "Epoch [2/3], Loss_main: 0.3965964913368225, Loss_domain: 0.07102010399103165\n",
      "Epoch [3/3], Loss_main: 0.432343989610672, Loss_domain: 0.035988032817840576\n",
      "Training complete for fold 4\n",
      "Domain: {1, 2, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.39853590726852417, Loss_domain: 0.1268593668937683\n",
      "Epoch [2/3], Loss_main: 0.4736884832382202, Loss_domain: 0.05416859686374664\n",
      "Epoch [3/3], Loss_main: 0.5183688402175903, Loss_domain: 0.032033491879701614\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 2, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.41103246808052063, Loss_domain: 0.12251565605401993\n",
      "Epoch [2/3], Loss_main: 0.42319896817207336, Loss_domain: 0.048373542726039886\n",
      "Epoch [3/3], Loss_main: 0.41732078790664673, Loss_domain: 0.026200775057077408\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 2, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.4304339289665222, Loss_domain: 0.15461592376232147\n",
      "Epoch [2/3], Loss_main: 0.4048375189304352, Loss_domain: 0.053311701864004135\n",
      "Epoch [3/3], Loss_main: 0.39244791865348816, Loss_domain: 0.0334613211452961\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 2, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3229966163635254, Loss_domain: 0.14345450699329376\n",
      "Epoch [2/3], Loss_main: 0.3961339592933655, Loss_domain: 0.06734989583492279\n",
      "Epoch [3/3], Loss_main: 0.4528326392173767, Loss_domain: 0.037826571613550186\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 2, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.363748162984848, Loss_domain: 0.147207111120224\n",
      "Epoch [2/3], Loss_main: 0.39445000886917114, Loss_domain: 0.05470823124051094\n",
      "Epoch [3/3], Loss_main: 0.44085627794265747, Loss_domain: 0.03276090323925018\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 2, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3660063147544861, Loss_domain: 0.10817432403564453\n",
      "Epoch [2/3], Loss_main: 0.5107788443565369, Loss_domain: 0.040814854204654694\n",
      "Epoch [3/3], Loss_main: 0.48891422152519226, Loss_domain: 0.024059977382421494\n",
      "Training complete for fold 5\n",
      "Domain: {3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3969613313674927, Loss_domain: 0.10486124455928802\n",
      "Epoch [2/3], Loss_main: 0.477113276720047, Loss_domain: 0.055063050240278244\n",
      "Epoch [3/3], Loss_main: 0.44655999541282654, Loss_domain: 0.02781110629439354\n",
      "Training complete for fold 1\n",
      "Domain: {3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.42475223541259766, Loss_domain: 0.12539590895175934\n",
      "Epoch [2/3], Loss_main: 0.40709662437438965, Loss_domain: 0.04838241636753082\n",
      "Epoch [3/3], Loss_main: 0.368721067905426, Loss_domain: 0.024306854233145714\n",
      "Training complete for fold 2\n",
      "Domain: {3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3810187578201294, Loss_domain: 0.218506321310997\n",
      "Epoch [2/3], Loss_main: 0.3511534035205841, Loss_domain: 0.10238209366798401\n",
      "Epoch [3/3], Loss_main: 0.38200464844703674, Loss_domain: 0.05514591187238693\n",
      "Training complete for fold 3\n",
      "Domain: {3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.42581236362457275, Loss_domain: 0.12324631959199905\n",
      "Epoch [2/3], Loss_main: 0.4055079221725464, Loss_domain: 0.04727619141340256\n",
      "Epoch [3/3], Loss_main: 0.37514346837997437, Loss_domain: 0.031532298773527145\n",
      "Training complete for fold 4\n",
      "Domain: {3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.35157087445259094, Loss_domain: 0.1745809018611908\n",
      "Epoch [2/3], Loss_main: 0.3670606017112732, Loss_domain: 0.07455848157405853\n",
      "Epoch [3/3], Loss_main: 0.4725932776927948, Loss_domain: 0.041394975036382675\n",
      "Training complete for fold 5\n",
      "Domain: {0, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.33627527952194214, Loss_domain: 0.1378890573978424\n",
      "Epoch [2/3], Loss_main: 0.4219552278518677, Loss_domain: 0.05978047102689743\n",
      "Epoch [3/3], Loss_main: 0.508001446723938, Loss_domain: 0.0256422720849514\n",
      "Training complete for fold 1\n",
      "Domain: {0, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.4464051425457001, Loss_domain: 0.18700876832008362\n",
      "Epoch [2/3], Loss_main: 0.41585591435432434, Loss_domain: 0.08902129530906677\n",
      "Epoch [3/3], Loss_main: 0.3487434983253479, Loss_domain: 0.04492674395442009\n",
      "Training complete for fold 2\n",
      "Domain: {0, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.37315282225608826, Loss_domain: 0.1253276765346527\n",
      "Epoch [2/3], Loss_main: 0.4164051413536072, Loss_domain: 0.05054410547018051\n",
      "Epoch [3/3], Loss_main: 0.445779025554657, Loss_domain: 0.033100709319114685\n",
      "Training complete for fold 3\n",
      "Domain: {0, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.37769657373428345, Loss_domain: 0.1649644821882248\n",
      "Epoch [2/3], Loss_main: 0.43773794174194336, Loss_domain: 0.08147399872541428\n",
      "Epoch [3/3], Loss_main: 0.41612711548805237, Loss_domain: 0.03634979948401451\n",
      "Training complete for fold 4\n",
      "Domain: {0, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.41540390253067017, Loss_domain: 0.11139707267284393\n",
      "Epoch [2/3], Loss_main: 0.4989642798900604, Loss_domain: 0.04550466313958168\n",
      "Epoch [3/3], Loss_main: 0.42301902174949646, Loss_domain: 0.028707893565297127\n",
      "Training complete for fold 5\n",
      "Domain: {1, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3753831386566162, Loss_domain: 0.1459953933954239\n",
      "Epoch [2/3], Loss_main: 0.45732125639915466, Loss_domain: 0.06086108833551407\n",
      "Epoch [3/3], Loss_main: 0.41560330986976624, Loss_domain: 0.03871581703424454\n",
      "Training complete for fold 1\n",
      "Domain: {1, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.29778289794921875, Loss_domain: 0.18237249553203583\n",
      "Epoch [2/3], Loss_main: 0.3710617423057556, Loss_domain: 0.07610075175762177\n",
      "Epoch [3/3], Loss_main: 0.3941943645477295, Loss_domain: 0.04214441403746605\n",
      "Training complete for fold 2\n",
      "Domain: {1, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.39481011033058167, Loss_domain: 0.14091947674751282\n",
      "Epoch [2/3], Loss_main: 0.4288572669029236, Loss_domain: 0.061462581157684326\n",
      "Epoch [3/3], Loss_main: 0.38386598229408264, Loss_domain: 0.033417847007513046\n",
      "Training complete for fold 3\n",
      "Domain: {1, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.41632556915283203, Loss_domain: 0.14324133098125458\n",
      "Epoch [2/3], Loss_main: 0.38772058486938477, Loss_domain: 0.05655250325798988\n",
      "Epoch [3/3], Loss_main: 0.3928050696849823, Loss_domain: 0.028185304254293442\n",
      "Training complete for fold 4\n",
      "Domain: {1, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3651087284088135, Loss_domain: 0.18778620660305023\n",
      "Epoch [2/3], Loss_main: 0.4259452819824219, Loss_domain: 0.07581456005573273\n",
      "Epoch [3/3], Loss_main: 0.4264083504676819, Loss_domain: 0.04937075078487396\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.2771388292312622, Loss_domain: 0.17923620343208313\n",
      "Epoch [2/3], Loss_main: 0.41598641872406006, Loss_domain: 0.08390085399150848\n",
      "Epoch [3/3], Loss_main: 0.32920244336128235, Loss_domain: 0.053593192249536514\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3824232220649719, Loss_domain: 0.1439174860715866\n",
      "Epoch [2/3], Loss_main: 0.4642099440097809, Loss_domain: 0.047445572912693024\n",
      "Epoch [3/3], Loss_main: 0.4081442654132843, Loss_domain: 0.03935892507433891\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.38874226808547974, Loss_domain: 0.14559249579906464\n",
      "Epoch [2/3], Loss_main: 0.47670286893844604, Loss_domain: 0.06264610588550568\n",
      "Epoch [3/3], Loss_main: 0.4269065260887146, Loss_domain: 0.04267589747905731\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3684728443622589, Loss_domain: 0.1368217170238495\n",
      "Epoch [2/3], Loss_main: 0.3396925628185272, Loss_domain: 0.0636957660317421\n",
      "Epoch [3/3], Loss_main: 0.48694759607315063, Loss_domain: 0.034028053283691406\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.2957574129104614, Loss_domain: 0.14404241740703583\n",
      "Epoch [2/3], Loss_main: 0.37339508533477783, Loss_domain: 0.059684328734874725\n",
      "Epoch [3/3], Loss_main: 0.4260142147541046, Loss_domain: 0.03148597851395607\n",
      "Training complete for fold 5\n",
      "Domain: {2, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.27195245027542114, Loss_domain: 0.19978497922420502\n",
      "Epoch [2/3], Loss_main: 0.4850306510925293, Loss_domain: 0.08837520331144333\n",
      "Epoch [3/3], Loss_main: 0.37319210171699524, Loss_domain: 0.04828913137316704\n",
      "Training complete for fold 1\n",
      "Domain: {2, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.32880017161369324, Loss_domain: 0.17193159461021423\n",
      "Epoch [2/3], Loss_main: 0.3863670825958252, Loss_domain: 0.06696151196956635\n",
      "Epoch [3/3], Loss_main: 0.43252885341644287, Loss_domain: 0.043316178023815155\n",
      "Training complete for fold 2\n",
      "Domain: {2, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.33415526151657104, Loss_domain: 0.11742056906223297\n",
      "Epoch [2/3], Loss_main: 0.4521123766899109, Loss_domain: 0.050397347658872604\n",
      "Epoch [3/3], Loss_main: 0.38214409351348877, Loss_domain: 0.02832728810608387\n",
      "Training complete for fold 3\n",
      "Domain: {2, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.35445636510849, Loss_domain: 0.18095755577087402\n",
      "Epoch [2/3], Loss_main: 0.3871627449989319, Loss_domain: 0.08593134582042694\n",
      "Epoch [3/3], Loss_main: 0.46317028999328613, Loss_domain: 0.04257421940565109\n",
      "Training complete for fold 4\n",
      "Domain: {2, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.32941365242004395, Loss_domain: 0.15171363949775696\n",
      "Epoch [2/3], Loss_main: 0.4356786608695984, Loss_domain: 0.05652058497071266\n",
      "Epoch [3/3], Loss_main: 0.28523871302604675, Loss_domain: 0.043962057679891586\n",
      "Training complete for fold 5\n",
      "Domain: {0, 2, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.4431840479373932, Loss_domain: 0.1172771155834198\n",
      "Epoch [2/3], Loss_main: 0.43182557821273804, Loss_domain: 0.0537654347717762\n",
      "Epoch [3/3], Loss_main: 0.3898219168186188, Loss_domain: 0.029402000829577446\n",
      "Training complete for fold 1\n",
      "Domain: {0, 2, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.39215922355651855, Loss_domain: 0.16319884359836578\n",
      "Epoch [2/3], Loss_main: 0.4446661174297333, Loss_domain: 0.06724961847066879\n",
      "Epoch [3/3], Loss_main: 0.4227901101112366, Loss_domain: 0.036860279738903046\n",
      "Training complete for fold 2\n",
      "Domain: {0, 2, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3780961334705353, Loss_domain: 0.11050217598676682\n",
      "Epoch [2/3], Loss_main: 0.38023197650909424, Loss_domain: 0.0530381016433239\n",
      "Epoch [3/3], Loss_main: 0.5167771577835083, Loss_domain: 0.030317988246679306\n",
      "Training complete for fold 3\n",
      "Domain: {0, 2, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.4262724220752716, Loss_domain: 0.11923566460609436\n",
      "Epoch [2/3], Loss_main: 0.4402000308036804, Loss_domain: 0.05894480645656586\n",
      "Epoch [3/3], Loss_main: 0.41898614168167114, Loss_domain: 0.03210807964205742\n",
      "Training complete for fold 4\n",
      "Domain: {0, 2, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.34353306889533997, Loss_domain: 0.13567036390304565\n",
      "Epoch [2/3], Loss_main: 0.4021899700164795, Loss_domain: 0.05303363874554634\n",
      "Epoch [3/3], Loss_main: 0.45026540756225586, Loss_domain: 0.031738266348838806\n",
      "Training complete for fold 5\n",
      "Domain: {1, 2, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.39637190103530884, Loss_domain: 0.1841496080160141\n",
      "Epoch [2/3], Loss_main: 0.45942533016204834, Loss_domain: 0.07939566671848297\n",
      "Epoch [3/3], Loss_main: 0.33266496658325195, Loss_domain: 0.04102051630616188\n",
      "Training complete for fold 1\n",
      "Domain: {1, 2, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3376622498035431, Loss_domain: 0.1337137222290039\n",
      "Epoch [2/3], Loss_main: 0.3244401514530182, Loss_domain: 0.06122090294957161\n",
      "Epoch [3/3], Loss_main: 0.5312821269035339, Loss_domain: 0.03407778963446617\n",
      "Training complete for fold 2\n",
      "Domain: {1, 2, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.37667256593704224, Loss_domain: 0.13260804116725922\n",
      "Epoch [2/3], Loss_main: 0.3645313084125519, Loss_domain: 0.05654405802488327\n",
      "Epoch [3/3], Loss_main: 0.4164345860481262, Loss_domain: 0.032252777367830276\n",
      "Training complete for fold 3\n",
      "Domain: {1, 2, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.29993659257888794, Loss_domain: 0.2074132114648819\n",
      "Epoch [2/3], Loss_main: 0.3312614858150482, Loss_domain: 0.08463975042104721\n",
      "Epoch [3/3], Loss_main: 0.405506432056427, Loss_domain: 0.046369731426239014\n",
      "Training complete for fold 4\n",
      "Domain: {1, 2, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.39833950996398926, Loss_domain: 0.12864607572555542\n",
      "Epoch [2/3], Loss_main: 0.4040534794330597, Loss_domain: 0.04475266858935356\n",
      "Epoch [3/3], Loss_main: 0.3931814134120941, Loss_domain: 0.031074894592165947\n",
      "Training complete for fold 5\n",
      "{'Cluster(0)': [0.8177052999417589, 0.8306214990664177, 0.9110590988882388, 0.8689828380075345, 0.7723212401538713], 'Cluster(1)': [0.8112987769365172, 0.8230322239830956, 0.9116442363955529, 0.8650749583564686, 0.7625157131198768], 'Cluster(0, 1)': [0.8066394874781596, 0.842671194114318, 0.8712697483908719, 0.8567318757192175, 0.7752194096830296], 'Cluster(2)': [0.8107163657542225, 0.8371515318796577, 0.8873610298420129, 0.8615253515125693, 0.7734554716434531], 'Cluster(0, 2)': [0.815569792273345, 0.8380821917808219, 0.8949678174370977, 0.8655913978494624, 0.7769703484184912], 'Cluster(1, 2)': [0.8132401475441662, 0.8191268191268192, 0.9221767115272089, 0.8676025323424167, 0.7602805081005924], 'Cluster(0, 1, 2)': [0.8145991069695205, 0.8311374025275612, 0.9043300175541252, 0.8661902760263416, 0.7709763186443448], 'Cluster(3)': [0.8136284216656959, 0.8361597374179431, 0.8943826799297835, 0.8642917726887193, 0.7743696434847993], 'Cluster(0, 3)': [0.8237235488254708, 0.8283621140763998, 0.9262726740784084, 0.874585635359116, 0.7738691702763653], 'Cluster(1, 3)': [0.8163463405164045, 0.8190500774393392, 0.9283206553540082, 0.8702687877125617, 0.7619098949014703], 'Cluster(0, 1, 3)': [0.8171228887594641, 0.8292553191489361, 0.9122293739028672, 0.8687656728893842, 0.7708867585036552], 'Cluster(2, 3)': [0.8190642593671131, 0.8361276365603029, 0.9046225863077824, 0.8690275435637999, 0.7774699775162686], 'Cluster(0, 2, 3)': [0.8114929139972821, 0.8193683111459149, 0.9183733177296665, 0.8660504897227204, 0.7595328792918384], 'Cluster(1, 2, 3)': [0.8132401475441662, 0.8236689509752241, 0.914277355178467, 0.8666112035496395, 0.7641207895338382], 'Cluster(0, 1, 2, 3)': [0.8140166957872258, 0.8243670886075949, 0.914569923932124, 0.867128987517337, 0.7651326249781798], 'Cluster(4)': [0.8147932440302854, 0.8240399789584429, 0.9166179052077238, 0.8678670360110803, 0.7652910645484667], 'Cluster(0, 4)': [0.815569792273345, 0.8371584699453551, 0.8964306612053833, 0.8657812941508901, 0.7762591851901124], 'Cluster(1, 4)': [0.8208114929139972, 0.8309896524276996, 0.9163253364540667, 0.8715736746904132, 0.7743773248917767], 'Cluster(0, 1, 4)': [0.8103280916326927, 0.8423562412342216, 0.8785839672322996, 0.8600887870542747, 0.7771454169687175], 'Cluster(2, 4)': [0.8111046398757523, 0.8304947283049473, 0.8987712112346401, 0.8632850920331601, 0.7684854325071064], 'Cluster(0, 2, 4)': [0.8167346146379344, 0.8346861471861472, 0.9025746050321826, 0.8673039077874614, 0.7750034017659472], 'Cluster(1, 2, 4)': [0.8184818481848185, 0.8340059187516815, 0.9069631363370392, 0.8689558514365803, 0.7754665652833493], 'Cluster(0, 1, 2, 4)': [0.8192583964278781, 0.8383673469387755, 0.9014043300175542, 0.86874383194699, 0.7793230536412064], 'Cluster(3, 4)': [0.8118811881188119, 0.8231723409870678, 0.9125219426565243, 0.8655473844873041, 0.7629545662503625], 'Cluster(0, 3, 4)': [0.8128518734226364, 0.8354291962821214, 0.8940901111761264, 0.8637648388920294, 0.7733578080404578], 'Cluster(1, 3, 4)': [0.8167346146379344, 0.824501573976915, 0.9195435927442949, 0.869432918395574, 0.7667539083167523], 'Cluster(0, 1, 3, 4)': [0.8177052999417589, 0.8247314645009169, 0.9210064365125804, 0.8702142363510712, 0.7674853302008949], 'Cluster(2, 3, 4)': [0.8163463405164045, 0.8315450643776824, 0.9069631363370392, 0.8676182479708928, 0.7722928780358018], 'Cluster(0, 2, 3, 4)': [0.8043098427489808, 0.8464634847613571, 0.8613224107665302, 0.8538283062645011, 0.776593115365954], 'Cluster(1, 2, 3, 4)': [0.8140166957872258, 0.8285256410256411, 0.9075482738443534, 0.866238480871265, 0.7685462084743984]}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for target_domain in subsets:\n",
    "    # powerset_train is a train function with modified script:\n",
    "                    # domain= torch.tensor([1 if x in target_domain else 0 for x in domain])\n",
    "                    # domain = domain.to(device)\n",
    "        \n",
    "    metrics = powerset_train(target_domain=target_domain)\n",
    "    name = 'Cluster(' + ', '.join([str(elem) for i,elem in enumerate(target_domain)]) + ')'\n",
    "    results[name] = metrics\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame with index and columns\n",
    "metrics = pd.read_csv('metrics.csv', index_col=0)\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC'])\n",
    "\n",
    "metrics = pd.concat([metrics, results_df])\n",
    "metrics.to_csv('metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
