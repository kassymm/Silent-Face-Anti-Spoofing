{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\", index_col=0)\n",
    "df.rename(columns={'Real': 'Prediction', '40':'SpoofType', '41':'Illumination', '42':'Environment', '43':'Spoof'}, inplace=True)\n",
    "# the prediction by silent face takes values 0-2. Gotta convert it.\n",
    "df['Prediction'] = df['Prediction'].replace({0.0: 1, 1.0: 0, 2.0: 1})\n",
    "df['Prediction'] = df['Prediction'].astype(int)\n",
    "\n",
    "embeddings = pd.read_csv('dropout_embeddings.csv', index_col=0)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "common_index = df.index.intersection(embeddings.index)\n",
    "df = df.loc[common_index]\n",
    "embeddings = embeddings.loc[common_index]\n",
    "\n",
    "pd.testing.assert_series_equal(df.index.to_series(), embeddings.index.to_series())\n",
    "\n",
    "\n",
    "gmm = GaussianMixture(n_components=100, random_state=42)\n",
    "clusters = gmm.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['Spoof'] = df['Spoof']\n",
    "embeddings['Cluster'] = clusters\n",
    "embeddings['Illumination'] = df['Illumination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = len(embeddings)\n",
    "np.random.seed(seed = 42)\n",
    "random_cluster = np.random.randint(5, size = n)\n",
    "\n",
    "embeddings['Random'] = random_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>Spoof</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Illumination</th>\n",
       "      <th>Random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data/test/6964/spoof/494405.png</th>\n",
       "      <td>0.420929</td>\n",
       "      <td>0.391198</td>\n",
       "      <td>0.395073</td>\n",
       "      <td>-0.421826</td>\n",
       "      <td>0.368112</td>\n",
       "      <td>0.353278</td>\n",
       "      <td>0.749079</td>\n",
       "      <td>0.455125</td>\n",
       "      <td>-0.878119</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765734</td>\n",
       "      <td>-0.755238</td>\n",
       "      <td>0.726697</td>\n",
       "      <td>-0.765685</td>\n",
       "      <td>-0.470916</td>\n",
       "      <td>0.775886</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6407/spoof/494411.png</th>\n",
       "      <td>-0.147611</td>\n",
       "      <td>0.357509</td>\n",
       "      <td>-0.029671</td>\n",
       "      <td>-0.238791</td>\n",
       "      <td>0.286075</td>\n",
       "      <td>-0.134124</td>\n",
       "      <td>0.624391</td>\n",
       "      <td>0.178886</td>\n",
       "      <td>-0.596819</td>\n",
       "      <td>-0.246836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.574118</td>\n",
       "      <td>-0.704340</td>\n",
       "      <td>0.688574</td>\n",
       "      <td>-0.656510</td>\n",
       "      <td>0.135470</td>\n",
       "      <td>0.686906</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6153/spoof/494412.png</th>\n",
       "      <td>-0.105897</td>\n",
       "      <td>-0.249867</td>\n",
       "      <td>-0.077944</td>\n",
       "      <td>0.181672</td>\n",
       "      <td>-0.120957</td>\n",
       "      <td>-0.085050</td>\n",
       "      <td>0.615739</td>\n",
       "      <td>-0.094033</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>0.141131</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.605586</td>\n",
       "      <td>-0.580930</td>\n",
       "      <td>0.522196</td>\n",
       "      <td>-0.592422</td>\n",
       "      <td>0.108543</td>\n",
       "      <td>0.643653</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6411/live/494418.png</th>\n",
       "      <td>0.441198</td>\n",
       "      <td>0.528195</td>\n",
       "      <td>0.404773</td>\n",
       "      <td>-0.509041</td>\n",
       "      <td>0.446964</td>\n",
       "      <td>0.322769</td>\n",
       "      <td>0.645940</td>\n",
       "      <td>0.537904</td>\n",
       "      <td>-0.691915</td>\n",
       "      <td>-0.460678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600595</td>\n",
       "      <td>-0.675778</td>\n",
       "      <td>0.649979</td>\n",
       "      <td>-0.616289</td>\n",
       "      <td>-0.460027</td>\n",
       "      <td>0.677907</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6336/spoof/494419.png</th>\n",
       "      <td>0.753356</td>\n",
       "      <td>0.419226</td>\n",
       "      <td>0.744619</td>\n",
       "      <td>-0.425761</td>\n",
       "      <td>0.417521</td>\n",
       "      <td>0.629097</td>\n",
       "      <td>0.054008</td>\n",
       "      <td>0.498429</td>\n",
       "      <td>-0.053692</td>\n",
       "      <td>-0.445463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103442</td>\n",
       "      <td>0.036649</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.089932</td>\n",
       "      <td>-0.705690</td>\n",
       "      <td>0.065030</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/9909/live/561566.png</th>\n",
       "      <td>-0.519080</td>\n",
       "      <td>-0.166543</td>\n",
       "      <td>-0.483397</td>\n",
       "      <td>0.018525</td>\n",
       "      <td>-0.025248</td>\n",
       "      <td>-0.455392</td>\n",
       "      <td>0.709185</td>\n",
       "      <td>-0.054364</td>\n",
       "      <td>-0.614981</td>\n",
       "      <td>0.179256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.637598</td>\n",
       "      <td>-0.820888</td>\n",
       "      <td>0.733749</td>\n",
       "      <td>-0.704391</td>\n",
       "      <td>0.470004</td>\n",
       "      <td>0.818063</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6794/spoof/561567.png</th>\n",
       "      <td>-0.518010</td>\n",
       "      <td>-0.176494</td>\n",
       "      <td>-0.482479</td>\n",
       "      <td>0.025188</td>\n",
       "      <td>-0.025568</td>\n",
       "      <td>-0.453009</td>\n",
       "      <td>0.698944</td>\n",
       "      <td>-0.063262</td>\n",
       "      <td>-0.605874</td>\n",
       "      <td>0.179502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.634073</td>\n",
       "      <td>-0.810268</td>\n",
       "      <td>0.728125</td>\n",
       "      <td>-0.697531</td>\n",
       "      <td>0.467782</td>\n",
       "      <td>0.813546</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6708/spoof/561569.png</th>\n",
       "      <td>-0.531236</td>\n",
       "      <td>-0.176063</td>\n",
       "      <td>-0.496923</td>\n",
       "      <td>0.025220</td>\n",
       "      <td>-0.030200</td>\n",
       "      <td>-0.465959</td>\n",
       "      <td>0.701359</td>\n",
       "      <td>-0.064843</td>\n",
       "      <td>-0.605759</td>\n",
       "      <td>0.184143</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.627472</td>\n",
       "      <td>-0.813603</td>\n",
       "      <td>0.736568</td>\n",
       "      <td>-0.705165</td>\n",
       "      <td>0.482111</td>\n",
       "      <td>0.815603</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6551/spoof/561570.png</th>\n",
       "      <td>-0.517504</td>\n",
       "      <td>-0.174754</td>\n",
       "      <td>-0.485925</td>\n",
       "      <td>0.022997</td>\n",
       "      <td>-0.027235</td>\n",
       "      <td>-0.453871</td>\n",
       "      <td>0.709815</td>\n",
       "      <td>-0.061128</td>\n",
       "      <td>-0.617192</td>\n",
       "      <td>0.182697</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638211</td>\n",
       "      <td>-0.822434</td>\n",
       "      <td>0.742672</td>\n",
       "      <td>-0.712718</td>\n",
       "      <td>0.464891</td>\n",
       "      <td>0.819803</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6283/spoof/561571.png</th>\n",
       "      <td>-0.506432</td>\n",
       "      <td>-0.163607</td>\n",
       "      <td>-0.467940</td>\n",
       "      <td>0.015219</td>\n",
       "      <td>-0.019917</td>\n",
       "      <td>-0.440108</td>\n",
       "      <td>0.698670</td>\n",
       "      <td>-0.053623</td>\n",
       "      <td>-0.602160</td>\n",
       "      <td>0.173266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.627014</td>\n",
       "      <td>-0.804246</td>\n",
       "      <td>0.730291</td>\n",
       "      <td>-0.697133</td>\n",
       "      <td>0.451270</td>\n",
       "      <td>0.808349</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25755 rows × 132 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        1         2         3         4  \\\n",
       "Data/test/6964/spoof/494405.png  0.420929  0.391198  0.395073 -0.421826   \n",
       "Data/test/6407/spoof/494411.png -0.147611  0.357509 -0.029671 -0.238791   \n",
       "Data/test/6153/spoof/494412.png -0.105897 -0.249867 -0.077944  0.181672   \n",
       "Data/test/6411/live/494418.png   0.441198  0.528195  0.404773 -0.509041   \n",
       "Data/test/6336/spoof/494419.png  0.753356  0.419226  0.744619 -0.425761   \n",
       "...                                   ...       ...       ...       ...   \n",
       "Data/test/9909/live/561566.png  -0.519080 -0.166543 -0.483397  0.018525   \n",
       "Data/test/6794/spoof/561567.png -0.518010 -0.176494 -0.482479  0.025188   \n",
       "Data/test/6708/spoof/561569.png -0.531236 -0.176063 -0.496923  0.025220   \n",
       "Data/test/6551/spoof/561570.png -0.517504 -0.174754 -0.485925  0.022997   \n",
       "Data/test/6283/spoof/561571.png -0.506432 -0.163607 -0.467940  0.015219   \n",
       "\n",
       "                                        5         6         7         8  \\\n",
       "Data/test/6964/spoof/494405.png  0.368112  0.353278  0.749079  0.455125   \n",
       "Data/test/6407/spoof/494411.png  0.286075 -0.134124  0.624391  0.178886   \n",
       "Data/test/6153/spoof/494412.png -0.120957 -0.085050  0.615739 -0.094033   \n",
       "Data/test/6411/live/494418.png   0.446964  0.322769  0.645940  0.537904   \n",
       "Data/test/6336/spoof/494419.png  0.417521  0.629097  0.054008  0.498429   \n",
       "...                                   ...       ...       ...       ...   \n",
       "Data/test/9909/live/561566.png  -0.025248 -0.455392  0.709185 -0.054364   \n",
       "Data/test/6794/spoof/561567.png -0.025568 -0.453009  0.698944 -0.063262   \n",
       "Data/test/6708/spoof/561569.png -0.030200 -0.465959  0.701359 -0.064843   \n",
       "Data/test/6551/spoof/561570.png -0.027235 -0.453871  0.709815 -0.061128   \n",
       "Data/test/6283/spoof/561571.png -0.019917 -0.440108  0.698670 -0.053623   \n",
       "\n",
       "                                        9        10  ...       123       124  \\\n",
       "Data/test/6964/spoof/494405.png -0.878119 -0.339846  ... -0.765734 -0.755238   \n",
       "Data/test/6407/spoof/494411.png -0.596819 -0.246836  ... -0.574118 -0.704340   \n",
       "Data/test/6153/spoof/494412.png -0.588235  0.141131  ... -0.605586 -0.580930   \n",
       "Data/test/6411/live/494418.png  -0.691915 -0.460678  ... -0.600595 -0.675778   \n",
       "Data/test/6336/spoof/494419.png -0.053692 -0.445463  ... -0.103442  0.036649   \n",
       "...                                   ...       ...  ...       ...       ...   \n",
       "Data/test/9909/live/561566.png  -0.614981  0.179256  ... -0.637598 -0.820888   \n",
       "Data/test/6794/spoof/561567.png -0.605874  0.179502  ... -0.634073 -0.810268   \n",
       "Data/test/6708/spoof/561569.png -0.605759  0.184143  ... -0.627472 -0.813603   \n",
       "Data/test/6551/spoof/561570.png -0.617192  0.182697  ... -0.638211 -0.822434   \n",
       "Data/test/6283/spoof/561571.png -0.602160  0.173266  ... -0.627014 -0.804246   \n",
       "\n",
       "                                      125       126       127       128  \\\n",
       "Data/test/6964/spoof/494405.png  0.726697 -0.765685 -0.470916  0.775886   \n",
       "Data/test/6407/spoof/494411.png  0.688574 -0.656510  0.135470  0.686906   \n",
       "Data/test/6153/spoof/494412.png  0.522196 -0.592422  0.108543  0.643653   \n",
       "Data/test/6411/live/494418.png   0.649979 -0.616289 -0.460027  0.677907   \n",
       "Data/test/6336/spoof/494419.png  0.001247 -0.089932 -0.705690  0.065030   \n",
       "...                                   ...       ...       ...       ...   \n",
       "Data/test/9909/live/561566.png   0.733749 -0.704391  0.470004  0.818063   \n",
       "Data/test/6794/spoof/561567.png  0.728125 -0.697531  0.467782  0.813546   \n",
       "Data/test/6708/spoof/561569.png  0.736568 -0.705165  0.482111  0.815603   \n",
       "Data/test/6551/spoof/561570.png  0.742672 -0.712718  0.464891  0.819803   \n",
       "Data/test/6283/spoof/561571.png  0.730291 -0.697133  0.451270  0.808349   \n",
       "\n",
       "                                 Spoof  Cluster  Illumination  Random  \n",
       "Data/test/6964/spoof/494405.png      1        6             2       3  \n",
       "Data/test/6407/spoof/494411.png      1       23             1       4  \n",
       "Data/test/6153/spoof/494412.png      1       41             1       2  \n",
       "Data/test/6411/live/494418.png       0       76             0       4  \n",
       "Data/test/6336/spoof/494419.png      1       50             1       4  \n",
       "...                                ...      ...           ...     ...  \n",
       "Data/test/9909/live/561566.png       0       89             0       1  \n",
       "Data/test/6794/spoof/561567.png      1       89             1       0  \n",
       "Data/test/6708/spoof/561569.png      1       89             1       2  \n",
       "Data/test/6551/spoof/561570.png      1       89             1       4  \n",
       "Data/test/6283/spoof/561571.png      1       89             2       4  \n",
       "\n",
       "[25755 rows x 132 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m counts \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpoof\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create a bar chart of the counts in the current subplot\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43maxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcluster_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mbar(counts\u001b[38;5;241m.\u001b[39mindex, counts\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     11\u001b[0m axs[cluster_id]\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpoof\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m axs[cluster_id]\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(20, 4))\n",
    "\n",
    "# Create a histogram for each cluster\n",
    "for cluster_id, group in embeddings.groupby('Cluster'):\n",
    "    # Count the frequency of each value of the current feature in the current cluster\n",
    "    counts = group[\"Spoof\"].value_counts()\n",
    "\n",
    "    # Create a bar chart of the counts in the current subplot\n",
    "    axs[cluster_id].bar(counts.index, counts.values)\n",
    "    axs[cluster_id].set_xlabel(\"Spoof\")\n",
    "    axs[cluster_id].set_ylabel(\"Count\")\n",
    "    axs[cluster_id].set_title(f\"Cluster {cluster_id}\")\n",
    "\n",
    "# Add a title to the overall plot\n",
    "plt.suptitle(\"Histograms of Spoof\")\n",
    "\n",
    "# Adjust the layout of the subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the overall plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sf_column = {'Spoof':128, 'Cluster':129, 'Illumination':130, 'Random':131}\n",
    "\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.embeddings.iloc[idx, :128].values, dtype=torch.float32)\n",
    "        spoof = torch.tensor(self.embeddings.iloc[idx, 128], dtype=torch.long)\n",
    "        # 128 = Spoof, 129 = Cluster, 130 = Illumination\n",
    "        domain = torch.tensor(self.embeddings.iloc[idx, 131], dtype=torch.long)\n",
    "        return embedding, spoof, domain\n",
    "\n",
    "\n",
    "class AdversarialModel(nn.Module):\n",
    "    def __init__(self, num_clusters):\n",
    "        super(AdversarialModel, self).__init__()\n",
    "        self.step = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.anti_spoofing_head = nn.Sequential(\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "        self.domain_head = nn.Sequential(\n",
    "            nn.Linear(64, num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.step(x)\n",
    "        anti_spoofing_logits = self.anti_spoofing_head(x)\n",
    "        domain_logits = self.domain_head(x)\n",
    "        return anti_spoofing_logits, domain_logits\n",
    "\n",
    "class RegularModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegularModel, self).__init__()\n",
    "        self.step = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.step(x)\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# sf_column = {'Spoof':128, 'Cluster':129, 'Illumination':130}\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.embeddings.iloc[idx, :128].values, dtype=torch.float32)\n",
    "        spoof = torch.tensor(self.embeddings.iloc[idx, 128], dtype=torch.long)\n",
    "        # 128 = Spoof, 129 = Cluster, 130 = Illumination\n",
    "        domain = torch.tensor(self.embeddings.iloc[idx, 129], dtype=torch.long)\n",
    "        return embedding, spoof, domain\n",
    "\n",
    "def train(target_domain, num_folds = 5, num_epochs = 4, batch_size = 64, learning_rate = 0.001):\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = EmbeddingsDataset(embeddings)\n",
    "\n",
    "    # Create the KFold object\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Iterate over the folds\n",
    "    for fold, (train_indices, test_indices) in enumerate(kf.split(dataset)):\n",
    "        print(f\"Domain: {target_domain}, Fold {fold+1}\")\n",
    "\n",
    "        # Create the data loaders for the current fold\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "        # Create the model and set the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AdversarialModel(2).to(device)\n",
    "\n",
    "        # Define the loss functions and optimizers\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_classifier = optim.Adam(model.anti_spoofing_head.parameters(), lr=learning_rate)\n",
    "        optimizer_domain = optim.Adam(model.domain_head.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the model for the current fold\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (embedding, spoof, domain) in enumerate(train_loader):\n",
    "                # Move the data to the device\n",
    "                embedding = embedding.to(device)\n",
    "                spoof = spoof.to(device)\n",
    "                domain = (domain == target_domain).long().to(device)\n",
    "\n",
    "                # Step 1: Update all weights except the domain head\n",
    "                optimizer_classifier.zero_grad()\n",
    "                anti_spoofing_logits, domain_logits = model(embedding)\n",
    "                loss_main = criterion(anti_spoofing_logits, spoof) - criterion(domain_logits, domain)\n",
    "                loss_main.backward()\n",
    "                optimizer_classifier.step()\n",
    "\n",
    "                # Step 2: Update the domain head \n",
    "                if i%100 == 0:\n",
    "                    optimizer_domain.zero_grad()\n",
    "                    _, domain_logits = model(embedding)\n",
    "                    loss_domain = criterion(domain_logits, domain)\n",
    "                    loss_domain.backward()\n",
    "                    optimizer_domain.step()\n",
    "\n",
    "                # # Print progress\n",
    "                # if (i+1) % 100 == 0:\n",
    "                #     print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss_main: {loss_main.item()}, Loss_domain: {loss_domain.item()}\")\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss_main: {loss_main.item()}, Loss_domain: {loss_domain.item()}\")\n",
    "        print(\"Training complete for fold\", fold+1)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Define the lists to store the true and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Iterate over the test data\n",
    "    for i, (embedding, spoof, domain) in enumerate(test_loader):\n",
    "\n",
    "        # Move the data to the device\n",
    "        embedding = embedding.to(device)\n",
    "        spoof = spoof.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        anti_spoofing_logits, domain_logits = model(embedding)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        _, predicted = torch.max(anti_spoofing_logits, 1)\n",
    "\n",
    "        # Append the true and predicted labels to the lists\n",
    "        true_labels += spoof.tolist()\n",
    "        predicted_labels += predicted.tolist()\n",
    "\n",
    "    # Calculate the evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    roc_auc = roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "    metrics = [accuracy, precision, recall, f1, roc_auc]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: 0, Fold 1\n",
      "Epoch [1/4], Loss_main: -0.18723362684249878, Loss_domain: 0.6874615550041199\n",
      "Epoch [2/4], Loss_main: -0.2076023519039154, Loss_domain: 0.675388514995575\n",
      "Epoch [3/4], Loss_main: -0.17381122708320618, Loss_domain: 0.659299910068512\n",
      "Epoch [4/4], Loss_main: -0.22934910655021667, Loss_domain: 0.6402658820152283\n",
      "Training complete for fold 1\n",
      "Domain: 0, Fold 2\n",
      "Epoch [1/4], Loss_main: -0.14696425199508667, Loss_domain: 0.6860234141349792\n",
      "Epoch [2/4], Loss_main: -0.22533896565437317, Loss_domain: 0.67408686876297\n",
      "Epoch [3/4], Loss_main: -0.30919480323791504, Loss_domain: 0.6550109386444092\n",
      "Epoch [4/4], Loss_main: -0.21617957949638367, Loss_domain: 0.638133704662323\n",
      "Training complete for fold 2\n",
      "Domain: 0, Fold 3\n",
      "Epoch [1/4], Loss_main: -0.10998457670211792, Loss_domain: 0.6758205890655518\n",
      "Epoch [2/4], Loss_main: -0.20072174072265625, Loss_domain: 0.6580411791801453\n",
      "Epoch [3/4], Loss_main: -0.12752848863601685, Loss_domain: 0.63765549659729\n",
      "Epoch [4/4], Loss_main: -0.15169402956962585, Loss_domain: 0.61913001537323\n",
      "Training complete for fold 3\n",
      "Domain: 0, Fold 4\n",
      "Epoch [1/4], Loss_main: -0.26304757595062256, Loss_domain: 0.7255219221115112\n",
      "Epoch [2/4], Loss_main: -0.21480965614318848, Loss_domain: 0.705118715763092\n",
      "Epoch [3/4], Loss_main: -0.22911038994789124, Loss_domain: 0.6881216764450073\n",
      "Epoch [4/4], Loss_main: -0.25224563479423523, Loss_domain: 0.6816453337669373\n",
      "Training complete for fold 4\n",
      "Domain: 0, Fold 5\n",
      "Epoch [1/4], Loss_main: -0.07584893703460693, Loss_domain: 0.6281323432922363\n",
      "Epoch [2/4], Loss_main: -0.0761178731918335, Loss_domain: 0.6205652952194214\n",
      "Epoch [3/4], Loss_main: -0.1562555730342865, Loss_domain: 0.6100083589553833\n",
      "Epoch [4/4], Loss_main: -0.09588024020195007, Loss_domain: 0.5821135640144348\n",
      "Training complete for fold 5\n",
      "Domain: 1, Fold 1\n",
      "Epoch [1/4], Loss_main: -0.250199556350708, Loss_domain: 0.755068838596344\n",
      "Epoch [2/4], Loss_main: -0.24521350860595703, Loss_domain: 0.7367857694625854\n",
      "Epoch [3/4], Loss_main: -0.20912709832191467, Loss_domain: 0.7172337770462036\n",
      "Epoch [4/4], Loss_main: -0.24708038568496704, Loss_domain: 0.6988050937652588\n",
      "Training complete for fold 1\n",
      "Domain: 1, Fold 2\n",
      "Epoch [1/4], Loss_main: -0.199382483959198, Loss_domain: 0.7078431844711304\n",
      "Epoch [2/4], Loss_main: -0.2701394557952881, Loss_domain: 0.69720858335495\n",
      "Epoch [3/4], Loss_main: -0.19760805368423462, Loss_domain: 0.6794225573539734\n",
      "Epoch [4/4], Loss_main: -0.28535351157188416, Loss_domain: 0.6614887714385986\n",
      "Training complete for fold 2\n",
      "Domain: 1, Fold 3\n",
      "Epoch [1/4], Loss_main: -0.2858823239803314, Loss_domain: 0.7807943820953369\n",
      "Epoch [2/4], Loss_main: -0.2943166494369507, Loss_domain: 0.7710484266281128\n",
      "Epoch [3/4], Loss_main: -0.2476567029953003, Loss_domain: 0.7551271319389343\n",
      "Epoch [4/4], Loss_main: -0.2541932463645935, Loss_domain: 0.737983226776123\n",
      "Training complete for fold 3\n",
      "Domain: 1, Fold 4\n",
      "Epoch [1/4], Loss_main: -0.16535118222236633, Loss_domain: 0.6633746027946472\n",
      "Epoch [2/4], Loss_main: -0.19388258457183838, Loss_domain: 0.6502496004104614\n",
      "Epoch [3/4], Loss_main: -0.1617247462272644, Loss_domain: 0.6357133388519287\n",
      "Epoch [4/4], Loss_main: -0.1407761573791504, Loss_domain: 0.6220418214797974\n",
      "Training complete for fold 4\n",
      "Domain: 1, Fold 5\n",
      "Epoch [1/4], Loss_main: -0.2309129238128662, Loss_domain: 0.7911370396614075\n",
      "Epoch [2/4], Loss_main: -0.2942734360694885, Loss_domain: 0.7783198356628418\n",
      "Epoch [3/4], Loss_main: -0.35923656821250916, Loss_domain: 0.7565909624099731\n",
      "Epoch [4/4], Loss_main: -0.2701161503791809, Loss_domain: 0.7380563616752625\n",
      "Training complete for fold 5\n",
      "Domain: 2, Fold 1\n",
      "Epoch [1/4], Loss_main: -0.1755046546459198, Loss_domain: 0.6713176369667053\n",
      "Epoch [2/4], Loss_main: -0.19414985179901123, Loss_domain: 0.653011679649353\n",
      "Epoch [3/4], Loss_main: -0.11249983310699463, Loss_domain: 0.6408197283744812\n",
      "Epoch [4/4], Loss_main: -0.17391812801361084, Loss_domain: 0.6258299350738525\n",
      "Training complete for fold 1\n",
      "Domain: 2, Fold 2\n",
      "Epoch [1/4], Loss_main: -0.140758216381073, Loss_domain: 0.738206148147583\n",
      "Epoch [2/4], Loss_main: -0.20458215475082397, Loss_domain: 0.7180298566818237\n",
      "Epoch [3/4], Loss_main: -0.3373298943042755, Loss_domain: 0.7036042213439941\n",
      "Epoch [4/4], Loss_main: -0.2509970963001251, Loss_domain: 0.6908735632896423\n",
      "Training complete for fold 2\n",
      "Domain: 2, Fold 3\n",
      "Epoch [1/4], Loss_main: -0.20932865142822266, Loss_domain: 0.7618722915649414\n",
      "Epoch [2/4], Loss_main: -0.31598976254463196, Loss_domain: 0.7456220984458923\n",
      "Epoch [3/4], Loss_main: -0.23292580246925354, Loss_domain: 0.7220783829689026\n",
      "Epoch [4/4], Loss_main: -0.1767248511314392, Loss_domain: 0.6977992653846741\n",
      "Training complete for fold 3\n",
      "Domain: 2, Fold 4\n",
      "Epoch [1/4], Loss_main: -0.24397653341293335, Loss_domain: 0.7637486457824707\n",
      "Epoch [2/4], Loss_main: -0.23771774768829346, Loss_domain: 0.7438950538635254\n",
      "Epoch [3/4], Loss_main: -0.38530433177948, Loss_domain: 0.7328639626502991\n",
      "Epoch [4/4], Loss_main: -0.3590331971645355, Loss_domain: 0.714425265789032\n",
      "Training complete for fold 4\n",
      "Domain: 2, Fold 5\n",
      "Epoch [1/4], Loss_main: -0.14603358507156372, Loss_domain: 0.666867733001709\n",
      "Epoch [2/4], Loss_main: -0.2564490735530853, Loss_domain: 0.6487429738044739\n",
      "Epoch [3/4], Loss_main: -0.18254625797271729, Loss_domain: 0.6432404518127441\n",
      "Epoch [4/4], Loss_main: -0.21238061785697937, Loss_domain: 0.6233871579170227\n",
      "Training complete for fold 5\n",
      "Domain: 3, Fold 1\n",
      "Epoch [1/4], Loss_main: -0.17432010173797607, Loss_domain: 0.6923646926879883\n",
      "Epoch [2/4], Loss_main: -0.1874723732471466, Loss_domain: 0.6781728267669678\n",
      "Epoch [3/4], Loss_main: -0.1525963544845581, Loss_domain: 0.6606741547584534\n",
      "Epoch [4/4], Loss_main: -0.19633185863494873, Loss_domain: 0.6472671627998352\n",
      "Training complete for fold 1\n",
      "Domain: 3, Fold 2\n",
      "Epoch [1/4], Loss_main: -0.17160367965698242, Loss_domain: 0.6940390467643738\n",
      "Epoch [2/4], Loss_main: -0.21460238099098206, Loss_domain: 0.6797941327095032\n",
      "Epoch [3/4], Loss_main: -0.2097669243812561, Loss_domain: 0.6606631875038147\n",
      "Epoch [4/4], Loss_main: -0.30071479082107544, Loss_domain: 0.6418524384498596\n",
      "Training complete for fold 2\n",
      "Domain: 3, Fold 3\n",
      "Epoch [1/4], Loss_main: -0.16583770513534546, Loss_domain: 0.7455141544342041\n",
      "Epoch [2/4], Loss_main: -0.32096824049949646, Loss_domain: 0.7246634364128113\n",
      "Epoch [3/4], Loss_main: -0.22387462854385376, Loss_domain: 0.7080873250961304\n",
      "Epoch [4/4], Loss_main: -0.3001128137111664, Loss_domain: 0.6915071606636047\n",
      "Training complete for fold 3\n",
      "Domain: 3, Fold 4\n",
      "Epoch [1/4], Loss_main: -0.08204585313796997, Loss_domain: 0.6180757880210876\n",
      "Epoch [2/4], Loss_main: -0.19172194600105286, Loss_domain: 0.6027772426605225\n",
      "Epoch [3/4], Loss_main: -0.13336440920829773, Loss_domain: 0.5844083428382874\n",
      "Epoch [4/4], Loss_main: -0.17080190777778625, Loss_domain: 0.5724923014640808\n",
      "Training complete for fold 4\n",
      "Domain: 3, Fold 5\n",
      "Epoch [1/4], Loss_main: -0.1816006600856781, Loss_domain: 0.6725448369979858\n",
      "Epoch [2/4], Loss_main: -0.24419885873794556, Loss_domain: 0.646255373954773\n",
      "Epoch [3/4], Loss_main: -0.2752259075641632, Loss_domain: 0.6351935267448425\n",
      "Epoch [4/4], Loss_main: -0.19038540124893188, Loss_domain: 0.6094112396240234\n",
      "Training complete for fold 5\n",
      "Domain: 4, Fold 1\n",
      "Epoch [1/4], Loss_main: -0.133877694606781, Loss_domain: 0.6147716045379639\n",
      "Epoch [2/4], Loss_main: -0.17567279934883118, Loss_domain: 0.6028610467910767\n",
      "Epoch [3/4], Loss_main: -0.16941577196121216, Loss_domain: 0.5906772017478943\n",
      "Epoch [4/4], Loss_main: -0.1674671173095703, Loss_domain: 0.5771489143371582\n",
      "Training complete for fold 1\n",
      "Domain: 4, Fold 2\n",
      "Epoch [1/4], Loss_main: -0.1821247935295105, Loss_domain: 0.7171621918678284\n",
      "Epoch [2/4], Loss_main: -0.1649962067604065, Loss_domain: 0.703586220741272\n",
      "Epoch [3/4], Loss_main: -0.2581658661365509, Loss_domain: 0.6897720098495483\n",
      "Epoch [4/4], Loss_main: -0.299651175737381, Loss_domain: 0.670779287815094\n",
      "Training complete for fold 2\n",
      "Domain: 4, Fold 3\n",
      "Epoch [1/4], Loss_main: -0.1420900523662567, Loss_domain: 0.6347730159759521\n",
      "Epoch [2/4], Loss_main: -0.14507442712783813, Loss_domain: 0.62180495262146\n",
      "Epoch [3/4], Loss_main: -0.1665305197238922, Loss_domain: 0.6108459234237671\n",
      "Epoch [4/4], Loss_main: -0.19557401537895203, Loss_domain: 0.592333197593689\n",
      "Training complete for fold 3\n",
      "Domain: 4, Fold 4\n",
      "Epoch [1/4], Loss_main: -0.2449670433998108, Loss_domain: 0.7807270288467407\n",
      "Epoch [2/4], Loss_main: -0.28829431533813477, Loss_domain: 0.7571449279785156\n",
      "Epoch [3/4], Loss_main: -0.24390912055969238, Loss_domain: 0.7462051510810852\n",
      "Epoch [4/4], Loss_main: -0.225352942943573, Loss_domain: 0.7275875210762024\n",
      "Training complete for fold 4\n",
      "Domain: 4, Fold 5\n",
      "Epoch [1/4], Loss_main: -0.156365305185318, Loss_domain: 0.625778317451477\n",
      "Epoch [2/4], Loss_main: -0.10929113626480103, Loss_domain: 0.6147165894508362\n",
      "Epoch [3/4], Loss_main: -0.08143693208694458, Loss_domain: 0.599963366985321\n",
      "Epoch [4/4], Loss_main: -0.23022368550300598, Loss_domain: 0.5829347968101501\n",
      "Training complete for fold 5\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for i in range(5):\n",
    "    metrics = train(target_domain=i)\n",
    "    name = f'Random_interval5_training_{i}'\n",
    "    results[name] = metrics\n",
    "\n",
    "metrics = pd.read_csv('metrics.csv', index_col=0)\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC'])\n",
    "\n",
    "metrics = pd.concat([metrics, results_df])\n",
    "\n",
    "metrics.to_csv('metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('HundredClusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0}, {1}, {0, 1}, {2}, {0, 2}, {1, 2}, {0, 1, 2}, {3}, {0, 3}, {1, 3}, {0, 1, 3}, {2, 3}, {0, 2, 3}, {1, 2, 3}, {0, 1, 2, 3}, {4}, {0, 4}, {1, 4}, {0, 1, 4}, {2, 4}, {0, 2, 4}, {1, 2, 4}, {0, 1, 2, 4}, {3, 4}, {0, 3, 4}, {1, 3, 4}, {0, 1, 3, 4}, {2, 3, 4}, {0, 2, 3, 4}, {1, 2, 3, 4}]\n"
     ]
    }
   ],
   "source": [
    "def powerset(s):\n",
    "    x = len(s)\n",
    "    masks = [1 << i for i in range(x)]\n",
    "    for i in range(1 << x):\n",
    "        yield {ss for mask, ss in zip(masks, s) if i & mask}\n",
    "\n",
    "subsets = list(powerset([i for i in range(5)]))\n",
    "\n",
    "# remove the full and empty set\n",
    "subsets = [x for x in subsets if x]\n",
    "subsets.remove({0,1,2,3,4})\n",
    "print(subsets)\n",
    "# print(len(subsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: {0}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.342884361743927, Loss_domain: 0.16027705371379852\n",
      "Epoch [2/3], Loss_main: 0.38096553087234497, Loss_domain: 0.06485865265130997\n",
      "Epoch [3/3], Loss_main: 0.4013586640357971, Loss_domain: 0.03605160862207413\n",
      "Training complete for fold 1\n",
      "Domain: {0}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.39563482999801636, Loss_domain: 0.1047213152050972\n",
      "Epoch [2/3], Loss_main: 0.443238228559494, Loss_domain: 0.05563052371144295\n",
      "Epoch [3/3], Loss_main: 0.5099105834960938, Loss_domain: 0.026765985414385796\n",
      "Training complete for fold 2\n",
      "Domain: {0}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.37540706992149353, Loss_domain: 0.14230605959892273\n",
      "Epoch [2/3], Loss_main: 0.45932862162590027, Loss_domain: 0.06985440105199814\n",
      "Epoch [3/3], Loss_main: 0.40205439925193787, Loss_domain: 0.03859870880842209\n",
      "Training complete for fold 3\n",
      "Domain: {0}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.39458411931991577, Loss_domain: 0.13456201553344727\n",
      "Epoch [2/3], Loss_main: 0.3555627465248108, Loss_domain: 0.052808042615652084\n",
      "Epoch [3/3], Loss_main: 0.3300926387310028, Loss_domain: 0.03028666414320469\n",
      "Training complete for fold 4\n",
      "Domain: {0}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.370656818151474, Loss_domain: 0.17580220103263855\n",
      "Epoch [2/3], Loss_main: 0.37714192271232605, Loss_domain: 0.06988835334777832\n",
      "Epoch [3/3], Loss_main: 0.42914462089538574, Loss_domain: 0.04034282639622688\n",
      "Training complete for fold 5\n",
      "Domain: {1}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.4225286841392517, Loss_domain: 0.1601424664258957\n",
      "Epoch [2/3], Loss_main: 0.4358278214931488, Loss_domain: 0.0665002167224884\n",
      "Epoch [3/3], Loss_main: 0.348580002784729, Loss_domain: 0.0360104963183403\n",
      "Training complete for fold 1\n",
      "Domain: {1}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3235998749732971, Loss_domain: 0.15924081206321716\n",
      "Epoch [2/3], Loss_main: 0.40181073546409607, Loss_domain: 0.06680040806531906\n",
      "Epoch [3/3], Loss_main: 0.3926204442977905, Loss_domain: 0.038164202123880386\n",
      "Training complete for fold 2\n",
      "Domain: {1}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3492480516433716, Loss_domain: 0.15752343833446503\n",
      "Epoch [2/3], Loss_main: 0.36937326192855835, Loss_domain: 0.060832660645246506\n",
      "Epoch [3/3], Loss_main: 0.4537489116191864, Loss_domain: 0.03274116292595863\n",
      "Training complete for fold 3\n",
      "Domain: {1}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.35560762882232666, Loss_domain: 0.19413013756275177\n",
      "Epoch [2/3], Loss_main: 0.39312201738357544, Loss_domain: 0.08148111402988434\n",
      "Epoch [3/3], Loss_main: 0.3700839579105377, Loss_domain: 0.046189308166503906\n",
      "Training complete for fold 4\n",
      "Domain: {1}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.4130737781524658, Loss_domain: 0.13910239934921265\n",
      "Epoch [2/3], Loss_main: 0.4522685408592224, Loss_domain: 0.05550694093108177\n",
      "Epoch [3/3], Loss_main: 0.3217916786670685, Loss_domain: 0.029674245044589043\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3322749733924866, Loss_domain: 0.23041220009326935\n",
      "Epoch [2/3], Loss_main: 0.34693190455436707, Loss_domain: 0.11067283153533936\n",
      "Epoch [3/3], Loss_main: 0.4367952346801758, Loss_domain: 0.06234065443277359\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3140040636062622, Loss_domain: 0.179125115275383\n",
      "Epoch [2/3], Loss_main: 0.38242796063423157, Loss_domain: 0.06968242675065994\n",
      "Epoch [3/3], Loss_main: 0.38770586252212524, Loss_domain: 0.04552384838461876\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3994058072566986, Loss_domain: 0.16976365447044373\n",
      "Epoch [2/3], Loss_main: 0.40117186307907104, Loss_domain: 0.06952103972434998\n",
      "Epoch [3/3], Loss_main: 0.5456827282905579, Loss_domain: 0.04437726363539696\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3708788752555847, Loss_domain: 0.15098808705806732\n",
      "Epoch [2/3], Loss_main: 0.34105536341667175, Loss_domain: 0.0781741514801979\n",
      "Epoch [3/3], Loss_main: 0.4202427268028259, Loss_domain: 0.03820659965276718\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.36502605676651, Loss_domain: 0.15522177517414093\n",
      "Epoch [2/3], Loss_main: 0.3899751603603363, Loss_domain: 0.07657480984926224\n",
      "Epoch [3/3], Loss_main: 0.49437230825424194, Loss_domain: 0.036669857800006866\n",
      "Training complete for fold 5\n",
      "Domain: {2}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3989788293838501, Loss_domain: 0.14857815206050873\n",
      "Epoch [2/3], Loss_main: 0.3793472647666931, Loss_domain: 0.07594027370214462\n",
      "Epoch [3/3], Loss_main: 0.3463188409805298, Loss_domain: 0.042252253741025925\n",
      "Training complete for fold 1\n",
      "Domain: {2}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3943098783493042, Loss_domain: 0.12718652188777924\n",
      "Epoch [2/3], Loss_main: 0.34848690032958984, Loss_domain: 0.05203479900956154\n",
      "Epoch [3/3], Loss_main: 0.40442004799842834, Loss_domain: 0.033879321068525314\n",
      "Training complete for fold 2\n",
      "Domain: {2}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.4397733509540558, Loss_domain: 0.11167659610509872\n",
      "Epoch [2/3], Loss_main: 0.4816715717315674, Loss_domain: 0.042269520461559296\n",
      "Epoch [3/3], Loss_main: 0.48893189430236816, Loss_domain: 0.02452259510755539\n",
      "Training complete for fold 3\n",
      "Domain: {2}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.34794849157333374, Loss_domain: 0.18265850841999054\n",
      "Epoch [2/3], Loss_main: 0.3171221613883972, Loss_domain: 0.08422230184078217\n",
      "Epoch [3/3], Loss_main: 0.4435265064239502, Loss_domain: 0.0526195764541626\n",
      "Training complete for fold 4\n",
      "Domain: {2}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.2969762980937958, Loss_domain: 0.1988828480243683\n",
      "Epoch [2/3], Loss_main: 0.38541179895401, Loss_domain: 0.08412650972604752\n",
      "Epoch [3/3], Loss_main: 0.3268076479434967, Loss_domain: 0.05285900458693504\n",
      "Training complete for fold 5\n",
      "Domain: {0, 2}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.2930804491043091, Loss_domain: 0.19050174951553345\n",
      "Epoch [2/3], Loss_main: 0.3442145586013794, Loss_domain: 0.09747631847858429\n",
      "Epoch [3/3], Loss_main: 0.3536628186702728, Loss_domain: 0.045500390231609344\n",
      "Training complete for fold 1\n",
      "Domain: {0, 2}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.42577534914016724, Loss_domain: 0.1469241976737976\n",
      "Epoch [2/3], Loss_main: 0.5026815533638, Loss_domain: 0.0609273724257946\n",
      "Epoch [3/3], Loss_main: 0.3943844139575958, Loss_domain: 0.03424040600657463\n",
      "Training complete for fold 2\n",
      "Domain: {0, 2}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.4332381784915924, Loss_domain: 0.10912749916315079\n",
      "Epoch [2/3], Loss_main: 0.3853822946548462, Loss_domain: 0.043634600937366486\n",
      "Epoch [3/3], Loss_main: 0.3856363594532013, Loss_domain: 0.02676871418952942\n",
      "Training complete for fold 3\n",
      "Domain: {0, 2}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.355194628238678, Loss_domain: 0.14365245401859283\n",
      "Epoch [2/3], Loss_main: 0.4109721779823303, Loss_domain: 0.062034446746110916\n",
      "Epoch [3/3], Loss_main: 0.43441712856292725, Loss_domain: 0.03934818506240845\n",
      "Training complete for fold 4\n",
      "Domain: {0, 2}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.4233976900577545, Loss_domain: 0.11584755778312683\n",
      "Epoch [2/3], Loss_main: 0.39222678542137146, Loss_domain: 0.05279003828763962\n",
      "Epoch [3/3], Loss_main: 0.48016729950904846, Loss_domain: 0.03392050787806511\n",
      "Training complete for fold 5\n",
      "Domain: {1, 2}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3875448703765869, Loss_domain: 0.1571858674287796\n",
      "Epoch [2/3], Loss_main: 0.4596617519855499, Loss_domain: 0.06670693308115005\n",
      "Epoch [3/3], Loss_main: 0.4593668282032013, Loss_domain: 0.033773355185985565\n",
      "Training complete for fold 1\n",
      "Domain: {1, 2}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.43102484941482544, Loss_domain: 0.16152416169643402\n",
      "Epoch [2/3], Loss_main: 0.4437929689884186, Loss_domain: 0.06795445829629898\n",
      "Epoch [3/3], Loss_main: 0.3420228064060211, Loss_domain: 0.04639924317598343\n",
      "Training complete for fold 2\n",
      "Domain: {1, 2}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.344595342874527, Loss_domain: 0.14831367135047913\n",
      "Epoch [2/3], Loss_main: 0.3286605477333069, Loss_domain: 0.06647361814975739\n",
      "Epoch [3/3], Loss_main: 0.432282030582428, Loss_domain: 0.03501035273075104\n",
      "Training complete for fold 3\n",
      "Domain: {1, 2}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3931930661201477, Loss_domain: 0.1450682431459427\n",
      "Epoch [2/3], Loss_main: 0.28649795055389404, Loss_domain: 0.05603742226958275\n",
      "Epoch [3/3], Loss_main: 0.43852898478507996, Loss_domain: 0.032504212111234665\n",
      "Training complete for fold 4\n",
      "Domain: {1, 2}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.34899431467056274, Loss_domain: 0.14578227698802948\n",
      "Epoch [2/3], Loss_main: 0.39962995052337646, Loss_domain: 0.06088189408183098\n",
      "Epoch [3/3], Loss_main: 0.39896172285079956, Loss_domain: 0.03280024230480194\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 2}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3977917432785034, Loss_domain: 0.14315910637378693\n",
      "Epoch [2/3], Loss_main: 0.41364043951034546, Loss_domain: 0.06244385987520218\n",
      "Epoch [3/3], Loss_main: 0.3733387887477875, Loss_domain: 0.03572831302881241\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 2}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3544608950614929, Loss_domain: 0.13470283150672913\n",
      "Epoch [2/3], Loss_main: 0.3990345001220703, Loss_domain: 0.0585697703063488\n",
      "Epoch [3/3], Loss_main: 0.3871178925037384, Loss_domain: 0.03660818189382553\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 2}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.32770997285842896, Loss_domain: 0.1505347341299057\n",
      "Epoch [2/3], Loss_main: 0.427675724029541, Loss_domain: 0.05510997772216797\n",
      "Epoch [3/3], Loss_main: 0.42050448060035706, Loss_domain: 0.035566262900829315\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 2}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3634774684906006, Loss_domain: 0.14276719093322754\n",
      "Epoch [2/3], Loss_main: 0.38032394647598267, Loss_domain: 0.05944656953215599\n",
      "Epoch [3/3], Loss_main: 0.37934961915016174, Loss_domain: 0.03154934197664261\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 2}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.37977588176727295, Loss_domain: 0.14268280565738678\n",
      "Epoch [2/3], Loss_main: 0.44946756958961487, Loss_domain: 0.054707322269678116\n",
      "Epoch [3/3], Loss_main: 0.3582911193370819, Loss_domain: 0.03353910893201828\n",
      "Training complete for fold 5\n",
      "Domain: {3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.327452152967453, Loss_domain: 0.1713705062866211\n",
      "Epoch [2/3], Loss_main: 0.40827032923698425, Loss_domain: 0.08200345188379288\n",
      "Epoch [3/3], Loss_main: 0.44333580136299133, Loss_domain: 0.04743387550115585\n",
      "Training complete for fold 1\n",
      "Domain: {3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.4189727008342743, Loss_domain: 0.11540428549051285\n",
      "Epoch [2/3], Loss_main: 0.4263172149658203, Loss_domain: 0.04998566210269928\n",
      "Epoch [3/3], Loss_main: 0.37526753544807434, Loss_domain: 0.02879183739423752\n",
      "Training complete for fold 2\n",
      "Domain: {3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3766164183616638, Loss_domain: 0.19512587785720825\n",
      "Epoch [2/3], Loss_main: 0.4147266447544098, Loss_domain: 0.0862986147403717\n",
      "Epoch [3/3], Loss_main: 0.43621984124183655, Loss_domain: 0.05407743528485298\n",
      "Training complete for fold 3\n",
      "Domain: {3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.42183148860931396, Loss_domain: 0.13597773015499115\n",
      "Epoch [2/3], Loss_main: 0.39795005321502686, Loss_domain: 0.05479966476559639\n",
      "Epoch [3/3], Loss_main: 0.5227000117301941, Loss_domain: 0.02805917337536812\n",
      "Training complete for fold 4\n",
      "Domain: {3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3321616053581238, Loss_domain: 0.14753471314907074\n",
      "Epoch [2/3], Loss_main: 0.43382200598716736, Loss_domain: 0.06726953387260437\n",
      "Epoch [3/3], Loss_main: 0.38498350977897644, Loss_domain: 0.03601551428437233\n",
      "Training complete for fold 5\n",
      "Domain: {0, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.39774563908576965, Loss_domain: 0.13128164410591125\n",
      "Epoch [2/3], Loss_main: 0.45726191997528076, Loss_domain: 0.05461371690034866\n",
      "Epoch [3/3], Loss_main: 0.5181876420974731, Loss_domain: 0.02559295855462551\n",
      "Training complete for fold 1\n",
      "Domain: {0, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.32066598534584045, Loss_domain: 0.156999409198761\n",
      "Epoch [2/3], Loss_main: 0.4362192153930664, Loss_domain: 0.07055199146270752\n",
      "Epoch [3/3], Loss_main: 0.37521055340766907, Loss_domain: 0.04513067752122879\n",
      "Training complete for fold 2\n",
      "Domain: {0, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.42918193340301514, Loss_domain: 0.14737500250339508\n",
      "Epoch [2/3], Loss_main: 0.38854968547821045, Loss_domain: 0.0656382292509079\n",
      "Epoch [3/3], Loss_main: 0.46894633769989014, Loss_domain: 0.03581206500530243\n",
      "Training complete for fold 3\n",
      "Domain: {0, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.36503756046295166, Loss_domain: 0.14141803979873657\n",
      "Epoch [2/3], Loss_main: 0.3283742666244507, Loss_domain: 0.06556911766529083\n",
      "Epoch [3/3], Loss_main: 0.3601369559764862, Loss_domain: 0.03395015373826027\n",
      "Training complete for fold 4\n",
      "Domain: {0, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3381555676460266, Loss_domain: 0.16878144443035126\n",
      "Epoch [2/3], Loss_main: 0.3744536340236664, Loss_domain: 0.06280618906021118\n",
      "Epoch [3/3], Loss_main: 0.428895503282547, Loss_domain: 0.03597491979598999\n",
      "Training complete for fold 5\n",
      "Domain: {1, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.4279143214225769, Loss_domain: 0.13851362466812134\n",
      "Epoch [2/3], Loss_main: 0.470393568277359, Loss_domain: 0.05734070762991905\n",
      "Epoch [3/3], Loss_main: 0.4525853097438812, Loss_domain: 0.034924767911434174\n",
      "Training complete for fold 1\n",
      "Domain: {1, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3908514380455017, Loss_domain: 0.131709024310112\n",
      "Epoch [2/3], Loss_main: 0.39089423418045044, Loss_domain: 0.05813298001885414\n",
      "Epoch [3/3], Loss_main: 0.4585496783256531, Loss_domain: 0.03699948638677597\n",
      "Training complete for fold 2\n",
      "Domain: {1, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.42388954758644104, Loss_domain: 0.18624040484428406\n",
      "Epoch [2/3], Loss_main: 0.4412126839160919, Loss_domain: 0.08523640036582947\n",
      "Epoch [3/3], Loss_main: 0.36696857213974, Loss_domain: 0.04475143924355507\n",
      "Training complete for fold 3\n",
      "Domain: {1, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.311603844165802, Loss_domain: 0.1590299904346466\n",
      "Epoch [2/3], Loss_main: 0.44049689173698425, Loss_domain: 0.06773313879966736\n",
      "Epoch [3/3], Loss_main: 0.45204925537109375, Loss_domain: 0.028650663793087006\n",
      "Training complete for fold 4\n",
      "Domain: {1, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.31585246324539185, Loss_domain: 0.14084139466285706\n",
      "Epoch [2/3], Loss_main: 0.3780750632286072, Loss_domain: 0.06558296084403992\n",
      "Epoch [3/3], Loss_main: 0.4358713924884796, Loss_domain: 0.035404931753873825\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.2977018356323242, Loss_domain: 0.15130676329135895\n",
      "Epoch [2/3], Loss_main: 0.3221377730369568, Loss_domain: 0.06310092657804489\n",
      "Epoch [3/3], Loss_main: 0.38030189275741577, Loss_domain: 0.0362384095788002\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.38872087001800537, Loss_domain: 0.1666427105665207\n",
      "Epoch [2/3], Loss_main: 0.35943278670310974, Loss_domain: 0.06996046751737595\n",
      "Epoch [3/3], Loss_main: 0.41552871465682983, Loss_domain: 0.037181638181209564\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.40661710500717163, Loss_domain: 0.1385882943868637\n",
      "Epoch [2/3], Loss_main: 0.4246887266635895, Loss_domain: 0.05393685773015022\n",
      "Epoch [3/3], Loss_main: 0.5116384029388428, Loss_domain: 0.03188764676451683\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3876721262931824, Loss_domain: 0.13030217587947845\n",
      "Epoch [2/3], Loss_main: 0.398674339056015, Loss_domain: 0.060141146183013916\n",
      "Epoch [3/3], Loss_main: 0.3456289768218994, Loss_domain: 0.03632209450006485\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3927721381187439, Loss_domain: 0.1273983120918274\n",
      "Epoch [2/3], Loss_main: 0.4395105242729187, Loss_domain: 0.06191324070096016\n",
      "Epoch [3/3], Loss_main: 0.3761559724807739, Loss_domain: 0.03514270484447479\n",
      "Training complete for fold 5\n",
      "Domain: {2, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.33718639612197876, Loss_domain: 0.14801345765590668\n",
      "Epoch [2/3], Loss_main: 0.42792728543281555, Loss_domain: 0.06949935108423233\n",
      "Epoch [3/3], Loss_main: 0.42708638310432434, Loss_domain: 0.03459494560956955\n",
      "Training complete for fold 1\n",
      "Domain: {2, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.33005291223526, Loss_domain: 0.14219358563423157\n",
      "Epoch [2/3], Loss_main: 0.39107969403266907, Loss_domain: 0.055245038121938705\n",
      "Epoch [3/3], Loss_main: 0.3850593566894531, Loss_domain: 0.03402561694383621\n",
      "Training complete for fold 2\n",
      "Domain: {2, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.33558154106140137, Loss_domain: 0.1712321788072586\n",
      "Epoch [2/3], Loss_main: 0.4087311029434204, Loss_domain: 0.08833909034729004\n",
      "Epoch [3/3], Loss_main: 0.5034211277961731, Loss_domain: 0.05298526957631111\n",
      "Training complete for fold 3\n",
      "Domain: {2, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.34609153866767883, Loss_domain: 0.13936668634414673\n",
      "Epoch [2/3], Loss_main: 0.29810959100723267, Loss_domain: 0.0629243403673172\n",
      "Epoch [3/3], Loss_main: 0.3336695730686188, Loss_domain: 0.03366832807660103\n",
      "Training complete for fold 4\n",
      "Domain: {2, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.32722973823547363, Loss_domain: 0.13215431571006775\n",
      "Epoch [2/3], Loss_main: 0.35823357105255127, Loss_domain: 0.06052635237574577\n",
      "Epoch [3/3], Loss_main: 0.3938932716846466, Loss_domain: 0.03481867164373398\n",
      "Training complete for fold 5\n",
      "Domain: {0, 2, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.4163625240325928, Loss_domain: 0.12279313057661057\n",
      "Epoch [2/3], Loss_main: 0.4785798192024231, Loss_domain: 0.051939595490694046\n",
      "Epoch [3/3], Loss_main: 0.4841751456260681, Loss_domain: 0.02615046128630638\n",
      "Training complete for fold 1\n",
      "Domain: {0, 2, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.31487447023391724, Loss_domain: 0.1409161537885666\n",
      "Epoch [2/3], Loss_main: 0.49012014269828796, Loss_domain: 0.055397067219018936\n",
      "Epoch [3/3], Loss_main: 0.38828474283218384, Loss_domain: 0.03733087331056595\n",
      "Training complete for fold 2\n",
      "Domain: {0, 2, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3894660472869873, Loss_domain: 0.12372362613677979\n",
      "Epoch [2/3], Loss_main: 0.3581194579601288, Loss_domain: 0.06095709651708603\n",
      "Epoch [3/3], Loss_main: 0.39158540964126587, Loss_domain: 0.024926453828811646\n",
      "Training complete for fold 3\n",
      "Domain: {0, 2, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.40143391489982605, Loss_domain: 0.14243033528327942\n",
      "Epoch [2/3], Loss_main: 0.5209810137748718, Loss_domain: 0.07082711160182953\n",
      "Epoch [3/3], Loss_main: 0.45003190636634827, Loss_domain: 0.039313655346632004\n",
      "Training complete for fold 4\n",
      "Domain: {0, 2, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.34723392128944397, Loss_domain: 0.17492881417274475\n",
      "Epoch [2/3], Loss_main: 0.3401470482349396, Loss_domain: 0.0721210241317749\n",
      "Epoch [3/3], Loss_main: 0.3889400064945221, Loss_domain: 0.040958795696496964\n",
      "Training complete for fold 5\n",
      "Domain: {1, 2, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3778797686100006, Loss_domain: 0.12539377808570862\n",
      "Epoch [2/3], Loss_main: 0.36679166555404663, Loss_domain: 0.06219387426972389\n",
      "Epoch [3/3], Loss_main: 0.5587149858474731, Loss_domain: 0.03774946555495262\n",
      "Training complete for fold 1\n",
      "Domain: {1, 2, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3848465085029602, Loss_domain: 0.12998981773853302\n",
      "Epoch [2/3], Loss_main: 0.42311757802963257, Loss_domain: 0.06280489265918732\n",
      "Epoch [3/3], Loss_main: 0.4130707085132599, Loss_domain: 0.03323349729180336\n",
      "Training complete for fold 2\n",
      "Domain: {1, 2, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3484151363372803, Loss_domain: 0.17216402292251587\n",
      "Epoch [2/3], Loss_main: 0.35350722074508667, Loss_domain: 0.06828215718269348\n",
      "Epoch [3/3], Loss_main: 0.43644875288009644, Loss_domain: 0.038461849093437195\n",
      "Training complete for fold 3\n",
      "Domain: {1, 2, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3353322446346283, Loss_domain: 0.16133543848991394\n",
      "Epoch [2/3], Loss_main: 0.4556187689304352, Loss_domain: 0.07810495048761368\n",
      "Epoch [3/3], Loss_main: 0.33030974864959717, Loss_domain: 0.039530832320451736\n",
      "Training complete for fold 4\n",
      "Domain: {1, 2, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3935033082962036, Loss_domain: 0.1582966446876526\n",
      "Epoch [2/3], Loss_main: 0.46607333421707153, Loss_domain: 0.06538562476634979\n",
      "Epoch [3/3], Loss_main: 0.44223248958587646, Loss_domain: 0.035245660692453384\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 2, 3}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3405487537384033, Loss_domain: 0.21365875005722046\n",
      "Epoch [2/3], Loss_main: 0.39781591296195984, Loss_domain: 0.08899242430925369\n",
      "Epoch [3/3], Loss_main: 0.4212818741798401, Loss_domain: 0.05518876761198044\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 2, 3}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.40010079741477966, Loss_domain: 0.12423649430274963\n",
      "Epoch [2/3], Loss_main: 0.3285403847694397, Loss_domain: 0.053869783878326416\n",
      "Epoch [3/3], Loss_main: 0.4971540868282318, Loss_domain: 0.029330356046557426\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 2, 3}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.37698981165885925, Loss_domain: 0.12747910618782043\n",
      "Epoch [2/3], Loss_main: 0.40242648124694824, Loss_domain: 0.04997639358043671\n",
      "Epoch [3/3], Loss_main: 0.31476306915283203, Loss_domain: 0.032343052327632904\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 2, 3}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3543672561645508, Loss_domain: 0.11256448924541473\n",
      "Epoch [2/3], Loss_main: 0.46523338556289673, Loss_domain: 0.052505262196063995\n",
      "Epoch [3/3], Loss_main: 0.3798759877681732, Loss_domain: 0.028884174302220345\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 2, 3}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.39615797996520996, Loss_domain: 0.12415234744548798\n",
      "Epoch [2/3], Loss_main: 0.4443715810775757, Loss_domain: 0.057940948754549026\n",
      "Epoch [3/3], Loss_main: 0.3259168863296509, Loss_domain: 0.03349299356341362\n",
      "Training complete for fold 5\n",
      "Domain: {4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.36739304661750793, Loss_domain: 0.15919968485832214\n",
      "Epoch [2/3], Loss_main: 0.412930428981781, Loss_domain: 0.06555305421352386\n",
      "Epoch [3/3], Loss_main: 0.42039355635643005, Loss_domain: 0.03810282424092293\n",
      "Training complete for fold 1\n",
      "Domain: {4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3623477816581726, Loss_domain: 0.1559787392616272\n",
      "Epoch [2/3], Loss_main: 0.43209224939346313, Loss_domain: 0.06640183925628662\n",
      "Epoch [3/3], Loss_main: 0.42398712038993835, Loss_domain: 0.04198571667075157\n",
      "Training complete for fold 2\n",
      "Domain: {4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.4063560366630554, Loss_domain: 0.19414283335208893\n",
      "Epoch [2/3], Loss_main: 0.4571065902709961, Loss_domain: 0.08614318817853928\n",
      "Epoch [3/3], Loss_main: 0.4012896418571472, Loss_domain: 0.03986399993300438\n",
      "Training complete for fold 3\n",
      "Domain: {4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3368059992790222, Loss_domain: 0.15220992267131805\n",
      "Epoch [2/3], Loss_main: 0.5762848854064941, Loss_domain: 0.07239183783531189\n",
      "Epoch [3/3], Loss_main: 0.4291568696498871, Loss_domain: 0.036255426704883575\n",
      "Training complete for fold 4\n",
      "Domain: {4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3793448805809021, Loss_domain: 0.1268150955438614\n",
      "Epoch [2/3], Loss_main: 0.4218886196613312, Loss_domain: 0.0566941499710083\n",
      "Epoch [3/3], Loss_main: 0.3581840395927429, Loss_domain: 0.03201134130358696\n",
      "Training complete for fold 5\n",
      "Domain: {0, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.343141108751297, Loss_domain: 0.11134939640760422\n",
      "Epoch [2/3], Loss_main: 0.36374667286872864, Loss_domain: 0.04768675938248634\n",
      "Epoch [3/3], Loss_main: 0.4359520673751831, Loss_domain: 0.018694844096899033\n",
      "Training complete for fold 1\n",
      "Domain: {0, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.355682373046875, Loss_domain: 0.1605730503797531\n",
      "Epoch [2/3], Loss_main: 0.461206316947937, Loss_domain: 0.0637252926826477\n",
      "Epoch [3/3], Loss_main: 0.3607715368270874, Loss_domain: 0.03735371306538582\n",
      "Training complete for fold 2\n",
      "Domain: {0, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.35193532705307007, Loss_domain: 0.16237188875675201\n",
      "Epoch [2/3], Loss_main: 0.41634389758110046, Loss_domain: 0.08494338393211365\n",
      "Epoch [3/3], Loss_main: 0.42513513565063477, Loss_domain: 0.04498542472720146\n",
      "Training complete for fold 3\n",
      "Domain: {0, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.32757148146629333, Loss_domain: 0.11382926255464554\n",
      "Epoch [2/3], Loss_main: 0.38871103525161743, Loss_domain: 0.04872173070907593\n",
      "Epoch [3/3], Loss_main: 0.4346486032009125, Loss_domain: 0.03170282393693924\n",
      "Training complete for fold 4\n",
      "Domain: {0, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.38189229369163513, Loss_domain: 0.11528695374727249\n",
      "Epoch [2/3], Loss_main: 0.3782927691936493, Loss_domain: 0.05032059922814369\n",
      "Epoch [3/3], Loss_main: 0.40827566385269165, Loss_domain: 0.026138752698898315\n",
      "Training complete for fold 5\n",
      "Domain: {1, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3639334440231323, Loss_domain: 0.17281381785869598\n",
      "Epoch [2/3], Loss_main: 0.37231969833374023, Loss_domain: 0.07737068086862564\n",
      "Epoch [3/3], Loss_main: 0.3614838421344757, Loss_domain: 0.04506311193108559\n",
      "Training complete for fold 1\n",
      "Domain: {1, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.4101489782333374, Loss_domain: 0.12302099913358688\n",
      "Epoch [2/3], Loss_main: 0.39865899085998535, Loss_domain: 0.055966682732105255\n",
      "Epoch [3/3], Loss_main: 0.3654099702835083, Loss_domain: 0.030753763392567635\n",
      "Training complete for fold 2\n",
      "Domain: {1, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.36348262429237366, Loss_domain: 0.16121670603752136\n",
      "Epoch [2/3], Loss_main: 0.4114993214607239, Loss_domain: 0.07402960956096649\n",
      "Epoch [3/3], Loss_main: 0.4086475074291229, Loss_domain: 0.03821039944887161\n",
      "Training complete for fold 3\n",
      "Domain: {1, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.4023332893848419, Loss_domain: 0.11910942941904068\n",
      "Epoch [2/3], Loss_main: 0.4055456519126892, Loss_domain: 0.057677652686834335\n",
      "Epoch [3/3], Loss_main: 0.42044469714164734, Loss_domain: 0.030216781422495842\n",
      "Training complete for fold 4\n",
      "Domain: {1, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3581113815307617, Loss_domain: 0.13854385912418365\n",
      "Epoch [2/3], Loss_main: 0.46857011318206787, Loss_domain: 0.05823655053973198\n",
      "Epoch [3/3], Loss_main: 0.4196709096431732, Loss_domain: 0.033533744513988495\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3902996778488159, Loss_domain: 0.16397446393966675\n",
      "Epoch [2/3], Loss_main: 0.35796505212783813, Loss_domain: 0.07350829243659973\n",
      "Epoch [3/3], Loss_main: 0.39927756786346436, Loss_domain: 0.0346854142844677\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.36213865876197815, Loss_domain: 0.1558552086353302\n",
      "Epoch [2/3], Loss_main: 0.5089906454086304, Loss_domain: 0.07242897152900696\n",
      "Epoch [3/3], Loss_main: 0.3359871506690979, Loss_domain: 0.03643012046813965\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3401402235031128, Loss_domain: 0.1656249612569809\n",
      "Epoch [2/3], Loss_main: 0.4377480149269104, Loss_domain: 0.07291863858699799\n",
      "Epoch [3/3], Loss_main: 0.34594741463661194, Loss_domain: 0.04330722242593765\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.28307825326919556, Loss_domain: 0.21845583617687225\n",
      "Epoch [2/3], Loss_main: 0.38129550218582153, Loss_domain: 0.1032630056142807\n",
      "Epoch [3/3], Loss_main: 0.3821524977684021, Loss_domain: 0.050130054354667664\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.32226261496543884, Loss_domain: 0.18832537531852722\n",
      "Epoch [2/3], Loss_main: 0.38165217638015747, Loss_domain: 0.0910770446062088\n",
      "Epoch [3/3], Loss_main: 0.4099384844303131, Loss_domain: 0.04650557413697243\n",
      "Training complete for fold 5\n",
      "Domain: {2, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.30866900086402893, Loss_domain: 0.16113489866256714\n",
      "Epoch [2/3], Loss_main: 0.3775630593299866, Loss_domain: 0.07163836807012558\n",
      "Epoch [3/3], Loss_main: 0.4215542674064636, Loss_domain: 0.03746248781681061\n",
      "Training complete for fold 1\n",
      "Domain: {2, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.36941415071487427, Loss_domain: 0.13315285742282867\n",
      "Epoch [2/3], Loss_main: 0.37716561555862427, Loss_domain: 0.06063465029001236\n",
      "Epoch [3/3], Loss_main: 0.33565863966941833, Loss_domain: 0.03212523087859154\n",
      "Training complete for fold 2\n",
      "Domain: {2, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.31597062945365906, Loss_domain: 0.15598297119140625\n",
      "Epoch [2/3], Loss_main: 0.37319648265838623, Loss_domain: 0.06687726080417633\n",
      "Epoch [3/3], Loss_main: 0.49224838614463806, Loss_domain: 0.033161718398332596\n",
      "Training complete for fold 3\n",
      "Domain: {2, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.35863858461380005, Loss_domain: 0.1648082137107849\n",
      "Epoch [2/3], Loss_main: 0.35843053460121155, Loss_domain: 0.054568685591220856\n",
      "Epoch [3/3], Loss_main: 0.4360114336013794, Loss_domain: 0.03785412013530731\n",
      "Training complete for fold 4\n",
      "Domain: {2, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.350469708442688, Loss_domain: 0.1343727558851242\n",
      "Epoch [2/3], Loss_main: 0.36632901430130005, Loss_domain: 0.0638340562582016\n",
      "Epoch [3/3], Loss_main: 0.32867011427879333, Loss_domain: 0.03770100325345993\n",
      "Training complete for fold 5\n",
      "Domain: {0, 2, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3453032374382019, Loss_domain: 0.1798122674226761\n",
      "Epoch [2/3], Loss_main: 0.4178342819213867, Loss_domain: 0.07266966998577118\n",
      "Epoch [3/3], Loss_main: 0.3832200765609741, Loss_domain: 0.04517771303653717\n",
      "Training complete for fold 1\n",
      "Domain: {0, 2, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3763276934623718, Loss_domain: 0.1226348876953125\n",
      "Epoch [2/3], Loss_main: 0.3569090664386749, Loss_domain: 0.05713510140776634\n",
      "Epoch [3/3], Loss_main: 0.3501705825328827, Loss_domain: 0.03396648168563843\n",
      "Training complete for fold 2\n",
      "Domain: {0, 2, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3763655424118042, Loss_domain: 0.10427466779947281\n",
      "Epoch [2/3], Loss_main: 0.326342910528183, Loss_domain: 0.042987626045942307\n",
      "Epoch [3/3], Loss_main: 0.43006250262260437, Loss_domain: 0.02566027268767357\n",
      "Training complete for fold 3\n",
      "Domain: {0, 2, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.44386571645736694, Loss_domain: 0.12196684628725052\n",
      "Epoch [2/3], Loss_main: 0.479810506105423, Loss_domain: 0.04884801432490349\n",
      "Epoch [3/3], Loss_main: 0.34255900979042053, Loss_domain: 0.02633720636367798\n",
      "Training complete for fold 4\n",
      "Domain: {0, 2, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.28042471408843994, Loss_domain: 0.169718936085701\n",
      "Epoch [2/3], Loss_main: 0.44298920035362244, Loss_domain: 0.06339874863624573\n",
      "Epoch [3/3], Loss_main: 0.324737548828125, Loss_domain: 0.03666189685463905\n",
      "Training complete for fold 5\n",
      "Domain: {1, 2, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.35565727949142456, Loss_domain: 0.16947001218795776\n",
      "Epoch [2/3], Loss_main: 0.3443673253059387, Loss_domain: 0.07589273154735565\n",
      "Epoch [3/3], Loss_main: 0.4183711111545563, Loss_domain: 0.04214949160814285\n",
      "Training complete for fold 1\n",
      "Domain: {1, 2, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3631996512413025, Loss_domain: 0.13726694881916046\n",
      "Epoch [2/3], Loss_main: 0.44614169001579285, Loss_domain: 0.057174526154994965\n",
      "Epoch [3/3], Loss_main: 0.464408278465271, Loss_domain: 0.03967630863189697\n",
      "Training complete for fold 2\n",
      "Domain: {1, 2, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3500734567642212, Loss_domain: 0.13226275146007538\n",
      "Epoch [2/3], Loss_main: 0.40853598713874817, Loss_domain: 0.06152012199163437\n",
      "Epoch [3/3], Loss_main: 0.4694114029407501, Loss_domain: 0.039904508739709854\n",
      "Training complete for fold 3\n",
      "Domain: {1, 2, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.34792160987854004, Loss_domain: 0.16813330352306366\n",
      "Epoch [2/3], Loss_main: 0.3965964913368225, Loss_domain: 0.07102010399103165\n",
      "Epoch [3/3], Loss_main: 0.432343989610672, Loss_domain: 0.035988032817840576\n",
      "Training complete for fold 4\n",
      "Domain: {1, 2, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.39853590726852417, Loss_domain: 0.1268593668937683\n",
      "Epoch [2/3], Loss_main: 0.4736884832382202, Loss_domain: 0.05416859686374664\n",
      "Epoch [3/3], Loss_main: 0.5183688402175903, Loss_domain: 0.032033491879701614\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 2, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.41103246808052063, Loss_domain: 0.12251565605401993\n",
      "Epoch [2/3], Loss_main: 0.42319896817207336, Loss_domain: 0.048373542726039886\n",
      "Epoch [3/3], Loss_main: 0.41732078790664673, Loss_domain: 0.026200775057077408\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 2, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.4304339289665222, Loss_domain: 0.15461592376232147\n",
      "Epoch [2/3], Loss_main: 0.4048375189304352, Loss_domain: 0.053311701864004135\n",
      "Epoch [3/3], Loss_main: 0.39244791865348816, Loss_domain: 0.0334613211452961\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 2, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3229966163635254, Loss_domain: 0.14345450699329376\n",
      "Epoch [2/3], Loss_main: 0.3961339592933655, Loss_domain: 0.06734989583492279\n",
      "Epoch [3/3], Loss_main: 0.4528326392173767, Loss_domain: 0.037826571613550186\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 2, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.363748162984848, Loss_domain: 0.147207111120224\n",
      "Epoch [2/3], Loss_main: 0.39445000886917114, Loss_domain: 0.05470823124051094\n",
      "Epoch [3/3], Loss_main: 0.44085627794265747, Loss_domain: 0.03276090323925018\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 2, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3660063147544861, Loss_domain: 0.10817432403564453\n",
      "Epoch [2/3], Loss_main: 0.5107788443565369, Loss_domain: 0.040814854204654694\n",
      "Epoch [3/3], Loss_main: 0.48891422152519226, Loss_domain: 0.024059977382421494\n",
      "Training complete for fold 5\n",
      "Domain: {3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3969613313674927, Loss_domain: 0.10486124455928802\n",
      "Epoch [2/3], Loss_main: 0.477113276720047, Loss_domain: 0.055063050240278244\n",
      "Epoch [3/3], Loss_main: 0.44655999541282654, Loss_domain: 0.02781110629439354\n",
      "Training complete for fold 1\n",
      "Domain: {3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.42475223541259766, Loss_domain: 0.12539590895175934\n",
      "Epoch [2/3], Loss_main: 0.40709662437438965, Loss_domain: 0.04838241636753082\n",
      "Epoch [3/3], Loss_main: 0.368721067905426, Loss_domain: 0.024306854233145714\n",
      "Training complete for fold 2\n",
      "Domain: {3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3810187578201294, Loss_domain: 0.218506321310997\n",
      "Epoch [2/3], Loss_main: 0.3511534035205841, Loss_domain: 0.10238209366798401\n",
      "Epoch [3/3], Loss_main: 0.38200464844703674, Loss_domain: 0.05514591187238693\n",
      "Training complete for fold 3\n",
      "Domain: {3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.42581236362457275, Loss_domain: 0.12324631959199905\n",
      "Epoch [2/3], Loss_main: 0.4055079221725464, Loss_domain: 0.04727619141340256\n",
      "Epoch [3/3], Loss_main: 0.37514346837997437, Loss_domain: 0.031532298773527145\n",
      "Training complete for fold 4\n",
      "Domain: {3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.35157087445259094, Loss_domain: 0.1745809018611908\n",
      "Epoch [2/3], Loss_main: 0.3670606017112732, Loss_domain: 0.07455848157405853\n",
      "Epoch [3/3], Loss_main: 0.4725932776927948, Loss_domain: 0.041394975036382675\n",
      "Training complete for fold 5\n",
      "Domain: {0, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.33627527952194214, Loss_domain: 0.1378890573978424\n",
      "Epoch [2/3], Loss_main: 0.4219552278518677, Loss_domain: 0.05978047102689743\n",
      "Epoch [3/3], Loss_main: 0.508001446723938, Loss_domain: 0.0256422720849514\n",
      "Training complete for fold 1\n",
      "Domain: {0, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.4464051425457001, Loss_domain: 0.18700876832008362\n",
      "Epoch [2/3], Loss_main: 0.41585591435432434, Loss_domain: 0.08902129530906677\n",
      "Epoch [3/3], Loss_main: 0.3487434983253479, Loss_domain: 0.04492674395442009\n",
      "Training complete for fold 2\n",
      "Domain: {0, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.37315282225608826, Loss_domain: 0.1253276765346527\n",
      "Epoch [2/3], Loss_main: 0.4164051413536072, Loss_domain: 0.05054410547018051\n",
      "Epoch [3/3], Loss_main: 0.445779025554657, Loss_domain: 0.033100709319114685\n",
      "Training complete for fold 3\n",
      "Domain: {0, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.37769657373428345, Loss_domain: 0.1649644821882248\n",
      "Epoch [2/3], Loss_main: 0.43773794174194336, Loss_domain: 0.08147399872541428\n",
      "Epoch [3/3], Loss_main: 0.41612711548805237, Loss_domain: 0.03634979948401451\n",
      "Training complete for fold 4\n",
      "Domain: {0, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.41540390253067017, Loss_domain: 0.11139707267284393\n",
      "Epoch [2/3], Loss_main: 0.4989642798900604, Loss_domain: 0.04550466313958168\n",
      "Epoch [3/3], Loss_main: 0.42301902174949646, Loss_domain: 0.028707893565297127\n",
      "Training complete for fold 5\n",
      "Domain: {1, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.3753831386566162, Loss_domain: 0.1459953933954239\n",
      "Epoch [2/3], Loss_main: 0.45732125639915466, Loss_domain: 0.06086108833551407\n",
      "Epoch [3/3], Loss_main: 0.41560330986976624, Loss_domain: 0.03871581703424454\n",
      "Training complete for fold 1\n",
      "Domain: {1, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.29778289794921875, Loss_domain: 0.18237249553203583\n",
      "Epoch [2/3], Loss_main: 0.3710617423057556, Loss_domain: 0.07610075175762177\n",
      "Epoch [3/3], Loss_main: 0.3941943645477295, Loss_domain: 0.04214441403746605\n",
      "Training complete for fold 2\n",
      "Domain: {1, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.39481011033058167, Loss_domain: 0.14091947674751282\n",
      "Epoch [2/3], Loss_main: 0.4288572669029236, Loss_domain: 0.061462581157684326\n",
      "Epoch [3/3], Loss_main: 0.38386598229408264, Loss_domain: 0.033417847007513046\n",
      "Training complete for fold 3\n",
      "Domain: {1, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.41632556915283203, Loss_domain: 0.14324133098125458\n",
      "Epoch [2/3], Loss_main: 0.38772058486938477, Loss_domain: 0.05655250325798988\n",
      "Epoch [3/3], Loss_main: 0.3928050696849823, Loss_domain: 0.028185304254293442\n",
      "Training complete for fold 4\n",
      "Domain: {1, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.3651087284088135, Loss_domain: 0.18778620660305023\n",
      "Epoch [2/3], Loss_main: 0.4259452819824219, Loss_domain: 0.07581456005573273\n",
      "Epoch [3/3], Loss_main: 0.4264083504676819, Loss_domain: 0.04937075078487396\n",
      "Training complete for fold 5\n",
      "Domain: {0, 1, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.2771388292312622, Loss_domain: 0.17923620343208313\n",
      "Epoch [2/3], Loss_main: 0.41598641872406006, Loss_domain: 0.08390085399150848\n",
      "Epoch [3/3], Loss_main: 0.32920244336128235, Loss_domain: 0.053593192249536514\n",
      "Training complete for fold 1\n",
      "Domain: {0, 1, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3824232220649719, Loss_domain: 0.1439174860715866\n",
      "Epoch [2/3], Loss_main: 0.4642099440097809, Loss_domain: 0.047445572912693024\n",
      "Epoch [3/3], Loss_main: 0.4081442654132843, Loss_domain: 0.03935892507433891\n",
      "Training complete for fold 2\n",
      "Domain: {0, 1, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.38874226808547974, Loss_domain: 0.14559249579906464\n",
      "Epoch [2/3], Loss_main: 0.47670286893844604, Loss_domain: 0.06264610588550568\n",
      "Epoch [3/3], Loss_main: 0.4269065260887146, Loss_domain: 0.04267589747905731\n",
      "Training complete for fold 3\n",
      "Domain: {0, 1, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.3684728443622589, Loss_domain: 0.1368217170238495\n",
      "Epoch [2/3], Loss_main: 0.3396925628185272, Loss_domain: 0.0636957660317421\n",
      "Epoch [3/3], Loss_main: 0.48694759607315063, Loss_domain: 0.034028053283691406\n",
      "Training complete for fold 4\n",
      "Domain: {0, 1, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.2957574129104614, Loss_domain: 0.14404241740703583\n",
      "Epoch [2/3], Loss_main: 0.37339508533477783, Loss_domain: 0.059684328734874725\n",
      "Epoch [3/3], Loss_main: 0.4260142147541046, Loss_domain: 0.03148597851395607\n",
      "Training complete for fold 5\n",
      "Domain: {2, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.27195245027542114, Loss_domain: 0.19978497922420502\n",
      "Epoch [2/3], Loss_main: 0.4850306510925293, Loss_domain: 0.08837520331144333\n",
      "Epoch [3/3], Loss_main: 0.37319210171699524, Loss_domain: 0.04828913137316704\n",
      "Training complete for fold 1\n",
      "Domain: {2, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.32880017161369324, Loss_domain: 0.17193159461021423\n",
      "Epoch [2/3], Loss_main: 0.3863670825958252, Loss_domain: 0.06696151196956635\n",
      "Epoch [3/3], Loss_main: 0.43252885341644287, Loss_domain: 0.043316178023815155\n",
      "Training complete for fold 2\n",
      "Domain: {2, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.33415526151657104, Loss_domain: 0.11742056906223297\n",
      "Epoch [2/3], Loss_main: 0.4521123766899109, Loss_domain: 0.050397347658872604\n",
      "Epoch [3/3], Loss_main: 0.38214409351348877, Loss_domain: 0.02832728810608387\n",
      "Training complete for fold 3\n",
      "Domain: {2, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.35445636510849, Loss_domain: 0.18095755577087402\n",
      "Epoch [2/3], Loss_main: 0.3871627449989319, Loss_domain: 0.08593134582042694\n",
      "Epoch [3/3], Loss_main: 0.46317028999328613, Loss_domain: 0.04257421940565109\n",
      "Training complete for fold 4\n",
      "Domain: {2, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.32941365242004395, Loss_domain: 0.15171363949775696\n",
      "Epoch [2/3], Loss_main: 0.4356786608695984, Loss_domain: 0.05652058497071266\n",
      "Epoch [3/3], Loss_main: 0.28523871302604675, Loss_domain: 0.043962057679891586\n",
      "Training complete for fold 5\n",
      "Domain: {0, 2, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.4431840479373932, Loss_domain: 0.1172771155834198\n",
      "Epoch [2/3], Loss_main: 0.43182557821273804, Loss_domain: 0.0537654347717762\n",
      "Epoch [3/3], Loss_main: 0.3898219168186188, Loss_domain: 0.029402000829577446\n",
      "Training complete for fold 1\n",
      "Domain: {0, 2, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.39215922355651855, Loss_domain: 0.16319884359836578\n",
      "Epoch [2/3], Loss_main: 0.4446661174297333, Loss_domain: 0.06724961847066879\n",
      "Epoch [3/3], Loss_main: 0.4227901101112366, Loss_domain: 0.036860279738903046\n",
      "Training complete for fold 2\n",
      "Domain: {0, 2, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.3780961334705353, Loss_domain: 0.11050217598676682\n",
      "Epoch [2/3], Loss_main: 0.38023197650909424, Loss_domain: 0.0530381016433239\n",
      "Epoch [3/3], Loss_main: 0.5167771577835083, Loss_domain: 0.030317988246679306\n",
      "Training complete for fold 3\n",
      "Domain: {0, 2, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.4262724220752716, Loss_domain: 0.11923566460609436\n",
      "Epoch [2/3], Loss_main: 0.4402000308036804, Loss_domain: 0.05894480645656586\n",
      "Epoch [3/3], Loss_main: 0.41898614168167114, Loss_domain: 0.03210807964205742\n",
      "Training complete for fold 4\n",
      "Domain: {0, 2, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.34353306889533997, Loss_domain: 0.13567036390304565\n",
      "Epoch [2/3], Loss_main: 0.4021899700164795, Loss_domain: 0.05303363874554634\n",
      "Epoch [3/3], Loss_main: 0.45026540756225586, Loss_domain: 0.031738266348838806\n",
      "Training complete for fold 5\n",
      "Domain: {1, 2, 3, 4}, Fold 1\n",
      "Epoch [1/3], Loss_main: 0.39637190103530884, Loss_domain: 0.1841496080160141\n",
      "Epoch [2/3], Loss_main: 0.45942533016204834, Loss_domain: 0.07939566671848297\n",
      "Epoch [3/3], Loss_main: 0.33266496658325195, Loss_domain: 0.04102051630616188\n",
      "Training complete for fold 1\n",
      "Domain: {1, 2, 3, 4}, Fold 2\n",
      "Epoch [1/3], Loss_main: 0.3376622498035431, Loss_domain: 0.1337137222290039\n",
      "Epoch [2/3], Loss_main: 0.3244401514530182, Loss_domain: 0.06122090294957161\n",
      "Epoch [3/3], Loss_main: 0.5312821269035339, Loss_domain: 0.03407778963446617\n",
      "Training complete for fold 2\n",
      "Domain: {1, 2, 3, 4}, Fold 3\n",
      "Epoch [1/3], Loss_main: 0.37667256593704224, Loss_domain: 0.13260804116725922\n",
      "Epoch [2/3], Loss_main: 0.3645313084125519, Loss_domain: 0.05654405802488327\n",
      "Epoch [3/3], Loss_main: 0.4164345860481262, Loss_domain: 0.032252777367830276\n",
      "Training complete for fold 3\n",
      "Domain: {1, 2, 3, 4}, Fold 4\n",
      "Epoch [1/3], Loss_main: 0.29993659257888794, Loss_domain: 0.2074132114648819\n",
      "Epoch [2/3], Loss_main: 0.3312614858150482, Loss_domain: 0.08463975042104721\n",
      "Epoch [3/3], Loss_main: 0.405506432056427, Loss_domain: 0.046369731426239014\n",
      "Training complete for fold 4\n",
      "Domain: {1, 2, 3, 4}, Fold 5\n",
      "Epoch [1/3], Loss_main: 0.39833950996398926, Loss_domain: 0.12864607572555542\n",
      "Epoch [2/3], Loss_main: 0.4040534794330597, Loss_domain: 0.04475266858935356\n",
      "Epoch [3/3], Loss_main: 0.3931814134120941, Loss_domain: 0.031074894592165947\n",
      "Training complete for fold 5\n",
      "{'Cluster(0)': [0.8177052999417589, 0.8306214990664177, 0.9110590988882388, 0.8689828380075345, 0.7723212401538713], 'Cluster(1)': [0.8112987769365172, 0.8230322239830956, 0.9116442363955529, 0.8650749583564686, 0.7625157131198768], 'Cluster(0, 1)': [0.8066394874781596, 0.842671194114318, 0.8712697483908719, 0.8567318757192175, 0.7752194096830296], 'Cluster(2)': [0.8107163657542225, 0.8371515318796577, 0.8873610298420129, 0.8615253515125693, 0.7734554716434531], 'Cluster(0, 2)': [0.815569792273345, 0.8380821917808219, 0.8949678174370977, 0.8655913978494624, 0.7769703484184912], 'Cluster(1, 2)': [0.8132401475441662, 0.8191268191268192, 0.9221767115272089, 0.8676025323424167, 0.7602805081005924], 'Cluster(0, 1, 2)': [0.8145991069695205, 0.8311374025275612, 0.9043300175541252, 0.8661902760263416, 0.7709763186443448], 'Cluster(3)': [0.8136284216656959, 0.8361597374179431, 0.8943826799297835, 0.8642917726887193, 0.7743696434847993], 'Cluster(0, 3)': [0.8237235488254708, 0.8283621140763998, 0.9262726740784084, 0.874585635359116, 0.7738691702763653], 'Cluster(1, 3)': [0.8163463405164045, 0.8190500774393392, 0.9283206553540082, 0.8702687877125617, 0.7619098949014703], 'Cluster(0, 1, 3)': [0.8171228887594641, 0.8292553191489361, 0.9122293739028672, 0.8687656728893842, 0.7708867585036552], 'Cluster(2, 3)': [0.8190642593671131, 0.8361276365603029, 0.9046225863077824, 0.8690275435637999, 0.7774699775162686], 'Cluster(0, 2, 3)': [0.8114929139972821, 0.8193683111459149, 0.9183733177296665, 0.8660504897227204, 0.7595328792918384], 'Cluster(1, 2, 3)': [0.8132401475441662, 0.8236689509752241, 0.914277355178467, 0.8666112035496395, 0.7641207895338382], 'Cluster(0, 1, 2, 3)': [0.8140166957872258, 0.8243670886075949, 0.914569923932124, 0.867128987517337, 0.7651326249781798], 'Cluster(4)': [0.8147932440302854, 0.8240399789584429, 0.9166179052077238, 0.8678670360110803, 0.7652910645484667], 'Cluster(0, 4)': [0.815569792273345, 0.8371584699453551, 0.8964306612053833, 0.8657812941508901, 0.7762591851901124], 'Cluster(1, 4)': [0.8208114929139972, 0.8309896524276996, 0.9163253364540667, 0.8715736746904132, 0.7743773248917767], 'Cluster(0, 1, 4)': [0.8103280916326927, 0.8423562412342216, 0.8785839672322996, 0.8600887870542747, 0.7771454169687175], 'Cluster(2, 4)': [0.8111046398757523, 0.8304947283049473, 0.8987712112346401, 0.8632850920331601, 0.7684854325071064], 'Cluster(0, 2, 4)': [0.8167346146379344, 0.8346861471861472, 0.9025746050321826, 0.8673039077874614, 0.7750034017659472], 'Cluster(1, 2, 4)': [0.8184818481848185, 0.8340059187516815, 0.9069631363370392, 0.8689558514365803, 0.7754665652833493], 'Cluster(0, 1, 2, 4)': [0.8192583964278781, 0.8383673469387755, 0.9014043300175542, 0.86874383194699, 0.7793230536412064], 'Cluster(3, 4)': [0.8118811881188119, 0.8231723409870678, 0.9125219426565243, 0.8655473844873041, 0.7629545662503625], 'Cluster(0, 3, 4)': [0.8128518734226364, 0.8354291962821214, 0.8940901111761264, 0.8637648388920294, 0.7733578080404578], 'Cluster(1, 3, 4)': [0.8167346146379344, 0.824501573976915, 0.9195435927442949, 0.869432918395574, 0.7667539083167523], 'Cluster(0, 1, 3, 4)': [0.8177052999417589, 0.8247314645009169, 0.9210064365125804, 0.8702142363510712, 0.7674853302008949], 'Cluster(2, 3, 4)': [0.8163463405164045, 0.8315450643776824, 0.9069631363370392, 0.8676182479708928, 0.7722928780358018], 'Cluster(0, 2, 3, 4)': [0.8043098427489808, 0.8464634847613571, 0.8613224107665302, 0.8538283062645011, 0.776593115365954], 'Cluster(1, 2, 3, 4)': [0.8140166957872258, 0.8285256410256411, 0.9075482738443534, 0.866238480871265, 0.7685462084743984]}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for target_domain in subsets:\n",
    "    # powerset_train is a train function with modified script:\n",
    "                    # domain= torch.tensor([1 if x in target_domain else 0 for x in domain])\n",
    "                    # domain = domain.to(device)\n",
    "        \n",
    "    metrics = powerset_train(target_domain=target_domain)\n",
    "    name = 'Cluster(' + ', '.join([str(elem) for i,elem in enumerate(target_domain)]) + ')'\n",
    "    results[name] = metrics\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame with index and columns\n",
    "metrics = pd.read_csv('metrics.csv', index_col=0)\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC'])\n",
    "\n",
    "metrics = pd.concat([metrics, results_df])\n",
    "metrics.to_csv('metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
