{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\", index_col=0)\n",
    "df.rename(columns={'Real': 'Prediction', '40':'SpoofType', '41':'Illumination', '42':'Environment', '43':'Spoof'}, inplace=True)\n",
    "# the prediction by silent face takes values 0-2. Gotta convert it.\n",
    "df['Prediction'] = df['Prediction'].replace({0.0: 1, 1.0: 0, 2.0: 1})\n",
    "df['Prediction'] = df['Prediction'].astype(int)\n",
    "\n",
    "embeddings = pd.read_csv('dropout_embeddings.csv', index_col=0)\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "common_index = df.index.intersection(embeddings.index)\n",
    "df = df.loc[common_index]\n",
    "embeddings = embeddings.loc[common_index]\n",
    "\n",
    "pd.testing.assert_series_equal(df.index.to_series(), embeddings.index.to_series())\n",
    "\n",
    "\n",
    "gmm = GaussianMixture(n_components=5, random_state=42)\n",
    "clusters = gmm.fit_predict(embeddings)\n",
    "\n",
    "\n",
    "# embeddings = pd.get_dummies(embeddings, columns=['Cluster'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings['Spoof'] = df['Spoof']\n",
    "embeddings['Cluster'] = clusters\n",
    "embeddings['Illumination'] = df['Illumination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(20, 4))\n",
    "\n",
    "# Create a histogram for each cluster\n",
    "for cluster_id, group in embeddings.groupby('Cluster'):\n",
    "    # Count the frequency of each value of the current feature in the current cluster\n",
    "    counts = group[\"Spoof\"].value_counts()\n",
    "\n",
    "    # Create a bar chart of the counts in the current subplot\n",
    "    axs[cluster_id].bar(counts.index, counts.values)\n",
    "    axs[cluster_id].set_xlabel(\"Spoof\")\n",
    "    axs[cluster_id].set_ylabel(\"Count\")\n",
    "    axs[cluster_id].set_title(f\"Cluster {cluster_id}\")\n",
    "\n",
    "# Add a title to the overall plot\n",
    "plt.suptitle(\"Histograms of Spoof\")\n",
    "\n",
    "# Adjust the layout of the subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the overall plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>Spoof</th>\n",
       "      <th>Cluster</th>\n",
       "      <th>Illumination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Data/test/6964/spoof/494405.png</th>\n",
       "      <td>0.420929</td>\n",
       "      <td>0.391198</td>\n",
       "      <td>0.395073</td>\n",
       "      <td>-0.421826</td>\n",
       "      <td>0.368112</td>\n",
       "      <td>0.353278</td>\n",
       "      <td>0.749079</td>\n",
       "      <td>0.455125</td>\n",
       "      <td>-0.878119</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688752</td>\n",
       "      <td>-0.765734</td>\n",
       "      <td>-0.755238</td>\n",
       "      <td>0.726697</td>\n",
       "      <td>-0.765685</td>\n",
       "      <td>-0.470916</td>\n",
       "      <td>0.775886</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6407/spoof/494411.png</th>\n",
       "      <td>-0.147611</td>\n",
       "      <td>0.357509</td>\n",
       "      <td>-0.029671</td>\n",
       "      <td>-0.238791</td>\n",
       "      <td>0.286075</td>\n",
       "      <td>-0.134124</td>\n",
       "      <td>0.624391</td>\n",
       "      <td>0.178886</td>\n",
       "      <td>-0.596819</td>\n",
       "      <td>-0.246836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680034</td>\n",
       "      <td>-0.574118</td>\n",
       "      <td>-0.704340</td>\n",
       "      <td>0.688574</td>\n",
       "      <td>-0.656510</td>\n",
       "      <td>0.135470</td>\n",
       "      <td>0.686906</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6153/spoof/494412.png</th>\n",
       "      <td>-0.105897</td>\n",
       "      <td>-0.249867</td>\n",
       "      <td>-0.077944</td>\n",
       "      <td>0.181672</td>\n",
       "      <td>-0.120957</td>\n",
       "      <td>-0.085050</td>\n",
       "      <td>0.615739</td>\n",
       "      <td>-0.094033</td>\n",
       "      <td>-0.588235</td>\n",
       "      <td>0.141131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588970</td>\n",
       "      <td>-0.605586</td>\n",
       "      <td>-0.580930</td>\n",
       "      <td>0.522196</td>\n",
       "      <td>-0.592422</td>\n",
       "      <td>0.108543</td>\n",
       "      <td>0.643653</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6411/live/494418.png</th>\n",
       "      <td>0.441198</td>\n",
       "      <td>0.528195</td>\n",
       "      <td>0.404773</td>\n",
       "      <td>-0.509041</td>\n",
       "      <td>0.446964</td>\n",
       "      <td>0.322769</td>\n",
       "      <td>0.645940</td>\n",
       "      <td>0.537904</td>\n",
       "      <td>-0.691915</td>\n",
       "      <td>-0.460678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608733</td>\n",
       "      <td>-0.600595</td>\n",
       "      <td>-0.675778</td>\n",
       "      <td>0.649979</td>\n",
       "      <td>-0.616289</td>\n",
       "      <td>-0.460027</td>\n",
       "      <td>0.677907</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data/test/6336/spoof/494419.png</th>\n",
       "      <td>0.753356</td>\n",
       "      <td>0.419226</td>\n",
       "      <td>0.744619</td>\n",
       "      <td>-0.425761</td>\n",
       "      <td>0.417521</td>\n",
       "      <td>0.629097</td>\n",
       "      <td>0.054008</td>\n",
       "      <td>0.498429</td>\n",
       "      <td>-0.053692</td>\n",
       "      <td>-0.445463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013625</td>\n",
       "      <td>-0.103442</td>\n",
       "      <td>0.036649</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>-0.089932</td>\n",
       "      <td>-0.705690</td>\n",
       "      <td>0.065030</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        1         2         3         4  \\\n",
       "Data/test/6964/spoof/494405.png  0.420929  0.391198  0.395073 -0.421826   \n",
       "Data/test/6407/spoof/494411.png -0.147611  0.357509 -0.029671 -0.238791   \n",
       "Data/test/6153/spoof/494412.png -0.105897 -0.249867 -0.077944  0.181672   \n",
       "Data/test/6411/live/494418.png   0.441198  0.528195  0.404773 -0.509041   \n",
       "Data/test/6336/spoof/494419.png  0.753356  0.419226  0.744619 -0.425761   \n",
       "\n",
       "                                        5         6         7         8  \\\n",
       "Data/test/6964/spoof/494405.png  0.368112  0.353278  0.749079  0.455125   \n",
       "Data/test/6407/spoof/494411.png  0.286075 -0.134124  0.624391  0.178886   \n",
       "Data/test/6153/spoof/494412.png -0.120957 -0.085050  0.615739 -0.094033   \n",
       "Data/test/6411/live/494418.png   0.446964  0.322769  0.645940  0.537904   \n",
       "Data/test/6336/spoof/494419.png  0.417521  0.629097  0.054008  0.498429   \n",
       "\n",
       "                                        9        10  ...       122       123  \\\n",
       "Data/test/6964/spoof/494405.png -0.878119 -0.339846  ...  0.688752 -0.765734   \n",
       "Data/test/6407/spoof/494411.png -0.596819 -0.246836  ...  0.680034 -0.574118   \n",
       "Data/test/6153/spoof/494412.png -0.588235  0.141131  ...  0.588970 -0.605586   \n",
       "Data/test/6411/live/494418.png  -0.691915 -0.460678  ...  0.608733 -0.600595   \n",
       "Data/test/6336/spoof/494419.png -0.053692 -0.445463  ...  0.013625 -0.103442   \n",
       "\n",
       "                                      124       125       126       127  \\\n",
       "Data/test/6964/spoof/494405.png -0.755238  0.726697 -0.765685 -0.470916   \n",
       "Data/test/6407/spoof/494411.png -0.704340  0.688574 -0.656510  0.135470   \n",
       "Data/test/6153/spoof/494412.png -0.580930  0.522196 -0.592422  0.108543   \n",
       "Data/test/6411/live/494418.png  -0.675778  0.649979 -0.616289 -0.460027   \n",
       "Data/test/6336/spoof/494419.png  0.036649  0.001247 -0.089932 -0.705690   \n",
       "\n",
       "                                      128  Spoof  Cluster  Illumination  \n",
       "Data/test/6964/spoof/494405.png  0.775886      1        2             2  \n",
       "Data/test/6407/spoof/494411.png  0.686906      1        2             1  \n",
       "Data/test/6153/spoof/494412.png  0.643653      1        2             1  \n",
       "Data/test/6411/live/494418.png   0.677907      0        2             0  \n",
       "Data/test/6336/spoof/494419.png  0.065030      1        0             1  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "sf_column = {'Spoof':128, 'Cluster':129, 'Illumination':130}\n",
    "\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        embedding = torch.tensor(self.embeddings.iloc[idx, :128].values, dtype=torch.float32)\n",
    "        spoof = torch.tensor(self.embeddings.iloc[idx, 128], dtype=torch.long)\n",
    "        # 128 = Spoof, 129 = Cluster, 130 = Illumination\n",
    "        domain = torch.tensor(self.embeddings.iloc[idx, 129], dtype=torch.long)\n",
    "        return embedding, spoof, domain\n",
    "\n",
    "\n",
    "class AdversarialModel(nn.Module):\n",
    "    def __init__(self, num_clusters):\n",
    "        super(AdversarialModel, self).__init__()\n",
    "        self.step = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.anti_spoofing_head = nn.Sequential(\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "        self.domain_head = nn.Sequential(\n",
    "            nn.Linear(64, num_clusters)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.step(x)\n",
    "        anti_spoofing_logits = self.anti_spoofing_head(x)\n",
    "        domain_logits = self.domain_head(x)\n",
    "        return anti_spoofing_logits, domain_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [50/403], Loss_main: -0.004761040210723877, Loss_domain: 0.6657522320747375\n",
      "Epoch [1/3], Step [100/403], Loss_main: -0.006015121936798096, Loss_domain: 0.6299014091491699\n",
      "Epoch [1/3], Step [150/403], Loss_main: -0.007017672061920166, Loss_domain: 0.5696182250976562\n",
      "Epoch [1/3], Step [200/403], Loss_main: -0.0065149664878845215, Loss_domain: 0.5178641676902771\n",
      "Epoch [1/3], Step [250/403], Loss_main: -0.0009778738021850586, Loss_domain: 0.6109321713447571\n",
      "Epoch [1/3], Step [300/403], Loss_main: -0.0045957863330841064, Loss_domain: 0.4940187931060791\n",
      "Epoch [2/3], Step [50/403], Loss_main: -0.003193408250808716, Loss_domain: 0.4571388363838196\n",
      "Epoch [2/3], Step [100/403], Loss_main: -0.0026853084564208984, Loss_domain: 0.5048352479934692\n",
      "Epoch [2/3], Step [150/403], Loss_main: -0.0017455220222473145, Loss_domain: 0.49129676818847656\n",
      "Epoch [2/3], Step [200/403], Loss_main: -0.0027265548706054688, Loss_domain: 0.47062504291534424\n",
      "Epoch [2/3], Step [250/403], Loss_main: -0.0009154081344604492, Loss_domain: 0.5909494757652283\n",
      "Epoch [2/3], Step [300/403], Loss_main: -0.0017892420291900635, Loss_domain: 0.3587036728858948\n",
      "Epoch [3/3], Step [50/403], Loss_main: -0.0017281770706176758, Loss_domain: 0.5139908194541931\n",
      "Epoch [3/3], Step [100/403], Loss_main: -4.437565803527832e-05, Loss_domain: 0.4797746539115906\n",
      "Epoch [3/3], Step [150/403], Loss_main: -0.0005874037742614746, Loss_domain: 0.4851216971874237\n",
      "Epoch [3/3], Step [200/403], Loss_main: 0.0012980401515960693, Loss_domain: 0.4806709885597229\n",
      "Epoch [3/3], Step [250/403], Loss_main: -0.002621680498123169, Loss_domain: 0.34218740463256836\n",
      "Epoch [3/3], Step [300/403], Loss_main: 0.0022600293159484863, Loss_domain: 0.5292176008224487\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_epochs = 3\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "train_embeddings, test_embeddings = train_test_split(embeddings, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "train_dataset = EmbeddingsDataset(train_embeddings)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = EmbeddingsDataset(test_embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AdversarialModel(2).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_classifier = optim.Adam(model.anti_spoofing_head.parameters(), lr=learning_rate)\n",
    "optimizer_domain = optim.Adam(model.domain_head.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "target_domain = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (embedding, spoof, domain) in enumerate(train_loader):\n",
    "        # Move the data to the device\n",
    "        embedding = embedding.to(device)\n",
    "        spoof = spoof.to(device)\n",
    "        domain = (domain == target_domain).long().to(device)\n",
    "\n",
    "\n",
    "\n",
    "        # Step 1: Update all weights except the domain head\n",
    "        optimizer_classifier.zero_grad()\n",
    "\n",
    "        anti_spoofing_logits, domain_logits = model(embedding)\n",
    "        loss_main = criterion(anti_spoofing_logits, spoof) - criterion(domain_logits, domain)\n",
    "        loss_main.backward()\n",
    "        optimizer_classifier.step()\n",
    "\n",
    "        # Step 2: Update the domain head\n",
    "        optimizer_domain.zero_grad()\n",
    "\n",
    "        _, domain_logits = model(embedding)\n",
    "        loss_domain = criterion(domain_logits, domain)\n",
    "        loss_domain.backward()\n",
    "        optimizer_domain.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (i+1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], Loss_main: {loss_main.item()}, Loss_domain: {loss_domain.item()}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "sf_column = {'Spoof':128, 'Cluster':129, 'Illumination':130}\n",
    "\n",
    "def train(target_domain = 0, sensitive_feature = \"Spoof\"):\n",
    "\n",
    "    num_folds = 5\n",
    "    num_epochs = 3\n",
    "    \n",
    "    class EmbeddingsDataset(Dataset):\n",
    "        def __init__(self, embeddings):\n",
    "            self.embeddings = embeddings\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.embeddings)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            embedding = torch.tensor(self.embeddings.iloc[idx, :128].values, dtype=torch.float32)\n",
    "            spoof = torch.tensor(self.embeddings.iloc[idx, 128], dtype=torch.long)\n",
    "            # 128 = Spoof, 129 = Cluster, 130 = Illumination\n",
    "            domain = torch.tensor(self.embeddings.iloc[idx, sf_column[sensitive_feature]], dtype=torch.long)\n",
    "            return embedding, spoof, domain\n",
    "\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = EmbeddingsDataset(embeddings)\n",
    "\n",
    "    # Create the KFold object\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Iterate over the folds\n",
    "    for fold, (train_indices, test_indices) in enumerate(kf.split(dataset)):\n",
    "        print(f\"Sensitive Feature: {sensitive_feature}, Domain: {td}, Fold {fold+1}\")\n",
    "\n",
    "        # Create the data loaders for the current fold\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        test_sampler = SubsetRandomSampler(test_indices)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "        # Create the model and set the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = AdversarialModel(2).to(device)\n",
    "\n",
    "        # Define the loss functions and optimizers\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer_classifier = optim.Adam(model.anti_spoofing_head.parameters(), lr=learning_rate)\n",
    "        optimizer_domain = optim.Adam(model.domain_head.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the model for the current fold\n",
    "        target_domain = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (embedding, spoof, domain) in enumerate(train_loader):\n",
    "                # Move the data to the device\n",
    "                embedding = embedding.to(device)\n",
    "                spoof = spoof.to(device)\n",
    "                domain = (domain == target_domain).long().to(device)\n",
    "\n",
    "                # Step 1: Update all weights except the domain head\n",
    "                optimizer_classifier.zero_grad()\n",
    "\n",
    "                anti_spoofing_logits, domain_logits = model(embedding)\n",
    "                loss_main = criterion(anti_spoofing_logits, spoof) - criterion(domain_logits, domain)\n",
    "                loss_main.backward()\n",
    "                optimizer_classifier.step()\n",
    "\n",
    "                # Step 2: Update the domain head\n",
    "                optimizer_domain.zero_grad()\n",
    "\n",
    "                _, domain_logits = model(embedding)\n",
    "                loss_domain = criterion(domain_logits, domain)\n",
    "                loss_domain.backward()\n",
    "                optimizer_domain.step()\n",
    "\n",
    "                # Print progress\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss_main: {loss_main.item()}, Loss_domain: {loss_domain.item()}\")\n",
    "        print(\"Training complete for fold\", fold+1)\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Define the lists to store the true and predicted labels\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Iterate over the test data\n",
    "    for i, (embedding, spoof, domain) in enumerate(test_loader):\n",
    "\n",
    "        # Move the data to the device\n",
    "        embedding = embedding.to(device)\n",
    "        spoof = spoof.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        anti_spoofing_logits, domain_logits = model(embedding)\n",
    "\n",
    "        # Get the predicted labels\n",
    "        _, predicted = torch.max(anti_spoofing_logits, 1)\n",
    "\n",
    "        # Append the true and predicted labels to the lists\n",
    "        true_labels += spoof.tolist()\n",
    "        predicted_labels += predicted.tolist()\n",
    "\n",
    "    # Calculate the evaluation metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    roc_auc = roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "    metrics = [accuracy, precision, recall, f1, roc_auc]\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive Feature: Spoof, Domain: 1, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.003677666187286377, Loss_domain: 0.5597242116928101\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.0007641315460205078, Loss_domain: 0.5949178338050842\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.002371877431869507, Loss_domain: 0.49965962767601013\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.001490563154220581, Loss_domain: 0.4884730577468872\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.007254302501678467, Loss_domain: 0.4659276008605957\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.004740417003631592, Loss_domain: 0.5337275266647339\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.000594794750213623, Loss_domain: 0.37029698491096497\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0020847320556640625, Loss_domain: 0.5119344592094421\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.0033956170082092285, Loss_domain: 0.4233514070510864\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Spoof, Domain: 1, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.03585314750671387, Loss_domain: 0.5922233462333679\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.024209260940551758, Loss_domain: 0.5474745631217957\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.01906454563140869, Loss_domain: 0.5108729600906372\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.00960010290145874, Loss_domain: 0.4975830018520355\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.005804538726806641, Loss_domain: 0.5152568817138672\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.005992293357849121, Loss_domain: 0.47626861929893494\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.010434716939926147, Loss_domain: 0.3715742230415344\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.003437519073486328, Loss_domain: 0.49991506338119507\n",
      "Epoch [3/3], Step [300/322], Loss_main: -1.2278556823730469e-05, Loss_domain: 0.4881656765937805\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Spoof, Domain: 1, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.01891458034515381, Loss_domain: 0.6274689435958862\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.013255000114440918, Loss_domain: 0.5574784874916077\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.004273951053619385, Loss_domain: 0.5478257536888123\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.0057378411293029785, Loss_domain: 0.4912551939487457\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.0026530325412750244, Loss_domain: 0.5004523992538452\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.0029034018516540527, Loss_domain: 0.5112428069114685\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.004506796598434448, Loss_domain: 0.45128339529037476\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.004306793212890625, Loss_domain: 0.3846442699432373\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.00015282630920410156, Loss_domain: 0.5141075849533081\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Spoof, Domain: 1, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.001864016056060791, Loss_domain: 0.6031206846237183\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.0005674958229064941, Loss_domain: 0.5274273753166199\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.00021719932556152344, Loss_domain: 0.5449886322021484\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.0031704604625701904, Loss_domain: 0.4366835355758667\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.0005404949188232422, Loss_domain: 0.5041496157646179\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.0009883642196655273, Loss_domain: 0.40218302607536316\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.00020879507064819336, Loss_domain: 0.42140763998031616\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.0011758506298065186, Loss_domain: 0.43739134073257446\n",
      "Epoch [3/3], Step [300/322], Loss_main: -3.6656856536865234e-06, Loss_domain: 0.4261930584907532\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Spoof, Domain: 1, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.0017415285110473633, Loss_domain: 0.6244562268257141\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.007941961288452148, Loss_domain: 0.5351046919822693\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.005894362926483154, Loss_domain: 0.5391729474067688\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.004602372646331787, Loss_domain: 0.5046864151954651\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.003717660903930664, Loss_domain: 0.45692434906959534\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.0037473738193511963, Loss_domain: 0.48799124360084534\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.0026860833168029785, Loss_domain: 0.4064045548439026\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0016729235649108887, Loss_domain: 0.4385806620121002\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0003840923309326172, Loss_domain: 0.3855680227279663\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Spoof, Domain: 0, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.010968983173370361, Loss_domain: 0.5875987410545349\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.005072116851806641, Loss_domain: 0.5598363876342773\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.0026836395263671875, Loss_domain: 0.6010517477989197\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.004935801029205322, Loss_domain: 0.5139584541320801\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.003568977117538452, Loss_domain: 0.4740599989891052\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.0011577606201171875, Loss_domain: 0.492402583360672\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.0006732344627380371, Loss_domain: 0.41176727414131165\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0010537505149841309, Loss_domain: 0.49305397272109985\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0013203322887420654, Loss_domain: 0.4487062692642212\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Spoof, Domain: 0, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.001971423625946045, Loss_domain: 0.6211979985237122\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.003528892993927002, Loss_domain: 0.5958114266395569\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.0018658638000488281, Loss_domain: 0.5585017800331116\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.0002532005310058594, Loss_domain: 0.49271535873413086\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.000602424144744873, Loss_domain: 0.48358234763145447\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.0007938146591186523, Loss_domain: 0.4415149688720703\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.0006753802299499512, Loss_domain: 0.5425153374671936\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0002671182155609131, Loss_domain: 0.39159896969795227\n",
      "Epoch [3/3], Step [300/322], Loss_main: 2.244114875793457e-05, Loss_domain: 0.4029143452644348\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Spoof, Domain: 0, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.009464919567108154, Loss_domain: 0.5958259105682373\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.010328292846679688, Loss_domain: 0.5407750606536865\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.008524566888809204, Loss_domain: 0.48378244042396545\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.0011377930641174316, Loss_domain: 0.47651350498199463\n",
      "Epoch [2/3], Step [200/322], Loss_main: -1.475214958190918e-05, Loss_domain: 0.46634358167648315\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.001558542251586914, Loss_domain: 0.526782751083374\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.0009355545043945312, Loss_domain: 0.4800177812576294\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.0006425380706787109, Loss_domain: 0.5650234818458557\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.0017771720886230469, Loss_domain: 0.41867557168006897\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Spoof, Domain: 0, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.0011374950408935547, Loss_domain: 0.6413244605064392\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.006668686866760254, Loss_domain: 0.5557816624641418\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.0038095712661743164, Loss_domain: 0.5328701138496399\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.004614591598510742, Loss_domain: 0.5587689280509949\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.0039587318897247314, Loss_domain: 0.4672432839870453\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.003511786460876465, Loss_domain: 0.5019404888153076\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.00525432825088501, Loss_domain: 0.4837493896484375\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.004779517650604248, Loss_domain: 0.4536246061325073\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.006735265254974365, Loss_domain: 0.4303491413593292\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Spoof, Domain: 0, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.0036780834197998047, Loss_domain: 0.6334561109542847\n",
      "Epoch [1/3], Step [200/322], Loss_main: -3.4928321838378906e-05, Loss_domain: 0.584657609462738\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.001977384090423584, Loss_domain: 0.5667469501495361\n",
      "Epoch [2/3], Step [100/322], Loss_main: 5.7578086853027344e-05, Loss_domain: 0.46298229694366455\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.0021910667419433594, Loss_domain: 0.4743751287460327\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.0019292235374450684, Loss_domain: 0.5126624703407288\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.0020419061183929443, Loss_domain: 0.4110714793205261\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0027799010276794434, Loss_domain: 0.5197907090187073\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.00588032603263855, Loss_domain: 0.40562957525253296\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Cluster, Domain: 2, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.1250011920928955, Loss_domain: 0.4854837656021118\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.14794710278511047, Loss_domain: 0.4236380159854889\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.1654413640499115, Loss_domain: 0.3275946378707886\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.23820827901363373, Loss_domain: 0.2198740392923355\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.13623860478401184, Loss_domain: 0.3093249499797821\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.09235820174217224, Loss_domain: 0.3551194667816162\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.2880171835422516, Loss_domain: 0.20860743522644043\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.2614858150482178, Loss_domain: 0.19233261048793793\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.2392805814743042, Loss_domain: 0.22651496529579163\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Cluster, Domain: 2, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.1798582673072815, Loss_domain: 0.4455319046974182\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.1432848870754242, Loss_domain: 0.37156763672828674\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.3218364715576172, Loss_domain: 0.20140071213245392\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.2673195004463196, Loss_domain: 0.23778557777404785\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.2154688686132431, Loss_domain: 0.21676789224147797\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.2568928003311157, Loss_domain: 0.23533812165260315\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.20565977692604065, Loss_domain: 0.2717841863632202\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.1519937813282013, Loss_domain: 0.30328568816185\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.240306556224823, Loss_domain: 0.24866566061973572\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Cluster, Domain: 2, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.11172249913215637, Loss_domain: 0.4974028766155243\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.21999937295913696, Loss_domain: 0.33309417963027954\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.20608514547348022, Loss_domain: 0.32511991262435913\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.29246222972869873, Loss_domain: 0.1999356895685196\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.09616082906723022, Loss_domain: 0.3040695786476135\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.21020853519439697, Loss_domain: 0.24680602550506592\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.25579243898391724, Loss_domain: 0.1992449164390564\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.11145605146884918, Loss_domain: 0.22544695436954498\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.14466384053230286, Loss_domain: 0.23223915696144104\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Cluster, Domain: 2, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.1694599986076355, Loss_domain: 0.4385342597961426\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.16012796759605408, Loss_domain: 0.3829672038555145\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.18415862321853638, Loss_domain: 0.3672689199447632\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.2602196931838989, Loss_domain: 0.25663328170776367\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.23928537964820862, Loss_domain: 0.24023354053497314\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.29197925329208374, Loss_domain: 0.18849536776542664\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.09951645135879517, Loss_domain: 0.45682793855667114\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.1879100799560547, Loss_domain: 0.17349687218666077\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.2414904683828354, Loss_domain: 0.2274692803621292\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Cluster, Domain: 2, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.23755252361297607, Loss_domain: 0.3890135884284973\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.23201578855514526, Loss_domain: 0.34088653326034546\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.33030563592910767, Loss_domain: 0.23613058030605316\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.3401123285293579, Loss_domain: 0.20662957429885864\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.21229416131973267, Loss_domain: 0.2543078362941742\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.2792397737503052, Loss_domain: 0.23176676034927368\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.1650615632534027, Loss_domain: 0.293003112077713\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.2997662127017975, Loss_domain: 0.12679225206375122\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.3025793433189392, Loss_domain: 0.2186206579208374\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Cluster, Domain: 0, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.1377805769443512, Loss_domain: 0.4809156358242035\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.18525007367134094, Loss_domain: 0.36690589785575867\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.26165497303009033, Loss_domain: 0.3122829794883728\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.19080066680908203, Loss_domain: 0.2578366994857788\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.20089778304100037, Loss_domain: 0.29585179686546326\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.2196372151374817, Loss_domain: 0.21276411414146423\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.21774430572986603, Loss_domain: 0.16899044811725616\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.1141628623008728, Loss_domain: 0.37137919664382935\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.3105463981628418, Loss_domain: 0.23612083494663239\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Cluster, Domain: 0, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.050399452447891235, Loss_domain: 0.48501846194267273\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.19555187225341797, Loss_domain: 0.34591370820999146\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.161001056432724, Loss_domain: 0.3237744867801666\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.23421019315719604, Loss_domain: 0.2857651710510254\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.2150162160396576, Loss_domain: 0.25750449299812317\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.10012015700340271, Loss_domain: 0.339725524187088\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.3180437684059143, Loss_domain: 0.16720128059387207\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.07447567582130432, Loss_domain: 0.28063806891441345\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.1373794674873352, Loss_domain: 0.31397199630737305\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Cluster, Domain: 0, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.15650221705436707, Loss_domain: 0.4397212564945221\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.15222132205963135, Loss_domain: 0.4163898229598999\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.2771216630935669, Loss_domain: 0.2428971529006958\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.25475195050239563, Loss_domain: 0.2785549461841583\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.17488397657871246, Loss_domain: 0.23517347872257233\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.2428644746541977, Loss_domain: 0.2170131355524063\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.12397360801696777, Loss_domain: 0.3492761254310608\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.22409376502037048, Loss_domain: 0.24378034472465515\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.22361396253108978, Loss_domain: 0.18916358053684235\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Cluster, Domain: 0, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.14988747239112854, Loss_domain: 0.4668353497982025\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.20357051491737366, Loss_domain: 0.3545639216899872\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.13075721263885498, Loss_domain: 0.388293981552124\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.27596914768218994, Loss_domain: 0.2293000966310501\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.21450325846672058, Loss_domain: 0.2954194247722626\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.32661399245262146, Loss_domain: 0.1398920714855194\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.2261638343334198, Loss_domain: 0.19921061396598816\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.2935972809791565, Loss_domain: 0.19216959178447723\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.20060136914253235, Loss_domain: 0.2456451654434204\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Cluster, Domain: 0, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.13068220019340515, Loss_domain: 0.49256888031959534\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.2619794011116028, Loss_domain: 0.29716312885284424\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.22718650102615356, Loss_domain: 0.2966713309288025\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.2779655456542969, Loss_domain: 0.22626809775829315\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.17808088660240173, Loss_domain: 0.24884992837905884\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.13549193739891052, Loss_domain: 0.35049453377723694\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.0819171667098999, Loss_domain: 0.3178647756576538\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.1319626271724701, Loss_domain: 0.31812748312950134\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.13538941740989685, Loss_domain: 0.26541391015052795\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Cluster, Domain: 4, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.18009275197982788, Loss_domain: 0.4381161332130432\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.19846820831298828, Loss_domain: 0.40700674057006836\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.19397801160812378, Loss_domain: 0.36845463514328003\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.3039935231208801, Loss_domain: 0.24968098104000092\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.37673234939575195, Loss_domain: 0.22758568823337555\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.20929458737373352, Loss_domain: 0.2951411306858063\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.2871529459953308, Loss_domain: 0.15961210429668427\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.14396575093269348, Loss_domain: 0.24837714433670044\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.29847609996795654, Loss_domain: 0.2006082981824875\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Cluster, Domain: 4, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.16903266310691833, Loss_domain: 0.42773690819740295\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.08705189824104309, Loss_domain: 0.39398786425590515\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.19557315111160278, Loss_domain: 0.34997910261154175\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.13725769519805908, Loss_domain: 0.3396621644496918\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.23811548948287964, Loss_domain: 0.31373369693756104\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.20480254292488098, Loss_domain: 0.23833629488945007\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.15783169865608215, Loss_domain: 0.2783671021461487\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.15736621618270874, Loss_domain: 0.2526238262653351\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.14590239524841309, Loss_domain: 0.2552441358566284\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Cluster, Domain: 4, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.16164052486419678, Loss_domain: 0.48578691482543945\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.19714531302452087, Loss_domain: 0.35297802090644836\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.24383622407913208, Loss_domain: 0.3166627883911133\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.23067063093185425, Loss_domain: 0.3397882580757141\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.1462874412536621, Loss_domain: 0.332788348197937\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.11717501282691956, Loss_domain: 0.31879788637161255\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.21132031083106995, Loss_domain: 0.2481013834476471\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.2603830397129059, Loss_domain: 0.1716105341911316\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.1905529648065567, Loss_domain: 0.23718957602977753\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Cluster, Domain: 4, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.17420244216918945, Loss_domain: 0.45705634355545044\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.2969500720500946, Loss_domain: 0.25119706988334656\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.20226949453353882, Loss_domain: 0.3515095114707947\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.3587762117385864, Loss_domain: 0.21586032211780548\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.39070791006088257, Loss_domain: 0.16915567219257355\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.19464465975761414, Loss_domain: 0.3406403362751007\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.2455892711877823, Loss_domain: 0.20753376185894012\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.11232909560203552, Loss_domain: 0.2823813557624817\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.19678112864494324, Loss_domain: 0.16628071665763855\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Cluster, Domain: 4, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.2010136842727661, Loss_domain: 0.4258689284324646\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.1455807387828827, Loss_domain: 0.3828113377094269\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.3320816159248352, Loss_domain: 0.24927334487438202\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.3664303421974182, Loss_domain: 0.1446109414100647\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.3309406638145447, Loss_domain: 0.18573646247386932\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.1943265199661255, Loss_domain: 0.4280405044555664\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.05180445313453674, Loss_domain: 0.33634766936302185\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.27500396966934204, Loss_domain: 0.1937611997127533\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.2889174222946167, Loss_domain: 0.17681394517421722\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Cluster, Domain: 1, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.1504080891609192, Loss_domain: 0.44751089811325073\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.15157029032707214, Loss_domain: 0.3837779462337494\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.20508328080177307, Loss_domain: 0.2989315688610077\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.24216443300247192, Loss_domain: 0.26974165439605713\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.06455937027931213, Loss_domain: 0.3235229253768921\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.2219114601612091, Loss_domain: 0.2780519127845764\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.2153940498828888, Loss_domain: 0.18967196345329285\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.2753467559814453, Loss_domain: 0.18674717843532562\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.12277272343635559, Loss_domain: 0.30052974820137024\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Cluster, Domain: 1, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.1274951994419098, Loss_domain: 0.4822230637073517\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.1878460943698883, Loss_domain: 0.3986046612262726\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.18243929743766785, Loss_domain: 0.3774605095386505\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.38541752099990845, Loss_domain: 0.16455359756946564\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.16566896438598633, Loss_domain: 0.3638916611671448\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.2117857038974762, Loss_domain: 0.3349423110485077\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.27947837114334106, Loss_domain: 0.2584574818611145\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.27466291189193726, Loss_domain: 0.18911127746105194\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.21287915110588074, Loss_domain: 0.261443167924881\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Cluster, Domain: 1, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.15199574828147888, Loss_domain: 0.45779529213905334\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.2415429949760437, Loss_domain: 0.2986953854560852\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.27244436740875244, Loss_domain: 0.21053941547870636\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.1697719395160675, Loss_domain: 0.2980096638202667\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.13390296697616577, Loss_domain: 0.2786283493041992\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.13662555813789368, Loss_domain: 0.2909698188304901\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.1559411585330963, Loss_domain: 0.3089289665222168\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.36262375116348267, Loss_domain: 0.1107863038778305\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.20771414041519165, Loss_domain: 0.2644825875759125\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Cluster, Domain: 1, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.1807234287261963, Loss_domain: 0.45291298627853394\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.20148470997810364, Loss_domain: 0.3825400173664093\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.26632267236709595, Loss_domain: 0.2706460952758789\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.15268564224243164, Loss_domain: 0.32156896591186523\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.19270285964012146, Loss_domain: 0.26248225569725037\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.2732618451118469, Loss_domain: 0.24503974616527557\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.20130234956741333, Loss_domain: 0.3093131184577942\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.15458860993385315, Loss_domain: 0.2699663043022156\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.33211517333984375, Loss_domain: 0.15296436846256256\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Cluster, Domain: 1, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.10917377471923828, Loss_domain: 0.5124294757843018\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.2564850449562073, Loss_domain: 0.31729358434677124\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.2915257215499878, Loss_domain: 0.24253351986408234\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.28652289509773254, Loss_domain: 0.22214558720588684\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.16705721616744995, Loss_domain: 0.2890812158584595\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.23850563168525696, Loss_domain: 0.2689085304737091\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.019337594509124756, Loss_domain: 0.4243510663509369\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.18759524822235107, Loss_domain: 0.27011507749557495\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.30104899406433105, Loss_domain: 0.2047518491744995\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Cluster, Domain: 3, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.20813283324241638, Loss_domain: 0.37718144059181213\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.20876628160476685, Loss_domain: 0.31599271297454834\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.3089008331298828, Loss_domain: 0.23927998542785645\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.24570578336715698, Loss_domain: 0.26539403200149536\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.09340861439704895, Loss_domain: 0.39574551582336426\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.2324531078338623, Loss_domain: 0.28041380643844604\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.31426024436950684, Loss_domain: 0.19700296223163605\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.20881396532058716, Loss_domain: 0.2114587426185608\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.3000637888908386, Loss_domain: 0.1973637044429779\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Cluster, Domain: 3, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.17338484525680542, Loss_domain: 0.4495803117752075\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.17713871598243713, Loss_domain: 0.39740678668022156\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.2263754904270172, Loss_domain: 0.29021158814430237\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.236845463514328, Loss_domain: 0.2756471335887909\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.22499486804008484, Loss_domain: 0.2753237783908844\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.3291582465171814, Loss_domain: 0.14173729717731476\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.15226376056671143, Loss_domain: 0.3145347833633423\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.16602925956249237, Loss_domain: 0.16377346217632294\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.1462155431509018, Loss_domain: 0.22604061663150787\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Cluster, Domain: 3, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.18883424997329712, Loss_domain: 0.44062674045562744\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.22120416164398193, Loss_domain: 0.3543245792388916\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.2034057378768921, Loss_domain: 0.3114767074584961\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.1635613739490509, Loss_domain: 0.35678866505622864\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.13266083598136902, Loss_domain: 0.31234097480773926\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.22712282836437225, Loss_domain: 0.24578140676021576\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.15921086072921753, Loss_domain: 0.35911816358566284\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.1272805631160736, Loss_domain: 0.31577053666114807\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.23064038157463074, Loss_domain: 0.20371747016906738\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Cluster, Domain: 3, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.2222898304462433, Loss_domain: 0.3833761513233185\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.19353100657463074, Loss_domain: 0.3111983835697174\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.2018958330154419, Loss_domain: 0.3033331632614136\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.2761353850364685, Loss_domain: 0.2876124978065491\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.2465842217206955, Loss_domain: 0.21307145059108734\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.09853869676589966, Loss_domain: 0.3188265562057495\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.25185972452163696, Loss_domain: 0.284345805644989\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.38147759437561035, Loss_domain: 0.11380921304225922\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.3133680820465088, Loss_domain: 0.22565947473049164\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Cluster, Domain: 3, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.16790348291397095, Loss_domain: 0.46237534284591675\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.2698215842247009, Loss_domain: 0.35142087936401367\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.19786569476127625, Loss_domain: 0.32194653153419495\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.26734453439712524, Loss_domain: 0.2755160927772522\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.17916211485862732, Loss_domain: 0.3148680031299591\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.07861480116844177, Loss_domain: 0.35284945368766785\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.07995748519897461, Loss_domain: 0.32682114839553833\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.2728689908981323, Loss_domain: 0.16020095348358154\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.2458488494157791, Loss_domain: 0.21439562737941742\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Illumination, Domain: 2, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.0005764961242675781, Loss_domain: 0.6162559986114502\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.0006228089332580566, Loss_domain: 0.5290777683258057\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.0031104087829589844, Loss_domain: 0.552257239818573\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.0004725456237792969, Loss_domain: 0.47303640842437744\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.00011783838272094727, Loss_domain: 0.43866634368896484\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.000773996114730835, Loss_domain: 0.48068952560424805\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.0016710460186004639, Loss_domain: 0.42138755321502686\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0020369291305541992, Loss_domain: 0.534442663192749\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.0007300972938537598, Loss_domain: 0.5024475455284119\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Illumination, Domain: 2, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: 7.593631744384766e-05, Loss_domain: 0.6467459201812744\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.012667179107666016, Loss_domain: 0.5373613238334656\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.00800621509552002, Loss_domain: 0.49436330795288086\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.004121363162994385, Loss_domain: 0.5021373629570007\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.0025266408920288086, Loss_domain: 0.6030412316322327\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.006021559238433838, Loss_domain: 0.42778468132019043\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.005554378032684326, Loss_domain: 0.4197213649749756\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.0025555193424224854, Loss_domain: 0.49909505248069763\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.0022365450859069824, Loss_domain: 0.40542876720428467\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Illumination, Domain: 2, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.0018393993377685547, Loss_domain: 0.5774183869361877\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.0007673203945159912, Loss_domain: 0.49274903535842896\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.0028221607208251953, Loss_domain: 0.5266457200050354\n",
      "Epoch [2/3], Step [100/322], Loss_main: -8.046627044677734e-07, Loss_domain: 0.4600200951099396\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.0023345947265625, Loss_domain: 0.4720892310142517\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.00047409534454345703, Loss_domain: 0.41621407866477966\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.0005092024803161621, Loss_domain: 0.47729912400245667\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0025417208671569824, Loss_domain: 0.4312498867511749\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0007033944129943848, Loss_domain: 0.3670736849308014\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Illumination, Domain: 2, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.017480313777923584, Loss_domain: 0.6411607265472412\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.014745891094207764, Loss_domain: 0.5686218738555908\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.012018084526062012, Loss_domain: 0.578468382358551\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.006804764270782471, Loss_domain: 0.574275016784668\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.008779555559158325, Loss_domain: 0.4844259023666382\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.001969635486602783, Loss_domain: 0.520685076713562\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.006327927112579346, Loss_domain: 0.5220829248428345\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.007326275110244751, Loss_domain: 0.41501274704933167\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.00305330753326416, Loss_domain: 0.5425400733947754\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Illumination, Domain: 2, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.01598113775253296, Loss_domain: 0.6031221747398376\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.008219420909881592, Loss_domain: 0.6015516519546509\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.007341921329498291, Loss_domain: 0.5228017568588257\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.003313899040222168, Loss_domain: 0.6300493478775024\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.004331618547439575, Loss_domain: 0.4844423532485962\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.0004311203956604004, Loss_domain: 0.5207992196083069\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.004672199487686157, Loss_domain: 0.4428015351295471\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.0010522902011871338, Loss_domain: 0.4280146658420563\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.001394122838973999, Loss_domain: 0.42851388454437256\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Illumination, Domain: 1, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.024051547050476074, Loss_domain: 0.6251111030578613\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.0069501399993896484, Loss_domain: 0.6353508234024048\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.020723968744277954, Loss_domain: 0.4952302873134613\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.016814589500427246, Loss_domain: 0.46005964279174805\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.007108360528945923, Loss_domain: 0.4775235950946808\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.011628776788711548, Loss_domain: 0.46772974729537964\n",
      "Epoch [3/3], Step [100/322], Loss_main: 2.682209014892578e-06, Loss_domain: 0.571797788143158\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.00991407036781311, Loss_domain: 0.42811447381973267\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.005852490663528442, Loss_domain: 0.39984938502311707\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Illumination, Domain: 1, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.004859864711761475, Loss_domain: 0.5936269760131836\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.005970895290374756, Loss_domain: 0.5305063128471375\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.0027988553047180176, Loss_domain: 0.6170319318771362\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.001705765724182129, Loss_domain: 0.4906136989593506\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.003043949604034424, Loss_domain: 0.554517388343811\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.00735783576965332, Loss_domain: 0.6268521547317505\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.00196114182472229, Loss_domain: 0.36794355511665344\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.00036832690238952637, Loss_domain: 0.4888997972011566\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.0023223161697387695, Loss_domain: 0.4654181897640228\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Illumination, Domain: 1, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.013510823249816895, Loss_domain: 0.6161196231842041\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.012749552726745605, Loss_domain: 0.5327634811401367\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.011439383029937744, Loss_domain: 0.5354334712028503\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.004143834114074707, Loss_domain: 0.46413367986679077\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.0077359676361083984, Loss_domain: 0.4028380215167999\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.004876434803009033, Loss_domain: 0.40810680389404297\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.0030455291271209717, Loss_domain: 0.42781180143356323\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.0017471015453338623, Loss_domain: 0.46590569615364075\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.0031328797340393066, Loss_domain: 0.4047488868236542\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Illumination, Domain: 1, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.0022992491722106934, Loss_domain: 0.616984486579895\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.0011063814163208008, Loss_domain: 0.5389407277107239\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.0028377175331115723, Loss_domain: 0.4956064224243164\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.0008365511894226074, Loss_domain: 0.4589667320251465\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.0008690059185028076, Loss_domain: 0.4666089713573456\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.00024318695068359375, Loss_domain: 0.43922126293182373\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.0012356340885162354, Loss_domain: 0.47519925236701965\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0006712973117828369, Loss_domain: 0.3914847671985626\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0011178553104400635, Loss_domain: 0.4521777331829071\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Illumination, Domain: 1, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.0205114483833313, Loss_domain: 0.6117560863494873\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.016523122787475586, Loss_domain: 0.5606762170791626\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.007387995719909668, Loss_domain: 0.5305486917495728\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.01521417498588562, Loss_domain: 0.49686068296432495\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.006868094205856323, Loss_domain: 0.4736776649951935\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.005197107791900635, Loss_domain: 0.5139201283454895\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.0045525431632995605, Loss_domain: 0.455147922039032\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.005258649587631226, Loss_domain: 0.48011457920074463\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0022364258766174316, Loss_domain: 0.4401501715183258\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Illumination, Domain: 0, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.02221667766571045, Loss_domain: 0.624171257019043\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.017416059970855713, Loss_domain: 0.6274541020393372\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.0233495831489563, Loss_domain: 0.5625508427619934\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.006223320960998535, Loss_domain: 0.5647514462471008\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.007457256317138672, Loss_domain: 0.5802126526832581\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.010113716125488281, Loss_domain: 0.5133051872253418\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.005173385143280029, Loss_domain: 0.5117040872573853\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.008356094360351562, Loss_domain: 0.49590104818344116\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.004709780216217041, Loss_domain: 0.5085148811340332\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Illumination, Domain: 0, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.02433985471725464, Loss_domain: 0.6177136301994324\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.013197839260101318, Loss_domain: 0.5667911767959595\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.00565493106842041, Loss_domain: 0.5653828978538513\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.008202970027923584, Loss_domain: 0.4980252683162689\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.008839964866638184, Loss_domain: 0.41735029220581055\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.0011126995086669922, Loss_domain: 0.5314646363258362\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.00534781813621521, Loss_domain: 0.4297409951686859\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.0009851455688476562, Loss_domain: 0.5577292442321777\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0033840537071228027, Loss_domain: 0.42880821228027344\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Illumination, Domain: 0, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.0018836259841918945, Loss_domain: 0.6375612616539001\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.0024547576904296875, Loss_domain: 0.5922580361366272\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.0017386674880981445, Loss_domain: 0.508781373500824\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.0013354122638702393, Loss_domain: 0.4469684660434723\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.001795351505279541, Loss_domain: 0.5490813851356506\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.0028517842292785645, Loss_domain: 0.47130587697029114\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.00023689866065979004, Loss_domain: 0.49425724148750305\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0007134675979614258, Loss_domain: 0.41591933369636536\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.001917511224746704, Loss_domain: 0.41476917266845703\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Illumination, Domain: 0, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.017973124980926514, Loss_domain: 0.6281560659408569\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.013781130313873291, Loss_domain: 0.6150193810462952\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.019189655780792236, Loss_domain: 0.5209829807281494\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.007956832647323608, Loss_domain: 0.4927266836166382\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.009229838848114014, Loss_domain: 0.48955294489860535\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.007725030183792114, Loss_domain: 0.4055626094341278\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.003431349992752075, Loss_domain: 0.4630531668663025\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.007706493139266968, Loss_domain: 0.4407701790332794\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0029539167881011963, Loss_domain: 0.4975365400314331\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Illumination, Domain: 0, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.0026100873947143555, Loss_domain: 0.5901901125907898\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.0025731921195983887, Loss_domain: 0.5877248644828796\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.0017325282096862793, Loss_domain: 0.5146673321723938\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.0011929869651794434, Loss_domain: 0.5122398734092712\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.0007136166095733643, Loss_domain: 0.46254485845565796\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.00022590160369873047, Loss_domain: 0.5285639762878418\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.0023314356803894043, Loss_domain: 0.4591367840766907\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0007470250129699707, Loss_domain: 0.4657920002937317\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.0008231699466705322, Loss_domain: 0.42003756761550903\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Illumination, Domain: 4, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.011502861976623535, Loss_domain: 0.5903922915458679\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.006348252296447754, Loss_domain: 0.5584051609039307\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.0031711459159851074, Loss_domain: 0.618222713470459\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.004051029682159424, Loss_domain: 0.5160061717033386\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.0034761428833007812, Loss_domain: 0.4892560839653015\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.0038059651851654053, Loss_domain: 0.4466666877269745\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.001639723777770996, Loss_domain: 0.6012328267097473\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.0009192526340484619, Loss_domain: 0.49664783477783203\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.0013923048973083496, Loss_domain: 0.4708423912525177\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Illumination, Domain: 4, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.015003442764282227, Loss_domain: 0.6484091281890869\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.003374814987182617, Loss_domain: 0.5745969414710999\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.0001240372657775879, Loss_domain: 0.58540278673172\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.0027785301208496094, Loss_domain: 0.5253074765205383\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.0055272579193115234, Loss_domain: 0.5358539819717407\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.000540614128112793, Loss_domain: 0.5163464546203613\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.002940267324447632, Loss_domain: 0.4187118113040924\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0028679370880126953, Loss_domain: 0.4268934726715088\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0007104873657226562, Loss_domain: 0.44638118147850037\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Illumination, Domain: 4, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.0029671192169189453, Loss_domain: 0.5765882730484009\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.0010051131248474121, Loss_domain: 0.5343287587165833\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.0033665597438812256, Loss_domain: 0.47639310359954834\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.0007965564727783203, Loss_domain: 0.5378533005714417\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.000345081090927124, Loss_domain: 0.4712825417518616\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.0023102760314941406, Loss_domain: 0.5060740113258362\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.0002708137035369873, Loss_domain: 0.36891689896583557\n",
      "Epoch [3/3], Step [200/322], Loss_main: -2.7626752853393555e-05, Loss_domain: 0.46194779872894287\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0011184215545654297, Loss_domain: 0.48355793952941895\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Illumination, Domain: 4, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.047713637351989746, Loss_domain: 0.5585119724273682\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.029401838779449463, Loss_domain: 0.5165240168571472\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.009173095226287842, Loss_domain: 0.5561810731887817\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.015456676483154297, Loss_domain: 0.44873467087745667\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.020739316940307617, Loss_domain: 0.4196012616157532\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.00943019986152649, Loss_domain: 0.42371684312820435\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.004556000232696533, Loss_domain: 0.5014287233352661\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.0024115443229675293, Loss_domain: 0.3974759578704834\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.004211723804473877, Loss_domain: 0.44621726870536804\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Illumination, Domain: 4, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.017914414405822754, Loss_domain: 0.6254681348800659\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.011953890323638916, Loss_domain: 0.5974758863449097\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.015028178691864014, Loss_domain: 0.5193616151809692\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.008236944675445557, Loss_domain: 0.510789155960083\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.00943806767463684, Loss_domain: 0.42099788784980774\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.0005341768264770508, Loss_domain: 0.48636168241500854\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.005519121885299683, Loss_domain: 0.4038654863834381\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.004699498414993286, Loss_domain: 0.45977526903152466\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.002529531717300415, Loss_domain: 0.40647903084754944\n",
      "Training complete for fold 5\n",
      "Sensitive Feature: Illumination, Domain: 3, Fold 1\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.0018326640129089355, Loss_domain: 0.5786129236221313\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.0007794499397277832, Loss_domain: 0.5425951480865479\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.002324044704437256, Loss_domain: 0.4732086956501007\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.0028444230556488037, Loss_domain: 0.47100818157196045\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.0006941258907318115, Loss_domain: 0.48620620369911194\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.002017974853515625, Loss_domain: 0.5287898778915405\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.00031828880310058594, Loss_domain: 0.41548681259155273\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.0016970038414001465, Loss_domain: 0.5226686596870422\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0008593201637268066, Loss_domain: 0.3567626178264618\n",
      "Training complete for fold 1\n",
      "Sensitive Feature: Illumination, Domain: 3, Fold 2\n",
      "Epoch [1/3], Step [100/322], Loss_main: -0.02244281768798828, Loss_domain: 0.5980860590934753\n",
      "Epoch [1/3], Step [200/322], Loss_main: -0.009869635105133057, Loss_domain: 0.5338850617408752\n",
      "Epoch [1/3], Step [300/322], Loss_main: -0.005263030529022217, Loss_domain: 0.5100521445274353\n",
      "Epoch [2/3], Step [100/322], Loss_main: -0.004439562559127808, Loss_domain: 0.4879371225833893\n",
      "Epoch [2/3], Step [200/322], Loss_main: -0.003627210855484009, Loss_domain: 0.3912481963634491\n",
      "Epoch [2/3], Step [300/322], Loss_main: -0.002074331045150757, Loss_domain: 0.40498384833335876\n",
      "Epoch [3/3], Step [100/322], Loss_main: -0.0009208619594573975, Loss_domain: 0.4311976134777069\n",
      "Epoch [3/3], Step [200/322], Loss_main: -0.004082679748535156, Loss_domain: 0.38373857736587524\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0016297698020935059, Loss_domain: 0.4367745518684387\n",
      "Training complete for fold 2\n",
      "Sensitive Feature: Illumination, Domain: 3, Fold 3\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.004352211952209473, Loss_domain: 0.6239400506019592\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.0014910101890563965, Loss_domain: 0.6036621332168579\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.004917412996292114, Loss_domain: 0.4890211224555969\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.0018215775489807129, Loss_domain: 0.47785815596580505\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.002556025981903076, Loss_domain: 0.44252488017082214\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.0014630556106567383, Loss_domain: 0.5261312127113342\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.000660240650177002, Loss_domain: 0.533402144908905\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.0017442107200622559, Loss_domain: 0.4603281319141388\n",
      "Epoch [3/3], Step [300/322], Loss_main: 0.0006034374237060547, Loss_domain: 0.5409502983093262\n",
      "Training complete for fold 3\n",
      "Sensitive Feature: Illumination, Domain: 3, Fold 4\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.001838088035583496, Loss_domain: 0.5991813540458679\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.003830134868621826, Loss_domain: 0.5598856210708618\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.0012919604778289795, Loss_domain: 0.4885297119617462\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.003009796142578125, Loss_domain: 0.4918649196624756\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.0009616315364837646, Loss_domain: 0.4729584753513336\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.0010853409767150879, Loss_domain: 0.5725569128990173\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.000985264778137207, Loss_domain: 0.4540550112724304\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.006019294261932373, Loss_domain: 0.47366026043891907\n",
      "Epoch [3/3], Step [300/322], Loss_main: -0.0006523430347442627, Loss_domain: 0.4952668845653534\n",
      "Training complete for fold 4\n",
      "Sensitive Feature: Illumination, Domain: 3, Fold 5\n",
      "Epoch [1/3], Step [100/322], Loss_main: 0.03132396936416626, Loss_domain: 0.5839453935623169\n",
      "Epoch [1/3], Step [200/322], Loss_main: 0.010377109050750732, Loss_domain: 0.5410562753677368\n",
      "Epoch [1/3], Step [300/322], Loss_main: 0.014191687107086182, Loss_domain: 0.4768483638763428\n",
      "Epoch [2/3], Step [100/322], Loss_main: 0.004516184329986572, Loss_domain: 0.4706437289714813\n",
      "Epoch [2/3], Step [200/322], Loss_main: 0.00757598876953125, Loss_domain: 0.42176565527915955\n",
      "Epoch [2/3], Step [300/322], Loss_main: 0.0033666491508483887, Loss_domain: 0.4597651958465576\n",
      "Epoch [3/3], Step [100/322], Loss_main: 0.0016470253467559814, Loss_domain: 0.462954580783844\n",
      "Epoch [3/3], Step [200/322], Loss_main: 0.003755122423171997, Loss_domain: 0.3774990737438202\n",
      "Epoch [3/3], Step [300/322], Loss_main: 5.418062210083008e-05, Loss_domain: 0.5164551734924316\n",
      "Training complete for fold 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Spoof_1': [0.8134342846049311,\n",
       "  0.8301531846277882,\n",
       "  0.903744880046811,\n",
       "  0.8653873091469394,\n",
       "  0.7695296818006704],\n",
       " 'Spoof_0': [0.811687051058047,\n",
       "  0.8290322580645161,\n",
       "  0.9022820362785254,\n",
       "  0.8641075931633511,\n",
       "  0.7676441918265103],\n",
       " 'Cluster_2': [0.8138225587264609,\n",
       "  0.8279541210989597,\n",
       "  0.9081334113516677,\n",
       "  0.8661922701269709,\n",
       "  0.7679732261605423],\n",
       " 'Cluster_0': [0.8182877111240536,\n",
       "  0.8409340659340659,\n",
       "  0.8955529549444119,\n",
       "  0.867384528194956,\n",
       "  0.7807251214422002],\n",
       " 'Cluster_4': [0.8147932440302854,\n",
       "  0.8276595744680851,\n",
       "  0.9104739613809245,\n",
       "  0.8670938980217331,\n",
       "  0.7682779501076579],\n",
       " 'Cluster_1': [0.8206173558532324,\n",
       "  0.8257575757575758,\n",
       "  0.9248098303101229,\n",
       "  0.8724813690311897,\n",
       "  0.769964061144675],\n",
       " 'Cluster_3': [0.8130460104834013,\n",
       "  0.8313090418353576,\n",
       "  0.901111761263897,\n",
       "  0.8648041555524357,\n",
       "  0.7702327415667437],\n",
       " 'Illumination_2': [0.8196466705494079,\n",
       "  0.8289717155696538,\n",
       "  0.9174956114686952,\n",
       "  0.8709901402582975,\n",
       "  0.7720772921740475],\n",
       " 'Illumination_1': [0.8184818481848185,\n",
       "  0.8255966430632048,\n",
       "  0.9210064365125804,\n",
       "  0.8706956160973586,\n",
       "  0.7686393982909123],\n",
       " 'Illumination_0': [0.8225587264608814,\n",
       "  0.8391115926327194,\n",
       "  0.906377998829725,\n",
       "  0.8714486638537271,\n",
       "  0.7818098880472919],\n",
       " 'Illumination_4': [0.8258590564938847,\n",
       "  0.8307530831802676,\n",
       "  0.9262726740784084,\n",
       "  0.8759164476414442,\n",
       "  0.7770428575239128],\n",
       " 'Illumination_3': [0.8169287516986993,\n",
       "  0.8300880234729261,\n",
       "  0.9104739613809245,\n",
       "  0.8684247244314218,\n",
       "  0.7714516373552056]}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = {}\n",
    "for sf in sf_column:\n",
    "    for td in embeddings[sf].unique():\n",
    "        metrics = train(target_domain=td, sensitive_feature=sf)\n",
    "        name = str(sf) + '_' + str(td)\n",
    "        table[name] = metrics\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = pd.DataFrame.from_dict(table, orient='index', columns=['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC'])\n",
    "table_df.to_csv('metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8167346146379344\n",
      "Precision: 0.8345051379123851\n",
      "Recall: 0.9028671737858397\n",
      "F1 score: 0.8673412029229904\n",
      "Roc Auc score: 0.7748611691202711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the lists to store the true and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterate over the test data\n",
    "for i, (embedding, spoof, domain) in enumerate(test_loader):\n",
    "\n",
    "    # Move the data to the device\n",
    "    embedding = embedding.to(device)\n",
    "    spoof = spoof.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    anti_spoofing_logits, domain_logits = model(embedding)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    _, predicted = torch.max(anti_spoofing_logits, 1)\n",
    "\n",
    "    # Append the true and predicted labels to the lists\n",
    "    true_labels += spoof.tolist()\n",
    "    predicted_labels += predicted.tolist()\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "roc_auc = roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 score: {f1}')\n",
    "print(f'Roc Auc score: {roc_auc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
